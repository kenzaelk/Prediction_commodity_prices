{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7217994,"sourceType":"datasetVersion","datasetId":4177443}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport os\nimport numpy as np \nimport pandas as pd \nimport requests\nimport pandas_datareader as web\n\n# Date\nimport datetime as dt\nfrom datetime import date, timedelta, datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error,explained_variance_score, r2_score, mean_absolute_percentage_error\nimport math\n\n\n# Modeling and preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom sklearn.base import BaseEstimator\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Daily data forecast ","metadata":{}},{"cell_type":"code","source":"#Daily dataset\ndf = pd.read_csv('/kaggle/input/commdaily/commodities_DAILY.csv', parse_dates=True)\ndisplay(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().values.any()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the 'Date' column to datetime format \ndf['Dates'] = pd.to_datetime(df['Dates'])\n\n# Add a column for the day of the week (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\ndf['DayOfWeek'] = df['Dates'].dt.dayofweek\n\n# Add a binary column indicating whether it's a weekend (1 = Saturday or Sunday, 0 = other days)\ndf['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n\n# Display the modified Data\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ncolumns_of_interest = ['CL1_PX_LAST', 'NG1_PX_LAST', 'HO1_PX_LAST', 'W1_PX_LAST', 'C1_PX_LAST', 'S1_PX_LAST', 'BO1_PX_LAST', 'HG1_PX_LAST', 'GC1_PX_LAST', 'CT1_PX_LAST', 'LC1_PX_LAST']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Dates'] = df['Dates'].astype(int) / 10**9  # Convert nanoseconds to seconds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_px_last = df[columns_of_interest]\n\ncorrelation_matrix = df_px_last.corr()\n\n# Display the correlation matrix\nprint(correlation_matrix)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mapping of original column names to nicer labels\nlabel_map = {\n    'CL1_PX_LAST': 'Crude Oil',\n    'NG1_PX_LAST': 'Natural Gas',\n    'HO1_PX_LAST': 'Heating Oil',\n    'W1_PX_LAST': 'Wheat',\n    'C1_PX_LAST': 'Corn',\n    'S1_PX_LAST': 'Soybeans',\n    'BO1_PX_LAST': 'Soybean Oil',\n    'HG1_PX_LAST': 'HG Copper',\n    'GC1_PX_LAST': 'Gold',\n    'CT1_PX_LAST': 'Cotton',\n    'LC1_PX_LAST': 'Live Cattle'\n}\n\n# Subset the dataframe and calculate correlation\nsubset_df = df[columns_of_interest]\ncorrelation = subset_df.corr()\n\n# Rename columns and index using the mapping\ncorrelation.rename(columns=label_map, index=label_map, inplace=True)\n\nplt.figure(figsize=(8,8))\nplt.title(\"Correlation Matrix of Commodity Prices\")\nsns.heatmap(correlation, cbar=True, square=True, fmt=\".1f\", annot=True, annot_kws={\"size\":8}, cmap=\"Reds\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Error metric\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n# Rolling forecast for sklearn models (LR, RF, GBR)\ndef rolling_forecast(df, target_column, model_instance, window_size=50):\n    y_true_all = []\n    y_pred_all = []\n\n    for i in range(window_size, len(df) - 1):\n        train = df.iloc[i - window_size:i]\n        test = df.iloc[i + 1:i + 2]  # 1-step ahead\n\n        X_train = train.drop(columns=[target_column])\n        y_train = train[target_column]\n        X_test = test.drop(columns=[target_column])\n        y_test = test[target_column]\n\n        model = model_instance\n        model.fit(X_train, y_train)\n        prediction = model.predict(X_test)[0]\n\n        y_true_all.append(y_test.values[0])\n        y_pred_all.append(prediction)\n\n    mse = mean_squared_error(y_true_all, y_pred_all)\n    r2 = r2_score(y_true_all, y_pred_all)\n    mape = mean_absolute_percentage_error(y_true_all, y_pred_all)\n\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n# Hyperparameter tuning for RF and GBR\ndef tune_rf_hyperparams(df, target_column, window_size=100):\n    train = df.iloc[:window_size]\n    X_train = train.drop(columns=[target_column])\n    y_train = train[target_column]\n\n    rf = RandomForestRegressor(random_state=42)\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [20, 30, 10, None]\n    }\n    grid = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n    grid.fit(X_train, y_train)\n    print(f\"Best RF params for {target_column}: {grid.best_params_}\")\n    return RandomForestRegressor(**grid.best_params_, random_state=42)\n\ndef tune_gb_hyperparams(df, target_column, window_size=100):\n    train = df.iloc[:window_size]\n    X_train = train.drop(columns=[target_column])\n    y_train = train[target_column]\n\n    gb = GradientBoostingRegressor(random_state=42)\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'learning_rate': [0.01, 0.1, 0.2],\n        'max_depth': [3, 4, 5],\n        'subsample': [0.8, 0.9, 1.0]\n    }\n    grid = GridSearchCV(gb, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n    grid.fit(X_train, y_train)\n    print(f\"Best GB params for {target_column}: {grid.best_params_}\")\n    return GradientBoostingRegressor(**grid.best_params_, random_state=42)\n\n# Build LSTM model function for KerasRegressor\ndef create_lstm_model(units=50, lr=0.001):\n    model = Sequential()\n    model.add(LSTM(units=units, input_shape=(1,1)))\n    model.add(Dense(1))\n    optimizer = Adam(learning_rate=lr)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n\n# Build DNN model function for KerasRegressor\ndef create_dnn_model(units=50, lr=0.001):\n    model = Sequential()\n    model.add(Dense(units, activation='relu', input_dim=1))\n    model.add(Dense(units, activation='relu'))\n    model.add(Dense(1))\n    optimizer = Adam(learning_rate=lr)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n\n# Prepare data for keras models (LSTM and DNN)\ndef prepare_lstm_data(series, window_size=50):\n    scaler = MinMaxScaler()\n    scaled = scaler.fit_transform(series.values.reshape(-1, 1))\n    X, y = [], []\n    for i in range(len(scaled) - window_size):\n        X.append(scaled[i:i+window_size])\n        y.append(scaled[i+window_size])\n    X = np.array(X).reshape(-1, window_size, 1)\n    y = np.array(y)\n    return X, y, scaler\n\ndef prepare_dnn_data(series, window_size=50):\n    scaler = MinMaxScaler()\n    scaled = scaler.fit_transform(series.values.reshape(-1, 1))\n    X, y = [], []\n    for i in range(len(scaled) - window_size):\n        X.append(scaled[i:i+window_size].flatten())  # flatten window for DNN\n        y.append(scaled[i+window_size])\n    X = np.array(X)\n    y = np.array(y)\n    return X, y, scaler\n\n# Tune LSTM hyperparameters \ndef tune_lstm(df, target_column, window_size=50, n_iter=5):\n    series = df[target_column]\n    X, y, scaler = prepare_lstm_data(series, window_size)\n\n    model = KerasRegressor(build_fn=create_lstm_model, verbose=0)\n\n    param_dist = {\n        'units': [50, 100, 150],\n        'epochs': [50, 100, 150],\n        'batch_size': [32, 64, 128]\n    }\n\n    random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n                                       n_iter=n_iter, cv=3, verbose=1, n_jobs=1)\n    random_search.fit(X, y)\n    print(f\"LSTM best params for {target_column}: {random_search.best_params_}\")\n    return random_search.best_params_, scaler\n\n# Tune DNN hyperparameters with RandomizedSearchCV\ndef tune_dnn(df, target_column, window_size=50, n_iter=5):\n    series = df[target_column]\n    X, y, scaler = prepare_dnn_data(series, window_size)\n\n    model = KerasRegressor(build_fn=create_dnn_model, verbose=0)\n\n    param_dist = {\n        'units': [50, 100, 150],\n        'epochs': [50, 100, 150],\n        'batch_size': [32, 64, 128]\n    }\n\n    random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n                                       n_iter=n_iter, cv=3, verbose=1, n_jobs=1)\n    random_search.fit(X, y)\n    print(f\"DNN best params for {target_column}: {random_search.best_params_}\")\n    return random_search.best_params_, scaler\n\n# Rolling forecast for LSTM with tuned params\ndef rolling_forecast_lstm(df, target_column, best_params, scaler, window_size=50):\n    y_true_all = []\n    y_pred_all = []\n\n    series = df[target_column].values\n    scaled = scaler.transform(series.reshape(-1,1))\n\n    for i in range(window_size, len(df) - 1):\n        train_seq = scaled[i-window_size:i].reshape(1, window_size, 1)\n        y_true = series[i]\n\n        model = create_lstm_model(units=best_params['units'], lr=best_params['lr'])\n        model.fit(train_seq, scaled[i-window_size:i], epochs=best_params['epochs'], verbose=0)\n        pred_scaled = model.predict(train_seq)\n        pred = scaler.inverse_transform(pred_scaled)[0][0]\n\n        y_true_all.append(y_true)\n        y_pred_all.append(pred)\n\n    mse = mean_squared_error(y_true_all, y_pred_all)\n    r2 = r2_score(y_true_all, y_pred_all)\n    mape = mean_absolute_percentage_error(y_true_all, y_pred_all)\n\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n# Rolling forecast for DNN with tuned params\ndef rolling_forecast_dnn(df, target_column, best_params, scaler, window_size=50):\n    y_true_all = []\n    y_pred_all = []\n\n    series = df[target_column].values\n    scaled = scaler.transform(series.reshape(-1,1))\n\n    for i in range(window_size, len(df) - 1):\n        train_seq = scaled[i-window_size:i].reshape(1, window_size)  # Flattened for DNN\n        y_true = series[i]\n\n        model = create_dnn_model(units=best_params['units'], lr=best_params['lr'])\n        model.fit(train_seq, scaled[i-window_size:i], epochs=best_params['epochs'], verbose=0)\n        pred_scaled = model.predict(train_seq)\n        pred = scaler.inverse_transform(pred_scaled)[0][0]\n\n        y_true_all.append(y_true)\n        y_pred_all.append(pred)\n\n    mse = mean_squared_error(y_true_all, y_pred_all)\n    r2 = r2_score(y_true_all, y_pred_all)\n    mape = mean_absolute_percentage_error(y_true_all, y_pred_all)\n\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n\nwindow_size = 50\n\nresults = {}\n\nfor col in columns_of_interest:\n    print(f\"\\n=== Processing {col} ===\")\n\n    # Linear Regression (no tuning)\n    lr_model = LinearRegression()\n    lr_results = rolling_forecast(df, col, lr_model, window_size=window_size)\n\n    # Random Forest tuning and forecast\n    rf_model = tune_rf_hyperparams(df, col, window_size=100)\n    rf_results = rolling_forecast(df, col, rf_model, window_size=window_size)\n\n    # Gradient Boosting tuning and forecast\n    gb_model = tune_gb_hyperparams(df, col, window_size=100)\n    gb_results = rolling_forecast(df, col, gb_model, window_size=window_size)\n\n    # LSTM tuning\n    lstm_best_params, lstm_scaler = tune_lstm(df, col, window_size=window_size, n_iter=5)\n    lstm_results = rolling_forecast_lstm(df, col, lstm_best_params, lstm_scaler, window_size=window_size)\n\n    # DNN tuning\n    dnn_best_params, dnn_scaler = tune_dnn(df, col, window_size=window_size, n_iter=5)\n    dnn_results = rolling_forecast_dnn(df, col, dnn_best_params, dnn_scaler, window_size=window_size)\n\n    results[col] = {\n        'LinearRegression': lr_results,\n        'RandomForest': rf_results,\n        'GradientBoosting': gb_results,\n        'LSTM': lstm_results,\n        'DNN': dnn_results\n    }\n\n# Print summarized results\nfor commodity, res in results.items():\n    print(f\"\\nResults for {commodity}:\")\n    for model_name, metrics in res.items():\n        print(f\" {model_name}: MSE={metrics['MSE']:.3f}, R2={metrics['R2']:.3f}, MAPE={metrics['MAPE']:.3f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Prediction sentiment price","metadata":{}},{"cell_type":"code","source":"!pip install praw\nimport praw\nfrom transformers import pipeline, RobertaForSequenceClassification, RobertaTokenizer, BertForSequenceClassification, BertTokenizer, DistilBertForSequenceClassification, DistilBertTokenizer\n\n# Reddit API credentials\nreddit_client_id = 'bfr--CssnTdgrFGIwK_prQ'\nreddit_client_secret = 'FPm-1pUWywIqnCjTo8enZwtg9lxtbQ'\nreddit_user_agent = 'MytestApp/1.0 by IndependenceNew2283'\n\n# Authenticate with Reddit API using PRAW\nreddit = praw.Reddit(client_id=reddit_client_id,\n                     client_secret=reddit_client_secret,\n                     user_agent=reddit_user_agent)\n\n# Define the subreddit and keywords related to commodities\nsubreddit_name = 'commodities'\nkeywords = ['gold', 'silver', 'oil', 'copper', 'soybean', 'cotton', 'cattle', 'cotton', 'wheat', 'corn', 'gas','commodity','commodities','stock market']\n\n# Fetch submissions from the subreddit and filter by keywords\nsubreddit = reddit.subreddit(subreddit_name)\n\n# Define the number of submissions to fetch (400 000 lines)\ntotal_submissions = 400000\nbatch_size = 100\n\n# Extract data from submissions\ndata = {'title': [], 'body': [], 'created_utc': []}\nfor _ in range(total_submissions // batch_size):\n    submissions = subreddit.search(' OR '.join(keywords), sort='new', limit=batch_size)\n    for submission in submissions:\n        data['title'].append(submission.title)\n        data['body'].append(submission.selftext)\n        data['created_utc'].append(submission.created_utc)\n\n# Create a DataFrame from the extracted data\nreddit_data = pd.DataFrame(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load sentiment analysis models and tokenizers\nroberta_sentiment_model = pipeline('sentiment-analysis', model=RobertaForSequenceClassification.from_pretrained('roberta-base'), tokenizer=RobertaTokenizer.from_pretrained('roberta-base'))\nbert_sentiment_model = pipeline('sentiment-analysis', model=BertForSequenceClassification.from_pretrained('bert-base-uncased'), tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\ndistilbert_sentiment_model = pipeline('sentiment-analysis', model=DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased'), tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased'))\n\nmax_seq_length = 512\nreddit_data['body'] = reddit_data['body'].apply(lambda x: x[:max_seq_length])\n\n# Apply sentiment analysis using RoBERTa, BERT, and DistilBERT\nreddit_data['title_sentiment_roberta'] = reddit_data['title'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\nreddit_data['body_sentiment_roberta'] = reddit_data['body'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\n\nreddit_data['title_sentiment_bert'] = reddit_data['title'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\nreddit_data['body_sentiment_bert'] = reddit_data['body'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\n\nreddit_data['title_sentiment_distilbert'] = reddit_data['title'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\nreddit_data['body_sentiment_distilbert'] = reddit_data['body'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\n\nprint(reddit_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reddit_data['created_utc'] = pd.to_datetime(reddit_data['created_utc'], unit='s', utc=True)\nreddit_data['created_utc'] = reddit_data['created_utc'].dt.tz_convert('GMT')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#label_mapping = {'LABEL_0': 'Negative', 'LABEL_1': 'Neutral', 'LABEL_2': 'Positive'}\nlabel_mapping = {'LABEL_0': '0', 'LABEL_1': '-1', 'LABEL_2': '1'}\n\nfor model in ['roberta', 'bert', 'distilbert']:\n    reddit_data[f'title_sentiment_{model}'] = reddit_data[f'title_sentiment_{model}'].map(label_mapping)\n    reddit_data[f'body_sentiment_{model}'] = reddit_data[f'body_sentiment_{model}'].map(label_mapping)\n\n# Visualize Sentiment Distribution\n#labels = ['Negative', 'Neutral', 'Positive']\nlabels = ['-1', '0', '1']\n# Plotting\nfig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\nbar_width = 0.35  # Adjust the width of the bars\nfor i, model in enumerate(['roberta', 'bert', 'distilbert']):\n    # Title Sentiment\n    title_sentiment_counts = reddit_data[f'title_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar(labels, title_sentiment_counts, color='skyblue', label=f'Title ({model.capitalize()})', width=bar_width)\n\n    # Body Sentiment\n    body_sentiment_counts = reddit_data[f'body_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar([x + bar_width for x in range(len(labels))], body_sentiment_counts, color='#CBC3E3', alpha=0.7, label=f'Body ({model.capitalize()})', width=bar_width)\n\n    axs[i].set_title(f'{model.capitalize()} Sentiment Distribution')\n    axs[i].legend()  # Add legend to each subplot\n\nfig.suptitle('Sentiment Analysis on Reddit Data')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the DataFrame to a CSV file\nreddit_data.to_csv('reddit_data.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load BERT pipeline\nbert_model = pipeline('sentiment-analysis',\n                      model=BertForSequenceClassification.from_pretrained('bert-base-uncased'),\n                      tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\n\n# Apply sentiment analysis\nreddit_data['title_sentiment_bert'] = reddit_data['title'].apply(lambda x: bert_model(x)[0]['label'])\nreddit_data['body_sentiment_bert'] = reddit_data['body'].apply(lambda x: bert_model(x)[0]['label'])\n\n# Convert sentiment labels\nlabel_mapping = {'LABEL_0': -1, 'LABEL_1': 0, 'LABEL_2': 1}\nreddit_data['title_sentiment_bert'] = reddit_data['title_sentiment_bert'].map(label_mapping)\nreddit_data['body_sentiment_bert'] = reddit_data['body_sentiment_bert'].map(label_mapping)\n\n# Daily average sentiment\n#daily_sentiment = reddit_data.groupby('date')[['title_sentiment_bert', 'body_sentiment_bert']].mean().reset_index()\nreddit_data['created_utc'] = pd.to_datetime(reddit_data['created_utc'], unit='s', utc=True)\nreddit_data['created_utc'] = reddit_data['created_utc'].dt.tz_convert('GMT')\n\n# FIX: Extract date for grouping\nreddit_data['date'] = reddit_data['created_utc'].dt.date\n\n# Now this will work\ndaily_sentiment = reddit_data.groupby('date')[['title_sentiment_bert', 'body_sentiment_bert']].mean().reset_index()\n\n# Ensure both columns are datetime type\n#df['Dates'] = pd.to_datetime(df['Dates'])\ndaily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n\n# Perform the merge\ndf = df.merge(daily_sentiment, how='left', left_on='Dates', right_on='date')\n\n# Drop extra column and fill missing sentiment values\ndf.drop(columns=['date'], inplace=True)\ndf.fillna(0, inplace=True)\n\n\n# Add calendar features\ndf['DayOfWeek'] = df['Dates'].dt.dayofweek\ndf['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n\n# Merge sentiment\ndf = df.merge(daily_sentiment, how='left', left_on='Dates', right_on='date')\ndf.drop(columns=['date'], inplace=True)\ndf.fillna(0, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#Rolling Forecast Function\ndef rolling_forecast(df, target_column, model_instance, window_size=50):\n    y_true_all = []\n    y_pred_all = []\n\n    for i in range(window_size, len(df) - 1):\n        train = df.iloc[i - window_size:i]\n        test = df.iloc[i + 1:i + 2]\n\n        X_train = train.drop(columns=[target_column])\n        y_train = train[target_column]\n        X_test = test.drop(columns=[target_column])\n        y_test = test[target_column]\n\n        model = model_instance\n        model.fit(X_train, y_train)\n        prediction = model.predict(X_test)[0]\n\n        y_true_all.append(y_test.values[0])\n        y_pred_all.append(prediction)\n\n    mse = mean_squared_error(y_true_all, y_pred_all)\n    r2 = r2_score(y_true_all, y_pred_all)\n    mape = mean_absolute_percentage_error(y_true_all, y_pred_all)\n\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n# LSTM Forecasting Function\ndef lstm_forecast(df, target_column, window_size=50):\n    df_scaled = MinMaxScaler().fit_transform(df)\n    target_index = df.columns.get_loc(target_column)\n\n    X, y = [], []\n    for i in range(window_size, len(df_scaled)):\n        X.append(df_scaled[i - window_size:i])\n        y.append(df_scaled[i, target_index])\n    X, y = np.array(X), np.array(y)\n\n    split = int(0.8 * len(X))\n    X_train, y_train = X[:split], y[:split]\n    X_test, y_test = X[split:], y[split:]\n\n    def build_model(hp):\n        model = Sequential()\n        model.add(LSTM(hp.Int('units', 32, 128, step=32), activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])))\n        model.add(Dense(1))\n        model.compile(optimizer='adam', loss='mse')\n        return model\n\n    tuner = RandomSearch(\n        build_model,\n        objective='val_loss',\n        max_trials=3,\n        executions_per_trial=1,\n        directory='lstm_tuning',\n        project_name=f'lstm_{target_column}'\n    )\n\n    tuner.search(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[EarlyStopping(patience=3)], verbose=0)\n    model = tuner.get_best_models(1)[0]\n    y_pred = model.predict(X_test).flatten()\n\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    mape = mean_absolute_percentage_error(y_test, y_pred)\n\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n# DNN Forecasting Function\ndef dnn_forecast(df, target_column, window_size=50):\n    df_scaled = MinMaxScaler().fit_transform(df)\n    target_index = df.columns.get_loc(target_column)\n\n    X, y = [], []\n    for i in range(window_size, len(df_scaled)):\n        X.append(df_scaled[i - window_size:i].flatten())\n        y.append(df_scaled[i, target_index])\n    X, y = np.array(X), np.array(y)\n\n    split = int(0.8 * len(X))\n    X_train, y_train = X[:split], y[:split]\n    X_test, y_test = X[split:], y[split:]\n\n    model = Sequential()\n    model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[EarlyStopping(patience=3)], verbose=0)\n    y_pred = model.predict(X_test).flatten()\n\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    mape = mean_absolute_percentage_error(y_test, y_pred)\n\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n# Model Evaluation \ndf = df.drop(columns=['Dates'])\ndf_scaled = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)\n\nall_results = {}\n\nfor model_name, model in {\n    'LinearRegression': LinearRegression(),\n    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n}.items():\n    model_results = {}\n    for col in columns_of_interest:\n        print(f\"Running {model_name} forecast for: {col}\")\n        res = rolling_forecast(df_scaled, target_column=col, model_instance=model, window_size=50)\n        model_results[col] = res\n    all_results[model_name] = pd.DataFrame(model_results).T\n\n# LSTM and DNN \nfor model_type in ['LSTM', 'DNN']:\n    model_results = {}\n    for col in columns_of_interest:\n        print(f\"Running {model_type} forecast for: {col}\")\n        if model_type == 'LSTM':\n            res = lstm_forecast(df_scaled, target_column=col, window_size=50)\n        else:\n            res = dnn_forecast(df_scaled, target_column=col, window_size=50)\n        model_results[col] = res\n    all_results[model_type] = pd.DataFrame(model_results).T\n\n# Display Results\nfor name, result_df in all_results.items():\n    print(f\"\\nModel: {name}\")\n    print(result_df.round(3))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install feedparser\nimport feedparser\nimport urllib.parse\n\n# Define keywords related to commodities\nkeywords = ['gold', 'silver', 'oil', 'copper', 'soybean', 'cotton', 'cattle', 'cotton', 'wheat', 'corn', 'gas', 'commodity', 'commodities', 'stock market']\n\n# Fetch news articles related to commodities from Google News (500 000 lines) \nnews_data = {'title': [], 'summary': [], 'published': []}\ntotal_lines = 500000\nlines_fetched = 0\nbatch_size = 100\n\nwhile lines_fetched < total_lines:\n    for keyword in keywords:\n        # Encode keyword for URL\n        encoded_keyword = urllib.parse.quote_plus(keyword)\n        feed_url = f'https://news.google.com/rss/search?q={encoded_keyword}&hl=en-US&gl=US&ceid=US:en'\n        feed = feedparser.parse(feed_url)\n        for entry in feed.entries:\n            news_data['title'].append(entry.title)\n            news_data['summary'].append(entry.summary)\n            news_data['published'].append(entry.published)\n            lines_fetched += 1\n            if lines_fetched >= total_lines:\n                break\n        if lines_fetched >= total_lines:\n            break\n    if lines_fetched >= total_lines:\n        break\n\n# Create a DataFrame from the extracted news data\ngoogle_news_df = pd.DataFrame(news_data)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load sentiment analysis models and tokenizers\nroberta_sentiment_model = pipeline('sentiment-analysis', model=RobertaForSequenceClassification.from_pretrained('roberta-base'), tokenizer=RobertaTokenizer.from_pretrained('roberta-base'))\nbert_sentiment_model = pipeline('sentiment-analysis', model=BertForSequenceClassification.from_pretrained('bert-base-uncased'), tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\ndistilbert_sentiment_model = pipeline('sentiment-analysis', model=DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased'), tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased'))\n\n# Truncate the 'summary' text to fit within the model's maximum sequence length\nmax_seq_length = 512\ngoogle_news_df['summary'] = google_news_df['summary'].apply(lambda x: x[:max_seq_length])\n\n# Apply sentiment analysis using RoBERTa, BERT, and DistilBERT\ngoogle_news_df['title_sentiment_roberta'] = google_news_df['title'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\ngoogle_news_df['summary_sentiment_roberta'] = google_news_df['summary'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\n\ngoogle_news_df['title_sentiment_bert'] = google_news_df['title'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\ngoogle_news_df['summary_sentiment_bert'] = google_news_df['summary'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\n\ngoogle_news_df['title_sentiment_distilbert'] = google_news_df['title'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\ngoogle_news_df['summary_sentiment_distilbert'] = google_news_df['summary'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\n\n# Display the DataFrame with sentiment analysis results\nprint(google_news_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map labels for sentiment categories\nlabel_mapping = {'LABEL_0': '-1', 'LABEL_1': '0', 'LABEL_2': '1'}\n\n# Map sentiment labels in the DataFrame\nfor model in ['roberta', 'bert', 'distilbert']:\n    google_news_df[f'title_sentiment_{model}'] = google_news_df[f'title_sentiment_{model}'].map(label_mapping)\n    google_news_df[f'summary_sentiment_{model}'] = google_news_df[f'summary_sentiment_{model}'].map(label_mapping)\n\n# Visualize Sentiment Distribution\nlabels = ['-1', '0', '1']\n\nfig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\nbar_width = 0.35\nfor i, model in enumerate(['roberta', 'bert', 'distilbert']):\n    title_sentiment_counts = google_news_df[f'title_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar(labels, title_sentiment_counts, color='skyblue', label=f'Title ({model.capitalize()})', width=bar_width)\n    summary_sentiment_counts = google_news_df[f'summary_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar([x + bar_width for x in range(len(labels))], summary_sentiment_counts, color='#CBC3E3', alpha=0.7, label=f'Summary ({model.capitalize()})', width=bar_width)\n\n    axs[i].set_title(f'{model.capitalize()} Sentiment Distribution')\n    axs[i].legend()\n\nfig.suptitle('Sentiment Analysis on Google News Data')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"news_df.to_csv('news_df.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert raw date strings to datetime.date\ndef parse_google_date(raw_date):\n    try:\n        if 'ago' in raw_date.lower():\n            num, unit = raw_date.split()[0], raw_date.split()[1]\n            unit = unit if unit.endswith('s') else unit + 's'\n            return (datetime.now() - timedelta(**{unit: int(num)})).date()\n        else:\n            return pd.to_datetime(raw_date).date()\n    except:\n        return pd.NaT\n\ngoogle_news_df['date'] = google_news_df['date'].apply(parse_google_date)\ngoogle_news_df.dropna(subset=['date'], inplace=True)\n\n# Load BERT pipeline for sentiment\nbert_model = pipeline('sentiment-analysis',\n                      model=BertForSequenceClassification.from_pretrained('bert-base-uncased'),\n                      tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\n\n# Apply BERT to headlines\ngoogle_news_df['title_sentiment_bert'] = google_news_df['title'].apply(lambda x: bert_model(x)[0]['label'])\n\n# Map BERT labels to scores\nlabel_mapping = {'LABEL_0': -1, 'LABEL_1': 0, 'LABEL_2': 1}\ngoogle_news_df['title_sentiment_bert'] = google_news_df['title_sentiment_bert'].map(label_mapping)\n\n# Daily average sentiment\ndaily_google_sentiment = google_news_df.groupby('date')['title_sentiment_bert'].mean().reset_index()\ndaily_google_sentiment.rename(columns={'title_sentiment_bert': 'google_news_sentiment'}, inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure both columns are datetime type\ndf['Dates'] = pd.to_datetime(df['Dates'])\ndaily_google_sentiment['date'] = pd.to_datetime(daily_google_sentiment['date'])\n\n# Perform the merge\ndf = df.merge(daily_google_sentiment, how='left', left_on='Dates', right_on='date')\n\n# Drop extra column and fill missing sentiment values\ndf.drop(columns=['date'], inplace=True)\ndf.fillna(0, inplace=True)\n\n\n# Add calendar features\ndf['DayOfWeek'] = df['Dates'].dt.dayofweek\ndf['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n\n# Merge sentiment\ndf = df.merge(daily_google_sentiment, how='left', left_on='Dates', right_on='date')\ndf.drop(columns=['date'], inplace=True)\ndf.fillna(0, inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop(columns=['Dates'])\ndf_scaled = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)\n\nall_results = {}\n\nfor model_name, model in {\n    'LinearRegression': LinearRegression(),\n    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n}.items():\n    model_results = {}\n    for col in columns_of_interest:\n        print(f\"Running {model_name} forecast for: {col}\")\n        res = rolling_forecast(df_scaled, target_column=col, model_instance=model, window_size=50)\n        model_results[col] = res\n    all_results[model_name] = pd.DataFrame(model_results).T\n\n# LSTM and DNN \nfor model_type in ['LSTM', 'DNN']:\n    model_results = {}\n    for col in columns_of_interest:\n        print(f\"Running {model_type} forecast for: {col}\")\n        if model_type == 'LSTM':\n            res = lstm_forecast(df_scaled, target_column=col, window_size=50)\n        else:\n            res = dnn_forecast(df_scaled, target_column=col, window_size=50)\n        model_results[col] = res\n    all_results[model_type] = pd.DataFrame(model_results).T\n\n# Display Results\nfor name, result_df in all_results.items():\n    print(f\"\\nModel: {name}\")\n    print(result_df.round(3))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install shap\nimport shap\n\nlabel_map = {\n    'CL1_PX_LAST': 'Crude Oil CL',\n    'NG1_PX_LAST': 'Natural Gas NG',\n    'HO1_PX_LAST': 'Heating Oil HO',\n    'W1_PX_LAST': 'Wheat W',\n    'C1_PX_LAST': 'Corn C',\n    'S1_PX_LAST': 'Soybeans S',\n    'BO1_PX_LAST': 'Soybean Oil BO',\n    'HG1_PX_LAST': 'Copper HG',\n    'GC1_PX_LAST': 'Gold GC',\n    'CT1_PX_LAST': 'Cotton CT',\n    'LC1_PX_LAST': 'Live Cattle LC',\n    'title_sentiment': 'title_sentiment',\n    'body_sentiment': 'body_sentiment',\n    'DayOfWeek'\t: 'DayOfWeek',\n    'IsWeekend': 'IsWeekend'\n}\n\nmodels_to_explain = {\n    'LR': LinearRegression(),\n    'RF': RandomForestRegressor(n_estimators=100, random_state=0),\n    'GBR': GradientBoostingRegressor(n_estimators=100, random_state=0)\n}\ntarget_columns = list(label_map.keys())\n\ny_dict = {col: df[col] for col in target_columns}\nX = df.drop(columns=target_columns)\n\nfor model_name, model in models_to_explain.items():\n    print(f\"\\n=== SHAP Summary for {model_name} ===\")\n    for target_col in y_dict:\n        readable_label = label_map.get(target_col, target_col)\n        print(f\"\\nTarget: {readable_label}\")\n        y = y_dict[target_col]\n        model.fit(X, y)\n        if model_name == 'LR':\n            explainer = shap.Explainer(model.predict, X)\n        else:\n            explainer = shap.TreeExplainer(model)\n        shap_values = explainer.shap_values(X)\n        # Plot SHAP summary\n        plt.figure(figsize=(10, 4))\n        shap.summary_plot(shap_values, X, show=False)\n        plt.title(f\"SHAP Summary Plot - {model_name}\")\n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df.drop(columns=target_columns)\ny_dict = {col: df[col] for col in target_columns}\nX_values = X.values.astype(np.float32)\nX_shap_sample = X_values[:100]  \nmodels = {\n    \"DNN\": dnn_model,      \n    \"LSTM\": lstm_model     \n}\n\nX_shap_lstm = np.expand_dims(X_shap_sample, axis=1)\nfor model_name, model in models.items():\n    print(f\"\\n=== SHAP Summary for {model_name} ===\")    \n    for target_col in target_columns:\n        readable_label = label_map.get(target_col, target_col)\n        print(f\"\\nTarget: {readable_label}\")\n        try:\n            input_data = X_shap_lstm if model_name == \"LSTM\" else X_shap_sample\n            explainer = shap.GradientExplainer(model, input_data)\n            shap_values = explainer.shap_values(input_data)\n            print(f\"SHAP Summary Plot - {model_name}\")\n            shap.summary_plot(shap_values, input_data, feature_names=X.columns.tolist())\n        except Exception as e:\n            print(f\"Error in SHAP for {model_name}, {readable_label}: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}