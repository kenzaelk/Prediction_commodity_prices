{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7217994,"sourceType":"datasetVersion","datasetId":4177443}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport os\nimport numpy as np \nimport pandas as pd \nimport requests\nimport pandas_datareader as web\n\n# Date\nimport datetime as dt\nfrom datetime import date, timedelta, datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error,explained_variance_score, r2_score, mean_absolute_percentage_error\nimport math\n\n\n# Modeling and preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom tensorflow.keras.layers import GRU, Dense\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom sklearn.base import BaseEstimator\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-10T20:23:19.202946Z","iopub.execute_input":"2024-04-10T20:23:19.203347Z","iopub.status.idle":"2024-04-10T20:23:39.223244Z","shell.execute_reply.started":"2024-04-10T20:23:19.203312Z","shell.execute_reply":"2024-04-10T20:23:39.221899Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Daily data forecast ","metadata":{}},{"cell_type":"code","source":"#Daily dataset\ndf = pd.read_csv('/kaggle/input/commdaily/commodities_DAILY.csv', parse_dates=True)\ndisplay(df)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T20:23:39.225067Z","iopub.execute_input":"2024-04-10T20:23:39.225897Z","iopub.status.idle":"2024-04-10T20:23:39.357104Z","shell.execute_reply.started":"2024-04-10T20:23:39.225856Z","shell.execute_reply":"2024-04-10T20:23:39.356008Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"           Dates  CL1_PX_LAST  CL1_VOLATILITY_10D  CL1_VOLATILITY_20D  \\\n0     12/30/1999        25.60               30.94               32.43   \n1     12/31/1999        25.60               30.94               32.43   \n2       1/3/2000        25.60               30.94               32.43   \n3       1/4/2000        25.55               30.98               32.44   \n4       1/5/2000        24.91               32.87               30.97   \n...          ...          ...                 ...                 ...   \n6246   12/8/2023        71.23               38.18               38.51   \n6247  12/11/2023        71.32               35.20               37.92   \n6248  12/12/2023        68.61               34.73               39.86   \n6249  12/13/2023        69.47               36.71               40.09   \n6250  12/14/2023        71.22               39.68               37.43   \n\n      NG1_PX_LAST  NG1_VOLATILITY_10D  NG1_VOLATILITY_20D  HO1_PX_LAST  \\\n0           2.329              46.714              60.137        69.03   \n1           2.329              46.714              60.137        69.03   \n2           2.329              46.714              60.137        69.03   \n3           2.176              52.823              62.008        67.78   \n4           2.168              53.389              59.658        66.55   \n...           ...                 ...                 ...          ...   \n6246        2.581              45.750              46.203       258.10   \n6247        2.431              52.322              43.354       260.87   \n6248        2.311              47.156              45.112       250.74   \n6249        2.335              49.072              43.268       254.81   \n6250        2.388              52.850              44.416       258.55   \n\n      HO1_VOLATILITY_10D  HO1_VOLATILITY_20D  ...  HG1_VOLATILITY_20D  \\\n0                  21.16               28.16  ...               22.09   \n1                  21.16               28.16  ...               22.09   \n2                  21.16               28.16  ...               22.09   \n3                  23.14               29.10  ...               22.94   \n4                  24.18               29.17  ...               23.07   \n...                  ...                 ...  ...                 ...   \n6246               39.31               39.22  ...               21.47   \n6247               36.00               36.83  ...               20.77   \n6248               38.64               39.01  ...               20.74   \n6249               42.04               39.38  ...               20.50   \n6250               31.20               37.44  ...               22.20   \n\n      GC1_PX_LAST  GC1_VOLATILITY_10D  GC1_VOLATILITY_20D  CT1_PX_LAST  \\\n0           289.6                6.59               15.48        50.74   \n1           289.6                6.59               15.48        50.74   \n2           289.6                6.59               15.48        51.07   \n3           283.7               14.06               16.17        50.73   \n4           282.1               14.34               15.50        51.56   \n...           ...                 ...                 ...          ...   \n6246       1998.3               20.41               15.55        81.44   \n6247       1978.0               18.91               16.01        82.00   \n6248       1977.8               18.58               15.74        81.05   \n6249       1982.3               18.86               15.75        81.18   \n6250       2033.0               22.25               17.81        81.68   \n\n      CT1_VOLATILITY_10D  CT1_VOLATILITY_20D  LC1_PX_LAST  LC1_VOLATILITY_10D  \\\n0                  18.38               18.96       68.475               9.598   \n1                  18.25               18.92       68.475               9.338   \n2                  18.21               18.90       69.700              13.451   \n3                  18.84               15.72       69.075              14.473   \n4                  19.99               16.30       68.975              14.386   \n...                  ...                 ...          ...                 ...   \n6246               30.36               26.20      165.450              24.832   \n6247               30.30               26.24      166.975              23.195   \n6248               31.51               26.30      167.700              23.428   \n6249               31.24               26.23      166.950              23.374   \n6250               29.93               26.27      166.950              23.374   \n\n      LC1_VOLATILITY_20D  \n0                  9.340  \n1                  9.343  \n2                 11.522  \n3                 11.736  \n4                 11.267  \n...                  ...  \n6246              20.781  \n6247              21.094  \n6248              21.047  \n6249              20.393  \n6250              20.393  \n\n[6251 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dates</th>\n      <th>CL1_PX_LAST</th>\n      <th>CL1_VOLATILITY_10D</th>\n      <th>CL1_VOLATILITY_20D</th>\n      <th>NG1_PX_LAST</th>\n      <th>NG1_VOLATILITY_10D</th>\n      <th>NG1_VOLATILITY_20D</th>\n      <th>HO1_PX_LAST</th>\n      <th>HO1_VOLATILITY_10D</th>\n      <th>HO1_VOLATILITY_20D</th>\n      <th>...</th>\n      <th>HG1_VOLATILITY_20D</th>\n      <th>GC1_PX_LAST</th>\n      <th>GC1_VOLATILITY_10D</th>\n      <th>GC1_VOLATILITY_20D</th>\n      <th>CT1_PX_LAST</th>\n      <th>CT1_VOLATILITY_10D</th>\n      <th>CT1_VOLATILITY_20D</th>\n      <th>LC1_PX_LAST</th>\n      <th>LC1_VOLATILITY_10D</th>\n      <th>LC1_VOLATILITY_20D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12/30/1999</td>\n      <td>25.60</td>\n      <td>30.94</td>\n      <td>32.43</td>\n      <td>2.329</td>\n      <td>46.714</td>\n      <td>60.137</td>\n      <td>69.03</td>\n      <td>21.16</td>\n      <td>28.16</td>\n      <td>...</td>\n      <td>22.09</td>\n      <td>289.6</td>\n      <td>6.59</td>\n      <td>15.48</td>\n      <td>50.74</td>\n      <td>18.38</td>\n      <td>18.96</td>\n      <td>68.475</td>\n      <td>9.598</td>\n      <td>9.340</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12/31/1999</td>\n      <td>25.60</td>\n      <td>30.94</td>\n      <td>32.43</td>\n      <td>2.329</td>\n      <td>46.714</td>\n      <td>60.137</td>\n      <td>69.03</td>\n      <td>21.16</td>\n      <td>28.16</td>\n      <td>...</td>\n      <td>22.09</td>\n      <td>289.6</td>\n      <td>6.59</td>\n      <td>15.48</td>\n      <td>50.74</td>\n      <td>18.25</td>\n      <td>18.92</td>\n      <td>68.475</td>\n      <td>9.338</td>\n      <td>9.343</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1/3/2000</td>\n      <td>25.60</td>\n      <td>30.94</td>\n      <td>32.43</td>\n      <td>2.329</td>\n      <td>46.714</td>\n      <td>60.137</td>\n      <td>69.03</td>\n      <td>21.16</td>\n      <td>28.16</td>\n      <td>...</td>\n      <td>22.09</td>\n      <td>289.6</td>\n      <td>6.59</td>\n      <td>15.48</td>\n      <td>51.07</td>\n      <td>18.21</td>\n      <td>18.90</td>\n      <td>69.700</td>\n      <td>13.451</td>\n      <td>11.522</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1/4/2000</td>\n      <td>25.55</td>\n      <td>30.98</td>\n      <td>32.44</td>\n      <td>2.176</td>\n      <td>52.823</td>\n      <td>62.008</td>\n      <td>67.78</td>\n      <td>23.14</td>\n      <td>29.10</td>\n      <td>...</td>\n      <td>22.94</td>\n      <td>283.7</td>\n      <td>14.06</td>\n      <td>16.17</td>\n      <td>50.73</td>\n      <td>18.84</td>\n      <td>15.72</td>\n      <td>69.075</td>\n      <td>14.473</td>\n      <td>11.736</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1/5/2000</td>\n      <td>24.91</td>\n      <td>32.87</td>\n      <td>30.97</td>\n      <td>2.168</td>\n      <td>53.389</td>\n      <td>59.658</td>\n      <td>66.55</td>\n      <td>24.18</td>\n      <td>29.17</td>\n      <td>...</td>\n      <td>23.07</td>\n      <td>282.1</td>\n      <td>14.34</td>\n      <td>15.50</td>\n      <td>51.56</td>\n      <td>19.99</td>\n      <td>16.30</td>\n      <td>68.975</td>\n      <td>14.386</td>\n      <td>11.267</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6246</th>\n      <td>12/8/2023</td>\n      <td>71.23</td>\n      <td>38.18</td>\n      <td>38.51</td>\n      <td>2.581</td>\n      <td>45.750</td>\n      <td>46.203</td>\n      <td>258.10</td>\n      <td>39.31</td>\n      <td>39.22</td>\n      <td>...</td>\n      <td>21.47</td>\n      <td>1998.3</td>\n      <td>20.41</td>\n      <td>15.55</td>\n      <td>81.44</td>\n      <td>30.36</td>\n      <td>26.20</td>\n      <td>165.450</td>\n      <td>24.832</td>\n      <td>20.781</td>\n    </tr>\n    <tr>\n      <th>6247</th>\n      <td>12/11/2023</td>\n      <td>71.32</td>\n      <td>35.20</td>\n      <td>37.92</td>\n      <td>2.431</td>\n      <td>52.322</td>\n      <td>43.354</td>\n      <td>260.87</td>\n      <td>36.00</td>\n      <td>36.83</td>\n      <td>...</td>\n      <td>20.77</td>\n      <td>1978.0</td>\n      <td>18.91</td>\n      <td>16.01</td>\n      <td>82.00</td>\n      <td>30.30</td>\n      <td>26.24</td>\n      <td>166.975</td>\n      <td>23.195</td>\n      <td>21.094</td>\n    </tr>\n    <tr>\n      <th>6248</th>\n      <td>12/12/2023</td>\n      <td>68.61</td>\n      <td>34.73</td>\n      <td>39.86</td>\n      <td>2.311</td>\n      <td>47.156</td>\n      <td>45.112</td>\n      <td>250.74</td>\n      <td>38.64</td>\n      <td>39.01</td>\n      <td>...</td>\n      <td>20.74</td>\n      <td>1977.8</td>\n      <td>18.58</td>\n      <td>15.74</td>\n      <td>81.05</td>\n      <td>31.51</td>\n      <td>26.30</td>\n      <td>167.700</td>\n      <td>23.428</td>\n      <td>21.047</td>\n    </tr>\n    <tr>\n      <th>6249</th>\n      <td>12/13/2023</td>\n      <td>69.47</td>\n      <td>36.71</td>\n      <td>40.09</td>\n      <td>2.335</td>\n      <td>49.072</td>\n      <td>43.268</td>\n      <td>254.81</td>\n      <td>42.04</td>\n      <td>39.38</td>\n      <td>...</td>\n      <td>20.50</td>\n      <td>1982.3</td>\n      <td>18.86</td>\n      <td>15.75</td>\n      <td>81.18</td>\n      <td>31.24</td>\n      <td>26.23</td>\n      <td>166.950</td>\n      <td>23.374</td>\n      <td>20.393</td>\n    </tr>\n    <tr>\n      <th>6250</th>\n      <td>12/14/2023</td>\n      <td>71.22</td>\n      <td>39.68</td>\n      <td>37.43</td>\n      <td>2.388</td>\n      <td>52.850</td>\n      <td>44.416</td>\n      <td>258.55</td>\n      <td>31.20</td>\n      <td>37.44</td>\n      <td>...</td>\n      <td>22.20</td>\n      <td>2033.0</td>\n      <td>22.25</td>\n      <td>17.81</td>\n      <td>81.68</td>\n      <td>29.93</td>\n      <td>26.27</td>\n      <td>166.950</td>\n      <td>23.374</td>\n      <td>20.393</td>\n    </tr>\n  </tbody>\n</table>\n<p>6251 rows Ã— 34 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-10T20:24:05.983861Z","iopub.execute_input":"2024-04-10T20:24:05.984225Z","iopub.status.idle":"2024-04-10T20:24:05.995601Z","shell.execute_reply.started":"2024-04-10T20:24:05.984196Z","shell.execute_reply":"2024-04-10T20:24:05.993949Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['Dates', 'CL1_PX_LAST', 'CL1_VOLATILITY_10D', 'CL1_VOLATILITY_20D',\n       'NG1_PX_LAST', 'NG1_VOLATILITY_10D', 'NG1_VOLATILITY_20D',\n       'HO1_PX_LAST', 'HO1_VOLATILITY_10D', 'HO1_VOLATILITY_20D', 'W1_PX_LAST',\n       'W1_VOLATILITY_10D', 'W1_VOLATILITY_20D', 'C1_PX_LAST',\n       'C1_VOLATILITY_10D', 'C1_VOLATILITY_20D', 'S1_PX_LAST',\n       'S1_VOLATILITY_10D', 'S1_VOLATILITY_20D', 'BO1_PX_LAST',\n       'BO1_VOLATILITY_10D', 'BO1_VOLATILITY_20D', 'HG1_PX_LAST',\n       'HG1_VOLATILITY_10D', 'HG1_VOLATILITY_20D', 'GC1_PX_LAST',\n       'GC1_VOLATILITY_10D', 'GC1_VOLATILITY_20D', 'CT1_PX_LAST',\n       'CT1_VOLATILITY_10D', 'CT1_VOLATILITY_20D', 'LC1_PX_LAST',\n       'LC1_VOLATILITY_10D', 'LC1_VOLATILITY_20D'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"df.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the 'Date' column to datetime format \ndf['Dates'] = pd.to_datetime(df['Dates'])\n\n# Add a column for the day of the week (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\ndf['DayOfWeek'] = df['Dates'].dt.dayofweek\n\n# Add a binary column indicating whether it's a weekend (1 = Saturday or Sunday, 0 = other days)\ndf['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n\n# Display the modified Data\nprint(df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ncolumns_of_interest = ['CL1_PX_LAST', 'NG1_PX_LAST', 'HO1_PX_LAST', 'W1_PX_LAST', 'C1_PX_LAST', 'S1_PX_LAST', 'BO1_PX_LAST', 'HG1_PX_LAST', 'GC1_PX_LAST', 'CT1_PX_LAST', 'LC1_PX_LAST']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Dates'] = df['Dates'].astype(int) / 10**9  # Convert nanoseconds to seconds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model LR","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    lr_model = LinearRegression()\n    lr_model.fit(X_train, y_train)\n    predictions = lr_model.predict(X_test)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    # Print evaluation metrics\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model RF","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    # Define the Random Forest regressor\n    rf_model = RandomForestRegressor()\n    # Specify hyperparameters to tune\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [None, 10, 20, 30],\n    }\n    # Perform grid search with cross-validation\n    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search.fit(X_train, y_train)\n    # Get the best hyperparameters\n    best_params = grid_search.best_params_\n    print(\"Best Hyperparameters:\", best_params)\n    # Use the best model for prediction\n    best_model = grid_search.best_estimator_\n    predictions = best_model.predict(X_test)\n    # Evaluate the model\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model DT","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    param_grid = {\n        'max_depth': [None, 10, 20, 30],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4]\n    }\n    dt_model = DecisionTreeRegressor()\n    # Perform GridSearchCV for hyperparameter tuning\n    grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n    grid_search.fit(X_train, y_train)\n    best_params = grid_search.best_params_\n    # Train the model with the best hyperparameters\n    best_dt_model = DecisionTreeRegressor(**best_params)\n    best_dt_model.fit(X_train, y_train)\n    predictions = best_dt_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print({target_column})\n    print(f\"Best Hyperparameters: {best_params}\")\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n    print(\"Mean Absolute Percentage Error:\", mape)    \n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model GBR","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    # Create the Gradient Boosting Regressor\n    gb_regressor = GradientBoostingRegressor(random_state=42)\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'learning_rate': [0.01, 0.1, 0.2],\n        'max_depth': [3, 4, 5],\n        'subsample': [0.8, 0.9, 1.0],\n    }\n    grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search.fit(X_train, y_train)\n    best_params = grid_search.best_params_\n    print(\"Best Hyperparameters:\", best_params)\n    final_gb_model = GradientBoostingRegressor(**best_params, random_state=42)\n    final_gb_model.fit(X_train, y_train)\n    # Make predictions on the test set\n    predictions = final_gb_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print({target_column})\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model kNN","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    # Normalize the data\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    param_grid = {\n    'n_neighbors': np.arange(1, 21),\n    'weights': ['uniform', 'distance']\n    }\n    knn_model = KNeighborsRegressor()\n    grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n    grid_search.fit(X_train, y_train)\n    best_knn_model = grid_search.best_estimator_\n    predictions = best_knn_model.predict(X_test)\n    # Evaluate the model\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)    \n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print({target_column})\n    print(\"Best Hyperparameters:\", grid_search.best_params_)\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n    print(\"Mean Absolute Percentage Error:\", mape)\n\n\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Models LSTM/GRU/DNN","metadata":{}},{"cell_type":"code","source":"class KerasLSTMRegressor(BaseEstimator):\n    def __init__(self, model_type='LSTM', units=50, epochs=50, batch_size=32, verbose=0):\n        self.model_type = model_type\n        self.units = units\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.model = self._build_model()\n\n    def _build_model(self):\n        model = Sequential()\n        if self.model_type == 'LSTM':\n            model.add(LSTM(units=self.units, input_shape=(1, 1)))\n        elif self.model_type == 'GRU':\n            model.add(GRU(units=self.units, input_shape=(1, 1)))\n        elif self.model_type == 'DNN':\n            model.add(Dense(units=self.units, input_shape=(1,)))\n            model.add(Dense(units=self.units))\n        model.add(Dense(units=1))\n        model.compile(optimizer='adam', loss='mean_squared_error')\n        return model\n\n    def fit(self, X, y):\n        X = X.reshape((X.shape[0], 1, 1))\n        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n        return self\n\n    def predict(self, X):\n        X = X.reshape((X.shape[0], 1, 1))\n        return self.model.predict(X)\n\n    def score(self, X, y):\n        X = X.reshape((X.shape[0], 1, 1))\n        return -self.model.evaluate(X, y, verbose=self.verbose)\n\ndef train_and_predict(df, target_column):\n    target = df[target_column].values.reshape(-1, 1)\n    scaler = MinMaxScaler()\n    scaled_target = scaler.fit_transform(target)\n    X_train, X_test, y_train, y_test = train_test_split(scaled_target[:-1], scaled_target[1:], test_size=0.2, random_state=42)\n\n    param_grid = {\n        'units': [50, 100, 150],  # Adjust units for LSTM, GRU, and DNN\n        'epochs': [50, 100, 150],  # Adjust epochs\n        'batch_size': [32, 64, 128]  # Adjust batch_size\n    }\n\n    # Create KerasRegressor for LSTM\n    keras_lstm_regressor = KerasLSTMRegressor(model_type='LSTM', epochs=50, batch_size=32, verbose=0)\n    grid_search_lstm = GridSearchCV(estimator=keras_lstm_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_lstm.fit(X_train, y_train)\n    # Get the best hyperparameters for LSTM\n    best_params_lstm = grid_search_lstm.best_params_\n    best_lstm_regressor = KerasLSTMRegressor(model_type='LSTM', units=best_params_lstm['units'], epochs=best_params_lstm['epochs'], batch_size=best_params_lstm['batch_size'], verbose=0)\n    best_lstm_regressor.fit(X_train, y_train)\n    predictions_lstm = best_lstm_regressor.predict(X_test)\n    mse_lstm = mean_squared_error(y_test, predictions_lstm)\n    mape_lstm = mean_absolute_percentage_error(y_test, predictions_lstm)\n    r2_lstm = r2_score(y_test, predictions_lstm)\n\n    # Create KerasRegressor for GRU\n    keras_gru_regressor = KerasLSTMRegressor(model_type='GRU', epochs=50, batch_size=32, verbose=0)\n    grid_search_gru = GridSearchCV(estimator=keras_gru_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_gru.fit(X_train, y_train)\n    # Get the best hyperparameters for GRU\n    best_params_gru = grid_search_gru.best_params_\n    best_gru_regressor = KerasLSTMRegressor(model_type='GRU', units=best_params_gru['units'], epochs=best_params_gru['epochs'], batch_size=best_params_gru['batch_size'], verbose=0)\n    best_gru_regressor.fit(X_train, y_train)\n    predictions_gru = best_gru_regressor.predict(X_test)\n    mape_gru = mean_absolute_percentage_error(y_test, predictions_gru)\n    mse_gru = mean_squared_error(y_test, predictions_gru)\n    r2_gru = r2_score(y_test, predictions_gru)\n\n    # Create KerasRegressor for DNN\n    keras_dnn_regressor = KerasLSTMRegressor(model_type='DNN', epochs=50, batch_size=32, verbose=0)\n    grid_search_dnn = GridSearchCV(estimator=keras_dnn_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_dnn.fit(X_train, y_train)\n    # Get the best hyperparameters for DNN\n    best_params_dnn = grid_search_dnn.best_params_\n    best_dnn_regressor = KerasLSTMRegressor(model_type='DNN', units=best_params_dnn['units'], epochs=best_params_dnn['epochs'], batch_size=best_params_dnn['batch_size'], verbose=0)\n    best_dnn_regressor.fit(X_train, y_train)\n    predictions_dnn = best_dnn_regressor.predict(X_test)\n    mape_dnn = mean_absolute_percentage_error(y_test, predictions_dnn)\n    mse_dnn = mean_squared_error(y_test, predictions_dnn)\n    r2_dnn = r2_score(y_test, predictions_dnn)\n\n    # Print the best parameters and results for LSTM, GRU, and DNN\n    print(f\"Target Column: {target_column}\")\n    print(\"Best parameters for LSTM:\", best_params_lstm)\n    print(f\"MSE for LSTM: {mse_lstm}, R-squared for LSTM: {r2_lstm}, MAPE:\", mape_lstm)\n    print(\"Best parameters for GRU:\", best_params_gru)\n    print(f\"MSE for GRU: {mse_gru}, R-squared for GRU: {r2_gru}, MAPE:\", mape_gru)\n    print(\"Best parameters for DNN:\", best_params_dnn)\n    print(f\"MSE for DNN: {mse_dnn}, R-squared for DNN: {r2_dnn}, MAPE:\", mape_dnn)\n    print()\n    \nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction sentiment price","metadata":{}},{"cell_type":"code","source":"!pip install praw\nimport praw\nfrom transformers import pipeline, RobertaForSequenceClassification, RobertaTokenizer, BertForSequenceClassification, BertTokenizer, DistilBertForSequenceClassification, DistilBertTokenizer\n\n# Reddit API credentials\nreddit_client_id = 'bfr--CssnTdgrFGIwK_prQ'\nreddit_client_secret = 'FPm-1pUWywIqnCjTo8enZwtg9lxtbQ'\nreddit_user_agent = 'MytestApp/1.0 by IndependenceNew2283'\n\n# Authenticate with Reddit API using PRAW\nreddit = praw.Reddit(client_id=reddit_client_id,\n                     client_secret=reddit_client_secret,\n                     user_agent=reddit_user_agent)\n\n# Define the subreddit and keywords related to commodities\nsubreddit_name = 'commodities'\nkeywords = ['gold', 'silver', 'oil', 'copper', 'soybean', 'cotton', 'cattle', 'cotton', 'wheat', 'corn', 'gas','commodity','commodities','stock market']\n\n# Fetch submissions from the subreddit and filter by keywords\nsubreddit = reddit.subreddit(subreddit_name)\n\n# Define the number of submissions to fetch (400 000 lines)\ntotal_submissions = 400000\nbatch_size = 100\n\n# Extract data from submissions\ndata = {'title': [], 'body': [], 'created_utc': []}\nfor _ in range(total_submissions // batch_size):\n    submissions = subreddit.search(' OR '.join(keywords), sort='new', limit=batch_size)\n    for submission in submissions:\n        data['title'].append(submission.title)\n        data['body'].append(submission.selftext)\n        data['created_utc'].append(submission.created_utc)\n\n# Create a DataFrame from the extracted data\nreddit_data = pd.DataFrame(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load sentiment analysis models and tokenizers\nroberta_sentiment_model = pipeline('sentiment-analysis', model=RobertaForSequenceClassification.from_pretrained('roberta-base'), tokenizer=RobertaTokenizer.from_pretrained('roberta-base'))\nbert_sentiment_model = pipeline('sentiment-analysis', model=BertForSequenceClassification.from_pretrained('bert-base-uncased'), tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\ndistilbert_sentiment_model = pipeline('sentiment-analysis', model=DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased'), tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased'))\n\nmax_seq_length = 512\nreddit_data['body'] = reddit_data['body'].apply(lambda x: x[:max_seq_length])\n\n# Apply sentiment analysis using RoBERTa, BERT, and DistilBERT\nreddit_data['title_sentiment_roberta'] = reddit_data['title'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\nreddit_data['body_sentiment_roberta'] = reddit_data['body'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\n\nreddit_data['title_sentiment_bert'] = reddit_data['title'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\nreddit_data['body_sentiment_bert'] = reddit_data['body'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\n\nreddit_data['title_sentiment_distilbert'] = reddit_data['title'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\nreddit_data['body_sentiment_distilbert'] = reddit_data['body'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\n\nprint(reddit_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_data['created_utc'] = pd.to_datetime(reddit_data['created_utc'], unit='s', utc=True)\nreddit_data['created_utc'] = reddit_data['created_utc'].dt.tz_convert('GMT')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#label_mapping = {'LABEL_0': 'Negative', 'LABEL_1': 'Neutral', 'LABEL_2': 'Positive'}\nlabel_mapping = {'LABEL_0': '0', 'LABEL_1': '-1', 'LABEL_2': '1'}\n\nfor model in ['roberta', 'bert', 'distilbert']:\n    reddit_data[f'title_sentiment_{model}'] = reddit_data[f'title_sentiment_{model}'].map(label_mapping)\n    reddit_data[f'body_sentiment_{model}'] = reddit_data[f'body_sentiment_{model}'].map(label_mapping)\n\n# Visualize Sentiment Distribution\n#labels = ['Negative', 'Neutral', 'Positive']\nlabels = ['-1', '0', '1']\n# Plotting\nfig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\nbar_width = 0.35  # Adjust the width of the bars\nfor i, model in enumerate(['roberta', 'bert', 'distilbert']):\n    # Title Sentiment\n    title_sentiment_counts = reddit_data[f'title_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar(labels, title_sentiment_counts, color='skyblue', label=f'Title ({model.capitalize()})', width=bar_width)\n\n    # Body Sentiment\n    body_sentiment_counts = reddit_data[f'body_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar([x + bar_width for x in range(len(labels))], body_sentiment_counts, color='#CBC3E3', alpha=0.7, label=f'Body ({model.capitalize()})', width=bar_width)\n\n    axs[i].set_title(f'{model.capitalize()} Sentiment Distribution')\n    axs[i].legend()  # Add legend to each subplot\n\nfig.suptitle('Sentiment Analysis on Reddit Data')\nplt.tight_layout()\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the DataFrame to a CSV file\nreddit_data.to_csv('reddit_data.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nprice_data = pd.read_csv('commodities_DAILY.csv')\nsentiment_data = pd.read_csv('reddit_data.csv')\n\n# Convert date columns to datetime format\nprice_data['Dates'] = pd.to_datetime(price_data['Dates'])\nsentiment_data['created_utc'] = pd.to_datetime(sentiment_data['created_utc'])\n\n# Convert 'created_utc' column in sentiment_data\nsentiment_data['created_utc'] = pd.to_datetime(sentiment_data['created_utc'], unit='s')\n# Extract date part only from 'created_utc'\nsentiment_data['created_utc'] = sentiment_data['created_utc'].dt.date\n# Convert 'Date' to the same format as 'created_date'\nprice_data['Dates'] = price_data['Dates'].dt.date\n# Verify the conversion\nprint(price_data.head())\n# Merge on the date part\nmerged_data = pd.merge(sentiment_data, price_data, left_on='created_utc', right_on='Dates', how='left')\n\nfor colonne in merged_data.columns:\n    merged_data = merged_data.dropna(subset=[colonne])\n\n# Define columns of interest\ncolumns_of_interest = ['CL1_PX_LAST', 'NG1_PX_LAST', 'HO1_PX_LAST', 'W1_PX_LAST', 'C1_PX_LAST', 'S1_PX_LAST',\n                        'BO1_PX_LAST', 'HG1_PX_LAST', 'GC1_PX_LAST', 'CT1_PX_LAST', 'LC1_PX_LAST']\n\ndef train_and_predict(df, target_column):\n\n    # Select features and target variable\n    features = merged_data.drop(['title', 'body', 'created_utc'] + [target_column], axis=1)\n    target = merged_data[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\n    # Define preprocessing steps\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('sentiment', SimpleImputer(strategy='constant', fill_value=0),\n                ['title_sentiment_roberta', 'title_sentiment_bert', 'title_sentiment_distilbert']),\n            ('numeric', StandardScaler(),\n                ['CL1_VOLATILITY_10D', 'CL1_VOLATILITY_20D', 'NG1_VOLATILITY_10D', 'NG1_VOLATILITY_20D',\n                 'HO1_VOLATILITY_10D', 'HO1_VOLATILITY_20D', 'W1_VOLATILITY_10D',\n                 'W1_VOLATILITY_20D', 'C1_VOLATILITY_10D', 'C1_VOLATILITY_20D',\n                 'S1_VOLATILITY_10D', 'S1_VOLATILITY_20D', 'BO1_VOLATILITY_10D', 'BO1_VOLATILITY_20D',\n                 'HG1_VOLATILITY_10D', 'HG1_VOLATILITY_20D', 'GC1_VOLATILITY_10D',\n                 'GC1_VOLATILITY_20D', 'CT1_VOLATILITY_10D', 'CT1_VOLATILITY_20D',\n                 'LC1_VOLATILITY_10D', 'LC1_VOLATILITY_20D']),\n        ])\n\n    # Define the models\n    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n    dt_model = DecisionTreeRegressor(random_state=42)\n\n    # Create and evaluate pipelines for each model\n    rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                   ('model', rf_model)])\n\n    dt_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                   ('model', dt_model)])\n\n    # Fit and evaluate Random Forest model\n    rf_pipeline.fit(X_train, y_train)\n    rf_y_pred = rf_pipeline.predict(X_test)\n    rf_mse = mean_squared_error(y_test, rf_y_pred)\n    rf_r2 = r2_score(y_test, rf_y_pred)\n    print(f'{target_column} - Random Forest - Mean Squared Error: {rf_mse}')\n    print(f'{target_column} - Random Forest - R-squared (R2) Score: {rf_r2}')\n\n    # Fit and evaluate Decision Tree model\n    dt_pipeline.fit(X_train, y_train)\n    dt_y_pred = dt_pipeline.predict(X_test)\n    dt_mse = mean_squared_error(y_test, dt_y_pred)\n    dt_r2 = r2_score(y_test, dt_y_pred)\n    print(f'{target_column} - Decision Tree - Mean Squared Error: {dt_mse}')\n    print(f'{target_column} - Decision Tree - R-squared (R2) Score: {dt_r2}')\n\n\n    print('-' * 50)\n\n\nfor column in columns_of_interest:\n    train_and_predict(merged_data, column)\n\ndef train_and_predict_lstm(df, target_column):\n    features = merged_data.drop(['title', 'body', 'created_utc'] + [target_column], axis=1)\n    target = merged_data[target_column]\n    features_array = features.select_dtypes(include=[np.number]).values\n    target_array = target.values.reshape(-1, 1)\n\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features_array)\n\n    X_train, X_test, y_train, y_test = train_test_split(features_scaled, target_array, test_size=0.2, random_state=42)\n\n    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n\n    # Define LSTM model\n    model = Sequential()\n    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(LSTM(units=50))\n    model.add(Dense(units=1))\n\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    print(f'{target_column} - LSTM - Mean Squared Error: {mse}')\n    print(f'{target_column} - LSTM - R-squared (R2) Score: {r2}')\n    print('-' * 50)\n\nfor column in columns_of_interest:\n    train_and_predict_lstm(merged_data, column)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install feedparser\nimport feedparser\nimport urllib.parse\n\n# Define keywords related to commodities\nkeywords = ['gold', 'silver', 'oil', 'copper', 'soybean', 'cotton', 'cattle', 'cotton', 'wheat', 'corn', 'gas', 'commodity', 'commodities', 'stock market']\n\n# Fetch news articles related to commodities from Google News (500 000 lines) \nnews_data = {'title': [], 'summary': [], 'published': []}\ntotal_lines = 500000\nlines_fetched = 0\nbatch_size = 100\n\nwhile lines_fetched < total_lines:\n    for keyword in keywords:\n        # Encode keyword for URL\n        encoded_keyword = urllib.parse.quote_plus(keyword)\n        feed_url = f'https://news.google.com/rss/search?q={encoded_keyword}&hl=en-US&gl=US&ceid=US:en'\n        feed = feedparser.parse(feed_url)\n        for entry in feed.entries:\n            news_data['title'].append(entry.title)\n            news_data['summary'].append(entry.summary)\n            news_data['published'].append(entry.published)\n            lines_fetched += 1\n            if lines_fetched >= total_lines:\n                break\n        if lines_fetched >= total_lines:\n            break\n    if lines_fetched >= total_lines:\n        break\n\n# Create a DataFrame from the extracted news data\nnews_df = pd.DataFrame(news_data)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load sentiment analysis models and tokenizers\nroberta_sentiment_model = pipeline('sentiment-analysis', model=RobertaForSequenceClassification.from_pretrained('roberta-base'), tokenizer=RobertaTokenizer.from_pretrained('roberta-base'))\nbert_sentiment_model = pipeline('sentiment-analysis', model=BertForSequenceClassification.from_pretrained('bert-base-uncased'), tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\ndistilbert_sentiment_model = pipeline('sentiment-analysis', model=DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased'), tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased'))\n\n# Truncate the 'summary' text to fit within the model's maximum sequence length\nmax_seq_length = 512\nnews_df['summary'] = news_df['summary'].apply(lambda x: x[:max_seq_length])\n\n# Apply sentiment analysis using RoBERTa, BERT, and DistilBERT\nnews_df['title_sentiment_roberta'] = news_df['title'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\nnews_df['summary_sentiment_roberta'] = news_df['summary'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\n\nnews_df['title_sentiment_bert'] = news_df['title'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\nnews_df['summary_sentiment_bert'] = news_df['summary'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\n\nnews_df['title_sentiment_distilbert'] = news_df['title'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\nnews_df['summary_sentiment_distilbert'] = news_df['summary'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\n\n# Display the DataFrame with sentiment analysis results\nprint(news_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map labels for sentiment categories\nlabel_mapping = {'LABEL_0': '-1', 'LABEL_1': '0', 'LABEL_2': '1'}\n\n# Map sentiment labels in the DataFrame\nfor model in ['roberta', 'bert', 'distilbert']:\n    news_df[f'title_sentiment_{model}'] = news_df[f'title_sentiment_{model}'].map(label_mapping)\n    news_df[f'summary_sentiment_{model}'] = news_df[f'summary_sentiment_{model}'].map(label_mapping)\n\n# Visualize Sentiment Distribution\nlabels = ['-1', '0', '1']\n\nfig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\nbar_width = 0.35\nfor i, model in enumerate(['roberta', 'bert', 'distilbert']):\n    title_sentiment_counts = news_df[f'title_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar(labels, title_sentiment_counts, color='skyblue', label=f'Title ({model.capitalize()})', width=bar_width)\n    summary_sentiment_counts = news_df[f'summary_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar([x + bar_width for x in range(len(labels))], summary_sentiment_counts, color='#CBC3E3', alpha=0.7, label=f'Summary ({model.capitalize()})', width=bar_width)\n\n    axs[i].set_title(f'{model.capitalize()} Sentiment Distribution')\n    axs[i].legend()\n\nfig.suptitle('Sentiment Analysis on Google News Data')\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_df.to_csv('news_df.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"price_data = pd.read_csv('commodities_DAILY.csv')\nsentiment_data = pd.read_csv('news_df.csv')\n\n# Convert 'published' column in sentiment_data\nsentiment_data['published'] = pd.to_datetime(sentiment_data['published'], unit='s')\n# Extract date part only from 'published'\nsentiment_data['published'] = sentiment_data['published'].dt.date\n# Convert 'Dates' to the same format as 'published'\nprice_data['Dates'] = price_data['Dates'].dt.date\n# Verify the conversion\nprint(price_data.head())\n# Merge on the date part\nmerged_data = pd.merge(sentiment_data, price_data, left_on='published', right_on='Dates', how='left')\n# Check columns of the merged data\nprint(\"merged_data columns:\", merged_data.columns)\n\ncolumns_of_interest = ['CL1_PX_LAST', 'NG1_PX_LAST', 'HO1_PX_LAST', 'W1_PX_LAST', 'C1_PX_LAST', 'S1_PX_LAST',\n                        'BO1_PX_LAST', 'HG1_PX_LAST', 'GC1_PX_LAST', 'CT1_PX_LAST', 'LC1_PX_LAST']\n\n# Dropping rows with missing values in sentiment columns if any\nmerged_data.dropna(subset=columns_of_interest, inplace=True)\n\ndef train_and_predict(df, target_column):\n    features = merged_data.drop(['title', 'summary', 'published'] + [target_column], axis=1)\n    target = merged_data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('sentiment', SimpleImputer(strategy='constant', fill_value=0),\n                ['title_sentiment_roberta', 'title_sentiment_bert', 'title_sentiment_distilbert']),\n            ('numeric', StandardScaler(),\n                ['CL1_VOLATILITY_10D', 'CL1_VOLATILITY_20D', 'NG1_VOLATILITY_10D', 'NG1_VOLATILITY_20D',\n                 'HO1_VOLATILITY_10D', 'HO1_VOLATILITY_20D', 'W1_VOLATILITY_10D',\n                 'W1_VOLATILITY_20D', 'C1_VOLATILITY_10D', 'C1_VOLATILITY_20D',\n                 'S1_VOLATILITY_10D', 'S1_VOLATILITY_20D', 'BO1_VOLATILITY_10D', 'BO1_VOLATILITY_20D',\n                 'HG1_VOLATILITY_10D', 'HG1_VOLATILITY_20D', 'GC1_VOLATILITY_10D',\n                 'GC1_VOLATILITY_20D', 'CT1_VOLATILITY_10D', 'CT1_VOLATILITY_20D',\n                 'LC1_VOLATILITY_10D', 'LC1_VOLATILITY_20D']),\n        ])\n\n    rf_model = RandomForestRegressor(n_estimators=200, random_state=42)\n    dt_model = DecisionTreeRegressor(random_state=42)\n    rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                   ('model', rf_model)])\n\n    dt_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                   ('model', dt_model)])\n\n    rf_pipeline.fit(X_train, y_train)\n    rf_y_pred = rf_pipeline.predict(X_test)\n    rf_mse = mean_squared_error(y_test, rf_y_pred)\n    rf_r2 = r2_score(y_test, rf_y_pred)\n    print(f'{target_column} - Random Forest - Mean Squared Error: {rf_mse}')\n    print(f'{target_column} - Random Forest - R-squared (R2) Score: {rf_r2}')\n\n    dt_pipeline.fit(X_train, y_train)\n    dt_y_pred = dt_pipeline.predict(X_test)\n    dt_mse = mean_squared_error(y_test, dt_y_pred)\n    dt_r2 = r2_score(y_test, dt_y_pred)\n    print(f'{target_column} - Decision Tree - Mean Squared Error: {dt_mse}')\n    print(f'{target_column} - Decision Tree - R-squared (R2) Score: {dt_r2}')\n\n    print('-' * 50)\n\nfor column in columns_of_interest:\n    train_and_predict(merged_data, column)\n\n\ndef train_and_predict_lstm(df, target_column):\n    features = merged_data.drop(['title', 'summary', 'published'] + [target_column], axis=1)\n    target = merged_data[target_column]\n\n    # Convert to numpy arrays\n    features_array = features.select_dtypes(include=[np.number]).values\n    target_array = target.values.reshape(-1, 1)\n\n    # Normalize features\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features_array)\n    X_train, X_test, y_train, y_test = train_test_split(features_scaled, target_array, test_size=0.2, random_state=42)\n\n    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n\n    # Define LSTM model\n    model = Sequential()\n    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(LSTM(units=50))\n    model.add(Dense(units=1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    print(f'{target_column} - LSTM - Mean Squared Error: {mse}')\n    print(f'{target_column} - LSTM - R-squared (R2) Score: {r2}')\n    print('-' * 50)\n\nfor column in columns_of_interest:\n    train_and_predict_lstm(merged_data, column)","metadata":{},"execution_count":null,"outputs":[]}]}