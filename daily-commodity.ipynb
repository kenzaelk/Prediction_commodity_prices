{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7217994,"sourceType":"datasetVersion","datasetId":4177443}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport os\nimport numpy as np \nimport pandas as pd \nimport requests\nimport pandas_datareader as web\n\n# Date\nimport datetime as dt\nfrom datetime import date, timedelta, datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error,explained_variance_score, r2_score, mean_absolute_percentage_error\nimport math\n\n\n# Modeling and preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom tensorflow.keras.layers import GRU, Dense\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom sklearn.base import BaseEstimator\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Daily data forecast ","metadata":{}},{"cell_type":"code","source":"#Daily dataset\ndf = pd.read_csv('commodities_DAILY.csv', parse_dates=True)\ndisplay(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the 'Date' column to datetime format \ndf['Dates'] = pd.to_datetime(df['Dates'])\n\n# Add a column for the day of the week (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\ndf['DayOfWeek'] = df['Dates'].dt.dayofweek\n\n# Add a binary column indicating whether it's a weekend (1 = Saturday or Sunday, 0 = other days)\ndf['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n\n# Display the modified Data\nprint(df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ncolumns_of_interest = ['CL1_PX_LAST', 'NG1_PX_LAST', 'HO1_PX_LAST', 'W1_PX_LAST', 'C1_PX_LAST', 'S1_PX_LAST', 'BO1_PX_LAST', 'HG1_PX_LAST', 'GC1_PX_LAST', 'CT1_PX_LAST', 'LC1_PX_LAST']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Dates'] = df['Dates'].astype(int) / 10**9  # Convert nanoseconds to seconds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model LR","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    lr_model = LinearRegression()\n    lr_model.fit(X_train, y_train)\n    predictions = lr_model.predict(X_test)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    # Print evaluation metrics\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model RF","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    # Define the Random Forest regressor\n    rf_model = RandomForestRegressor()\n    # Specify hyperparameters to tune\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [None, 10, 20, 30],\n    }\n    # Perform grid search with cross-validation\n    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search.fit(X_train, y_train)\n    # Get the best hyperparameters\n    best_params = grid_search.best_params_\n    print(\"Best Hyperparameters:\", best_params)\n    # Use the best model for prediction\n    best_model = grid_search.best_estimator_\n    predictions = best_model.predict(X_test)\n    # Evaluate the model\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model DT","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    param_grid = {\n        'max_depth': [None, 10, 20, 30],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4]\n    }\n    dt_model = DecisionTreeRegressor()\n    # Perform GridSearchCV for hyperparameter tuning\n    grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n    grid_search.fit(X_train, y_train)\n    best_params = grid_search.best_params_\n    # Train the model with the best hyperparameters\n    best_dt_model = DecisionTreeRegressor(**best_params)\n    best_dt_model.fit(X_train, y_train)\n    predictions = best_dt_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print({target_column})\n    print(f\"Best Hyperparameters: {best_params}\")\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n    print(\"Mean Absolute Percentage Error:\", mape)    \n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model GBR","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    # Create the Gradient Boosting Regressor\n    gb_regressor = GradientBoostingRegressor(random_state=42)\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'learning_rate': [0.01, 0.1, 0.2],\n        'max_depth': [3, 4, 5],\n        'subsample': [0.8, 0.9, 1.0],\n    }\n    grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search.fit(X_train, y_train)\n    best_params = grid_search.best_params_\n    print(\"Best Hyperparameters:\", best_params)\n    final_gb_model = GradientBoostingRegressor(**best_params, random_state=42)\n    final_gb_model.fit(X_train, y_train)\n    # Make predictions on the test set\n    predictions = final_gb_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print({target_column})\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model kNN","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    # Normalize the data\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    param_grid = {\n    'n_neighbors': np.arange(1, 21),\n    'weights': ['uniform', 'distance']\n    }\n    knn_model = KNeighborsRegressor()\n    grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n    grid_search.fit(X_train, y_train)\n    best_knn_model = grid_search.best_estimator_\n    predictions = best_knn_model.predict(X_test)\n    # Evaluate the model\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)    \n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print({target_column})\n    print(\"Best Hyperparameters:\", grid_search.best_params_)\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n    print(\"Mean Absolute Percentage Error:\", mape)\n\n\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Models LSTM/GRU/DNN","metadata":{}},{"cell_type":"code","source":"class KerasLSTMRegressor(BaseEstimator):\n    def __init__(self, model_type='LSTM', units=50, epochs=50, batch_size=32, verbose=0):\n        self.model_type = model_type\n        self.units = units\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.model = self._build_model()\n\n    def _build_model(self):\n        model = Sequential()\n        if self.model_type == 'LSTM':\n            model.add(LSTM(units=self.units, input_shape=(1, 1)))\n        elif self.model_type == 'GRU':\n            model.add(GRU(units=self.units, input_shape=(1, 1)))\n        elif self.model_type == 'DNN':\n            model.add(Dense(units=self.units, input_shape=(1,)))\n            model.add(Dense(units=self.units))\n        model.add(Dense(units=1))\n        model.compile(optimizer='adam', loss='mean_squared_error')\n        return model\n\n    def fit(self, X, y):\n        X = X.reshape((X.shape[0], 1, 1))\n        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n        return self\n\n    def predict(self, X):\n        X = X.reshape((X.shape[0], 1, 1))\n        return self.model.predict(X)\n\n    def score(self, X, y):\n        X = X.reshape((X.shape[0], 1, 1))\n        return -self.model.evaluate(X, y, verbose=self.verbose)\n\ndef train_and_predict(df, target_column):\n    target = df[target_column].values.reshape(-1, 1)\n    scaler = MinMaxScaler()\n    scaled_target = scaler.fit_transform(target)\n    X_train, X_test, y_train, y_test = train_test_split(scaled_target[:-1], scaled_target[1:], test_size=0.2, random_state=42)\n\n    param_grid = {\n        'units': [50, 100, 150],  # Adjust units for LSTM, GRU, and DNN\n        'epochs': [50, 100, 150],  # Adjust epochs\n        'batch_size': [32, 64, 128]  # Adjust batch_size\n    }\n\n    # Create KerasRegressor for LSTM\n    keras_lstm_regressor = KerasLSTMRegressor(model_type='LSTM', epochs=50, batch_size=32, verbose=0)\n    grid_search_lstm = GridSearchCV(estimator=keras_lstm_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_lstm.fit(X_train, y_train)\n    # Get the best hyperparameters for LSTM\n    best_params_lstm = grid_search_lstm.best_params_\n    best_lstm_regressor = KerasLSTMRegressor(model_type='LSTM', units=best_params_lstm['units'], epochs=best_params_lstm['epochs'], batch_size=best_params_lstm['batch_size'], verbose=0)\n    best_lstm_regressor.fit(X_train, y_train)\n    predictions_lstm = best_lstm_regressor.predict(X_test)\n    mse_lstm = mean_squared_error(y_test, predictions_lstm)\n    mape_lstm = mean_absolute_percentage_error(y_test, predictions_lstm)\n    r2_lstm = r2_score(y_test, predictions_lstm)\n\n    # Create KerasRegressor for GRU\n    keras_gru_regressor = KerasLSTMRegressor(model_type='GRU', epochs=50, batch_size=32, verbose=0)\n    grid_search_gru = GridSearchCV(estimator=keras_gru_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_gru.fit(X_train, y_train)\n    # Get the best hyperparameters for GRU\n    best_params_gru = grid_search_gru.best_params_\n    best_gru_regressor = KerasLSTMRegressor(model_type='GRU', units=best_params_gru['units'], epochs=best_params_gru['epochs'], batch_size=best_params_gru['batch_size'], verbose=0)\n    best_gru_regressor.fit(X_train, y_train)\n    predictions_gru = best_gru_regressor.predict(X_test)\n    mape_gru = mean_absolute_percentage_error(y_test, predictions_gru)\n    mse_gru = mean_squared_error(y_test, predictions_gru)\n    r2_gru = r2_score(y_test, predictions_gru)\n\n    # Create KerasRegressor for DNN\n    keras_dnn_regressor = KerasLSTMRegressor(model_type='DNN', epochs=50, batch_size=32, verbose=0)\n    grid_search_dnn = GridSearchCV(estimator=keras_dnn_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_dnn.fit(X_train, y_train)\n    # Get the best hyperparameters for DNN\n    best_params_dnn = grid_search_dnn.best_params_\n    best_dnn_regressor = KerasLSTMRegressor(model_type='DNN', units=best_params_dnn['units'], epochs=best_params_dnn['epochs'], batch_size=best_params_dnn['batch_size'], verbose=0)\n    best_dnn_regressor.fit(X_train, y_train)\n    predictions_dnn = best_dnn_regressor.predict(X_test)\n    mape_dnn = mean_absolute_percentage_error(y_test, predictions_dnn)\n    mse_dnn = mean_squared_error(y_test, predictions_dnn)\n    r2_dnn = r2_score(y_test, predictions_dnn)\n\n    # Print the best parameters and results for LSTM, GRU, and DNN\n    print(f\"Target Column: {target_column}\")\n    print(\"Best parameters for LSTM:\", best_params_lstm)\n    print(f\"MSE for LSTM: {mse_lstm}, R-squared for LSTM: {r2_lstm}, MAPE:\", mape_lstm)\n    print(\"Best parameters for GRU:\", best_params_gru)\n    print(f\"MSE for GRU: {mse_gru}, R-squared for GRU: {r2_gru}, MAPE:\", mape_gru)\n    print(\"Best parameters for DNN:\", best_params_dnn)\n    print(f\"MSE for DNN: {mse_dnn}, R-squared for DNN: {r2_dnn}, MAPE:\", mape_dnn)\n    print()\n    \nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction sentiment price","metadata":{}},{"cell_type":"code","source":"!pip install praw\nimport praw\nfrom transformers import pipeline, RobertaForSequenceClassification, RobertaTokenizer, BertForSequenceClassification, BertTokenizer, DistilBertForSequenceClassification, DistilBertTokenizer\n\n# Reddit API credentials\nreddit_client_id = 'bfr--CssnTdgrFGIwK_prQ'\nreddit_client_secret = 'FPm-1pUWywIqnCjTo8enZwtg9lxtbQ'\nreddit_user_agent = 'MytestApp/1.0 by IndependenceNew2283'\n\n# Authenticate with Reddit API using PRAW\nreddit = praw.Reddit(client_id=reddit_client_id,\n                     client_secret=reddit_client_secret,\n                     user_agent=reddit_user_agent)\n\n# Define the subreddit and keywords related to commodities\nsubreddit_name = 'commodities'\nkeywords = ['gold', 'silver', 'oil', 'copper', 'soybean', 'cotton', 'cattle', 'cotton', 'wheat', 'corn', 'gas','commodity','commodities','stock market']\n\n# Fetch submissions from the subreddit and filter by keywords\nsubreddit = reddit.subreddit(subreddit_name)\n\n# Define the number of submissions to fetch (400 000 lines)\ntotal_submissions = 400000\nbatch_size = 100\n\n# Extract data from submissions\ndata = {'title': [], 'body': [], 'created_utc': []}\nfor _ in range(total_submissions // batch_size):\n    submissions = subreddit.search(' OR '.join(keywords), sort='new', limit=batch_size)\n    for submission in submissions:\n        data['title'].append(submission.title)\n        data['body'].append(submission.selftext)\n        data['created_utc'].append(submission.created_utc)\n\n# Create a DataFrame from the extracted data\nreddit_data = pd.DataFrame(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load sentiment analysis models and tokenizers\nroberta_sentiment_model = pipeline('sentiment-analysis', model=RobertaForSequenceClassification.from_pretrained('roberta-base'), tokenizer=RobertaTokenizer.from_pretrained('roberta-base'))\nbert_sentiment_model = pipeline('sentiment-analysis', model=BertForSequenceClassification.from_pretrained('bert-base-uncased'), tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\ndistilbert_sentiment_model = pipeline('sentiment-analysis', model=DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased'), tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased'))\n\nmax_seq_length = 512\nreddit_data['body'] = reddit_data['body'].apply(lambda x: x[:max_seq_length])\n\n# Apply sentiment analysis using RoBERTa, BERT, and DistilBERT\nreddit_data['title_sentiment_roberta'] = reddit_data['title'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\nreddit_data['body_sentiment_roberta'] = reddit_data['body'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\n\nreddit_data['title_sentiment_bert'] = reddit_data['title'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\nreddit_data['body_sentiment_bert'] = reddit_data['body'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\n\nreddit_data['title_sentiment_distilbert'] = reddit_data['title'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\nreddit_data['body_sentiment_distilbert'] = reddit_data['body'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\n\nprint(reddit_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_data['created_utc'] = pd.to_datetime(reddit_data['created_utc'], unit='s', utc=True)\nreddit_data['created_utc'] = reddit_data['created_utc'].dt.tz_convert('GMT')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#label_mapping = {'LABEL_0': 'Negative', 'LABEL_1': 'Neutral', 'LABEL_2': 'Positive'}\nlabel_mapping = {'LABEL_0': '0', 'LABEL_1': '-1', 'LABEL_2': '1'}\n\nfor model in ['roberta', 'bert', 'distilbert']:\n    reddit_data[f'title_sentiment_{model}'] = reddit_data[f'title_sentiment_{model}'].map(label_mapping)\n    reddit_data[f'body_sentiment_{model}'] = reddit_data[f'body_sentiment_{model}'].map(label_mapping)\n\n# Visualize Sentiment Distribution\n#labels = ['Negative', 'Neutral', 'Positive']\nlabels = ['-1', '0', '1']\n# Plotting\nfig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\nbar_width = 0.35  # Adjust the width of the bars\nfor i, model in enumerate(['roberta', 'bert', 'distilbert']):\n    # Title Sentiment\n    title_sentiment_counts = reddit_data[f'title_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar(labels, title_sentiment_counts, color='skyblue', label=f'Title ({model.capitalize()})', width=bar_width)\n\n    # Body Sentiment\n    body_sentiment_counts = reddit_data[f'body_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar([x + bar_width for x in range(len(labels))], body_sentiment_counts, color='#CBC3E3', alpha=0.7, label=f'Body ({model.capitalize()})', width=bar_width)\n\n    axs[i].set_title(f'{model.capitalize()} Sentiment Distribution')\n    axs[i].legend()  # Add legend to each subplot\n\nfig.suptitle('Sentiment Analysis on Reddit Data')\nplt.tight_layout()\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the DataFrame to a CSV file\nreddit_data.to_csv('reddit_data.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nprice_data = pd.read_csv('commodities_DAILY.csv')\nsentiment_data = pd.read_csv('reddit_data.csv')\n\n# Convert date columns to datetime format\nprice_data['Dates'] = pd.to_datetime(price_data['Dates'])\nsentiment_data['created_utc'] = pd.to_datetime(sentiment_data['created_utc'])\n\n# Convert 'created_utc' column in sentiment_data\nsentiment_data['created_utc'] = pd.to_datetime(sentiment_data['created_utc'], unit='s')\n# Extract date part only from 'created_utc'\nsentiment_data['created_utc'] = sentiment_data['created_utc'].dt.date\n# Convert 'Date' to the same format as 'created_date'\nprice_data['Dates'] = price_data['Dates'].dt.date\n# Verify the conversion\nprint(price_data.head())\n# Merge on the date part\nmerged_data = pd.merge(sentiment_data, price_data, left_on='created_utc', right_on='Dates', how='left')\n\nfor colonne in merged_data.columns:\n    merged_data = merged_data.dropna(subset=[colonne])\n\n# Define columns of interest\ncolumns_of_interest = ['CL1_PX_LAST', 'NG1_PX_LAST', 'HO1_PX_LAST', 'W1_PX_LAST', 'C1_PX_LAST', 'S1_PX_LAST',\n                        'BO1_PX_LAST', 'HG1_PX_LAST', 'GC1_PX_LAST', 'CT1_PX_LAST', 'LC1_PX_LAST']\n\ndef train_and_predict(df, target_column):\n\n    # Select features and target variable\n    features = merged_data.drop(['title', 'body', 'created_utc'] + [target_column], axis=1)\n    target = merged_data[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\n    # Define preprocessing steps\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('sentiment', SimpleImputer(strategy='constant', fill_value=0),\n                ['title_sentiment_roberta', 'title_sentiment_bert', 'title_sentiment_distilbert']),\n            ('numeric', StandardScaler(),\n                ['CL1_VOLATILITY_10D', 'CL1_VOLATILITY_20D', 'NG1_VOLATILITY_10D', 'NG1_VOLATILITY_20D',\n                 'HO1_VOLATILITY_10D', 'HO1_VOLATILITY_20D', 'W1_VOLATILITY_10D',\n                 'W1_VOLATILITY_20D', 'C1_VOLATILITY_10D', 'C1_VOLATILITY_20D',\n                 'S1_VOLATILITY_10D', 'S1_VOLATILITY_20D', 'BO1_VOLATILITY_10D', 'BO1_VOLATILITY_20D',\n                 'HG1_VOLATILITY_10D', 'HG1_VOLATILITY_20D', 'GC1_VOLATILITY_10D',\n                 'GC1_VOLATILITY_20D', 'CT1_VOLATILITY_10D', 'CT1_VOLATILITY_20D',\n                 'LC1_VOLATILITY_10D', 'LC1_VOLATILITY_20D']),\n        ])\n\n    # Define the models\n    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n    dt_model = DecisionTreeRegressor(random_state=42)\n\n    # Create and evaluate pipelines for each model\n    rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                   ('model', rf_model)])\n\n    dt_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                   ('model', dt_model)])\n\n    # Fit and evaluate Random Forest model\n    rf_pipeline.fit(X_train, y_train)\n    rf_y_pred = rf_pipeline.predict(X_test)\n    rf_mse = mean_squared_error(y_test, rf_y_pred)\n    rf_r2 = r2_score(y_test, rf_y_pred)\n    print(f'{target_column} - Random Forest - Mean Squared Error: {rf_mse}')\n    print(f'{target_column} - Random Forest - R-squared (R2) Score: {rf_r2}')\n\n    # Fit and evaluate Decision Tree model\n    dt_pipeline.fit(X_train, y_train)\n    dt_y_pred = dt_pipeline.predict(X_test)\n    dt_mse = mean_squared_error(y_test, dt_y_pred)\n    dt_r2 = r2_score(y_test, dt_y_pred)\n    print(f'{target_column} - Decision Tree - Mean Squared Error: {dt_mse}')\n    print(f'{target_column} - Decision Tree - R-squared (R2) Score: {dt_r2}')\n\n\n    print('-' * 50)\n\n\nfor column in columns_of_interest:\n    train_and_predict(merged_data, column)\n\ndef train_and_predict_lstm(df, target_column):\n    features = merged_data.drop(['title', 'body', 'created_utc'] + [target_column], axis=1)\n    target = merged_data[target_column]\n    features_array = features.select_dtypes(include=[np.number]).values\n    target_array = target.values.reshape(-1, 1)\n\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features_array)\n\n    X_train, X_test, y_train, y_test = train_test_split(features_scaled, target_array, test_size=0.2, random_state=42)\n\n    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n\n    # Define LSTM model\n    model = Sequential()\n    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(LSTM(units=50))\n    model.add(Dense(units=1))\n\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    print(f'{target_column} - LSTM - Mean Squared Error: {mse}')\n    print(f'{target_column} - LSTM - R-squared (R2) Score: {r2}')\n    print('-' * 50)\n\nfor column in columns_of_interest:\n    train_and_predict_lstm(merged_data, column)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install feedparser\nimport feedparser\nimport urllib.parse\n\n# Define keywords related to commodities\nkeywords = ['gold', 'silver', 'oil', 'copper', 'soybean', 'cotton', 'cattle', 'cotton', 'wheat', 'corn', 'gas', 'commodity', 'commodities', 'stock market']\n\n# Fetch news articles related to commodities from Google News (500 000 lines) \nnews_data = {'title': [], 'summary': [], 'published': []}\ntotal_lines = 500000\nlines_fetched = 0\nbatch_size = 100\n\nwhile lines_fetched < total_lines:\n    for keyword in keywords:\n        # Encode keyword for URL\n        encoded_keyword = urllib.parse.quote_plus(keyword)\n        feed_url = f'https://news.google.com/rss/search?q={encoded_keyword}&hl=en-US&gl=US&ceid=US:en'\n        feed = feedparser.parse(feed_url)\n        for entry in feed.entries:\n            news_data['title'].append(entry.title)\n            news_data['summary'].append(entry.summary)\n            news_data['published'].append(entry.published)\n            lines_fetched += 1\n            if lines_fetched >= total_lines:\n                break\n        if lines_fetched >= total_lines:\n            break\n    if lines_fetched >= total_lines:\n        break\n\n# Create a DataFrame from the extracted news data\nnews_df = pd.DataFrame(news_data)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load sentiment analysis models and tokenizers\nroberta_sentiment_model = pipeline('sentiment-analysis', model=RobertaForSequenceClassification.from_pretrained('roberta-base'), tokenizer=RobertaTokenizer.from_pretrained('roberta-base'))\nbert_sentiment_model = pipeline('sentiment-analysis', model=BertForSequenceClassification.from_pretrained('bert-base-uncased'), tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\ndistilbert_sentiment_model = pipeline('sentiment-analysis', model=DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased'), tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased'))\n\n# Truncate the 'summary' text to fit within the model's maximum sequence length\nmax_seq_length = 512\nnews_df['summary'] = news_df['summary'].apply(lambda x: x[:max_seq_length])\n\n# Apply sentiment analysis using RoBERTa, BERT, and DistilBERT\nnews_df['title_sentiment_roberta'] = news_df['title'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\nnews_df['summary_sentiment_roberta'] = news_df['summary'].apply(lambda x: roberta_sentiment_model(x)[0]['label'])\n\nnews_df['title_sentiment_bert'] = news_df['title'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\nnews_df['summary_sentiment_bert'] = news_df['summary'].apply(lambda x: bert_sentiment_model(x)[0]['label'])\n\nnews_df['title_sentiment_distilbert'] = news_df['title'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\nnews_df['summary_sentiment_distilbert'] = news_df['summary'].apply(lambda x: distilbert_sentiment_model(x)[0]['label'])\n\n# Display the DataFrame with sentiment analysis results\nprint(news_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map labels for sentiment categories\nlabel_mapping = {'LABEL_0': '-1', 'LABEL_1': '0', 'LABEL_2': '1'}\n\n# Map sentiment labels in the DataFrame\nfor model in ['roberta', 'bert', 'distilbert']:\n    news_df[f'title_sentiment_{model}'] = news_df[f'title_sentiment_{model}'].map(label_mapping)\n    news_df[f'summary_sentiment_{model}'] = news_df[f'summary_sentiment_{model}'].map(label_mapping)\n\n# Visualize Sentiment Distribution\nlabels = ['-1', '0', '1']\n\nfig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\nbar_width = 0.35\nfor i, model in enumerate(['roberta', 'bert', 'distilbert']):\n    title_sentiment_counts = news_df[f'title_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar(labels, title_sentiment_counts, color='skyblue', label=f'Title ({model.capitalize()})', width=bar_width)\n    summary_sentiment_counts = news_df[f'summary_sentiment_{model}'].value_counts().reindex(labels, fill_value=0)\n    axs[i].bar([x + bar_width for x in range(len(labels))], summary_sentiment_counts, color='#CBC3E3', alpha=0.7, label=f'Summary ({model.capitalize()})', width=bar_width)\n\n    axs[i].set_title(f'{model.capitalize()} Sentiment Distribution')\n    axs[i].legend()\n\nfig.suptitle('Sentiment Analysis on Google News Data')\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_df.to_csv('news_df.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"price_data = pd.read_csv('commodities_DAILY.csv')\nsentiment_data = pd.read_csv('news_df.csv')\n\n# Convert 'published' column in sentiment_data\nsentiment_data['published'] = pd.to_datetime(sentiment_data['published'], unit='s')\n# Extract date part only from 'published'\nsentiment_data['published'] = sentiment_data['published'].dt.date\n# Convert 'Dates' to the same format as 'published'\nprice_data['Dates'] = price_data['Dates'].dt.date\n# Verify the conversion\nprint(price_data.head())\n# Merge on the date part\nmerged_data = pd.merge(sentiment_data, price_data, left_on='published', right_on='Dates', how='left')\n# Check columns of the merged data\nprint(\"merged_data columns:\", merged_data.columns)\n\ncolumns_of_interest = ['CL1_PX_LAST', 'NG1_PX_LAST', 'HO1_PX_LAST', 'W1_PX_LAST', 'C1_PX_LAST', 'S1_PX_LAST',\n                        'BO1_PX_LAST', 'HG1_PX_LAST', 'GC1_PX_LAST', 'CT1_PX_LAST', 'LC1_PX_LAST']\n\n# Dropping rows with missing values in sentiment columns if any\nmerged_data.dropna(subset=columns_of_interest, inplace=True)\n\ndef train_and_predict(df, target_column):\n    features = merged_data.drop(['title', 'summary', 'published'] + [target_column], axis=1)\n    target = merged_data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('sentiment', SimpleImputer(strategy='constant', fill_value=0),\n                ['title_sentiment_roberta', 'title_sentiment_bert', 'title_sentiment_distilbert']),\n            ('numeric', StandardScaler(),\n                ['CL1_VOLATILITY_10D', 'CL1_VOLATILITY_20D', 'NG1_VOLATILITY_10D', 'NG1_VOLATILITY_20D',\n                 'HO1_VOLATILITY_10D', 'HO1_VOLATILITY_20D', 'W1_VOLATILITY_10D',\n                 'W1_VOLATILITY_20D', 'C1_VOLATILITY_10D', 'C1_VOLATILITY_20D',\n                 'S1_VOLATILITY_10D', 'S1_VOLATILITY_20D', 'BO1_VOLATILITY_10D', 'BO1_VOLATILITY_20D',\n                 'HG1_VOLATILITY_10D', 'HG1_VOLATILITY_20D', 'GC1_VOLATILITY_10D',\n                 'GC1_VOLATILITY_20D', 'CT1_VOLATILITY_10D', 'CT1_VOLATILITY_20D',\n                 'LC1_VOLATILITY_10D', 'LC1_VOLATILITY_20D']),\n        ])\n\n    rf_model = RandomForestRegressor(n_estimators=200, random_state=42)\n    dt_model = DecisionTreeRegressor(random_state=42)\n    rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                   ('model', rf_model)])\n\n    dt_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                   ('model', dt_model)])\n\n    rf_pipeline.fit(X_train, y_train)\n    rf_y_pred = rf_pipeline.predict(X_test)\n    rf_mse = mean_squared_error(y_test, rf_y_pred)\n    rf_r2 = r2_score(y_test, rf_y_pred)\n    print(f'{target_column} - Random Forest - Mean Squared Error: {rf_mse}')\n    print(f'{target_column} - Random Forest - R-squared (R2) Score: {rf_r2}')\n\n    dt_pipeline.fit(X_train, y_train)\n    dt_y_pred = dt_pipeline.predict(X_test)\n    dt_mse = mean_squared_error(y_test, dt_y_pred)\n    dt_r2 = r2_score(y_test, dt_y_pred)\n    print(f'{target_column} - Decision Tree - Mean Squared Error: {dt_mse}')\n    print(f'{target_column} - Decision Tree - R-squared (R2) Score: {dt_r2}')\n\n    print('-' * 50)\n\nfor column in columns_of_interest:\n    train_and_predict(merged_data, column)\n\n\ndef train_and_predict_lstm(df, target_column):\n    features = merged_data.drop(['title', 'summary', 'published'] + [target_column], axis=1)\n    target = merged_data[target_column]\n\n    # Convert to numpy arrays\n    features_array = features.select_dtypes(include=[np.number]).values\n    target_array = target.values.reshape(-1, 1)\n\n    # Normalize features\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features_array)\n    X_train, X_test, y_train, y_test = train_test_split(features_scaled, target_array, test_size=0.2, random_state=42)\n\n    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n\n    # Define LSTM model\n    model = Sequential()\n    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(LSTM(units=50))\n    model.add(Dense(units=1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    print(f'{target_column} - LSTM - Mean Squared Error: {mse}')\n    print(f'{target_column} - LSTM - R-squared (R2) Score: {r2}')\n    print('-' * 50)\n\nfor column in columns_of_interest:\n    train_and_predict_lstm(merged_data, column)","metadata":{},"execution_count":null,"outputs":[]}]}