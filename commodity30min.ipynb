{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7250000,"sourceType":"datasetVersion","datasetId":4200475},{"sourceId":7250232,"sourceType":"datasetVersion","datasetId":4200624}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport random\nimport os\nimport numpy as np \nimport pandas as pd \nimport requests\nimport pandas_datareader as web\n# Date\nimport datetime as dt\nfrom datetime import date, timedelta, datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error,explained_variance_score, r2_score, mean_absolute_percentage_error\nimport math\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.base import BaseEstimator\nfrom tensorflow.keras.layers import GRU, SimpleRNN\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:16:39.004143Z","iopub.execute_input":"2025-06-02T22:16:39.004534Z","iopub.status.idle":"2025-06-02T22:16:55.728149Z","shell.execute_reply.started":"2025-06-02T22:16:39.004500Z","shell.execute_reply":"2025-06-02T22:16:55.727023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/testtttt/file30.csv')\ndisplay(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:17:08.452541Z","iopub.execute_input":"2025-06-02T22:17:08.453389Z","iopub.status.idle":"2025-06-02T22:17:08.555885Z","shell.execute_reply.started":"2025-06-02T22:17:08.453352Z","shell.execute_reply":"2025-06-02T22:17:08.554961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['CL1Dates'] = pd.to_datetime(df['CL1Dates'], format='%m/%d/%y %H:%M')\ndf['NG1Dates'] = pd.to_datetime(df['NG1Dates'], format='%m/%d/%y %H:%M')\ndf['HO1Dates'] = pd.to_datetime(df['HO1Dates'], format='%m/%d/%y %H:%M')\ndf['WDates'] = pd.to_datetime(df['WDates'], format='%m/%d/%y %H:%M')\ndf['CDates'] = pd.to_datetime(df['CDates'], format='%m/%d/%Y %H:%M')\ndf['SDates'] = pd.to_datetime(df['SDates'], format='%m/%d/%y %H:%M')\ndf['BO1Dates'] = pd.to_datetime(df['BO1Dates'], format='%m/%d/%y %H:%M')\ndf['HG1Dates'] = pd.to_datetime(df['HG1Dates'], format='%m/%d/%y %H:%M')\ndf['GC1Dates'] = pd.to_datetime(df['GC1Dates'], format='%m/%d/%y %H:%M')\ndf['CT1Dates'] = pd.to_datetime(df['CT1Dates'], format='%m/%d/%y %H:%M')\ndf['LC1Dates'] = pd.to_datetime(df['LC1Dates'], format='%m/%d/%y %H:%M')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:17:11.854862Z","iopub.execute_input":"2025-06-02T22:17:11.855373Z","iopub.status.idle":"2025-06-02T22:17:12.062231Z","shell.execute_reply.started":"2025-06-02T22:17:11.855330Z","shell.execute_reply":"2025-06-02T22:17:12.061068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:17:14.773189Z","iopub.execute_input":"2025-06-02T22:17:14.774060Z","iopub.status.idle":"2025-06-02T22:17:14.797824Z","shell.execute_reply.started":"2025-06-02T22:17:14.774024Z","shell.execute_reply":"2025-06-02T22:17:14.796689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"column_types = df.dtypes\nprint(column_types)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:17:21.355718Z","iopub.execute_input":"2025-06-02T22:17:21.356152Z","iopub.status.idle":"2025-06-02T22:17:21.364389Z","shell.execute_reply.started":"2025-06-02T22:17:21.356103Z","shell.execute_reply":"2025-06-02T22:17:21.362992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add a column for the day of the week \ndf['DayOfWeekCL'] = df['CL1Dates'].dt.dayofweek\ndf['DayOfWeekNG'] = df['NG1Dates'].dt.dayofweek\ndf['DayOfWeekLC'] = df['LC1Dates'].dt.dayofweek\ndf['DayOfWeekHO'] = df['HO1Dates'].dt.dayofweek\ndf['DayOfWeekW'] = df['WDates'].dt.dayofweek\ndf['DayOfWeekC'] = df['CDates'].dt.dayofweek\ndf['DayOfWeekS'] = df['SDates'].dt.dayofweek\ndf['DayOfWeekBO'] = df['BO1Dates'].dt.dayofweek\ndf['DayOfWeekHG'] = df['HG1Dates'].dt.dayofweek\ndf['DayOfWeekGC'] = df['GC1Dates'].dt.dayofweek\ndf['DayOfWeekCT'] = df['CT1Dates'].dt.dayofweek\n# Add a binary column indicating whether it's a weekend (1 = Saturday or Sunday, 0 = other days)\ndf['IsWeekendCL1'] = df['DayOfWeekCL'].isin([5, 6]).astype(int)\ndf['IsWeekendNG1'] = df['DayOfWeekNG'].isin([5, 6]).astype(int)\ndf['IsWeekendLC1'] = df['DayOfWeekLC'].isin([5, 6]).astype(int)\ndf['IsWeekendHO1'] = df['DayOfWeekHO'].isin([5, 6]).astype(int)\ndf['IsWeekendW'] = df['DayOfWeekW'].isin([5, 6]).astype(int)\ndf['IsWeekendC'] = df['DayOfWeekC'].isin([5, 6]).astype(int)\ndf['IsWeekendS'] = df['DayOfWeekS'].isin([5, 6]).astype(int)\ndf['IsWeekendBO1'] = df['DayOfWeekBO'].isin([5, 6]).astype(int)\ndf['IsWeekendHG1'] = df['DayOfWeekHG'].isin([5, 6]).astype(int)\ndf['IsWeekendGC1'] = df['DayOfWeekGC'].isin([5, 6]).astype(int)\ndf['IsWeekendCT1'] = df['DayOfWeekCT'].isin([5, 6]).astype(int)\n\nfig, axs = plt.subplots(11, 1, figsize=(10, 15), sharex=True)\nfor i, commodity in enumerate(['CL1', 'NG1', 'LC1', 'HO1', 'W', 'C', 'S', 'BO1', 'HG1', 'GC1', 'CT1']):\n    dates_column = f'{commodity}Dates'\n    close_column = f'{commodity}Close'\n    is_weekend_column = f'IsWeekend{commodity}'\n    axs[i].plot(df[dates_column], df[close_column], label=f'{commodity} Price', alpha=0.5)\n    weekend_data = df[df[is_weekend_column] == 1]\n    axs[i].scatter(weekend_data[dates_column], weekend_data[close_column], label='Weekends', color='red', marker='o', s=20)\n    axs[i].set_ylabel(f'{commodity} Price')\n    axs[i].legend()\n    axs[i].grid(True)\n\nfig.suptitle(\"30min Commodity Prices with Weekends Highlighted\", fontsize=16)\naxs[10].set_xlabel(\"Date and Time\")\nplt.tight_layout(rect=[0, 0, 1, 0.96])  \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:17:24.848577Z","iopub.execute_input":"2025-06-02T22:17:24.849004Z","iopub.status.idle":"2025-06-02T22:17:29.421127Z","shell.execute_reply.started":"2025-06-02T22:17:24.848968Z","shell.execute_reply":"2025-06-02T22:17:29.419951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for colonne in df.columns:\n    df = df.dropna(subset=[colonne])\n    \ndf.isnull().values.any()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:17:32.146224Z","iopub.execute_input":"2025-06-02T22:17:32.146652Z","iopub.status.idle":"2025-06-02T22:17:32.281746Z","shell.execute_reply.started":"2025-06-02T22:17:32.146621Z","shell.execute_reply":"2025-06-02T22:17:32.280807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert nanoseconds to seconds\ndf['CL1Dates'] = df['CL1Dates'].astype(int) / 10**9  \ndf['NG1Dates'] = df['NG1Dates'].astype(int) / 10**9  \ndf['HO1Dates'] = df['HO1Dates'].astype(int) / 10**9  \ndf['WDates'] = df['WDates'].astype(int) / 10**9  \ndf['CDates'] = df['CDates'].astype(int) / 10**9  \ndf['SDates'] = df['SDates'].astype(int) / 10**9  \ndf['BO1Dates'] = df['BO1Dates'].astype(int) / 10**9  \ndf['HG1Dates'] = df['HG1Dates'].astype(int) / 10**9  \ndf['GC1Dates'] = df['GC1Dates'].astype(int) / 10**9  \ndf['CT1Dates'] = df['CT1Dates'].astype(int) / 10**9  \ndf['LC1Dates'] = df['LC1Dates'].astype(int) / 10**9 \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:17:34.753237Z","iopub.execute_input":"2025-06-02T22:17:34.753609Z","iopub.status.idle":"2025-06-02T22:17:34.771770Z","shell.execute_reply.started":"2025-06-02T22:17:34.753582Z","shell.execute_reply":"2025-06-02T22:17:34.770554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ncolumns_of_interest = ['CL1Close', 'NG1Close', 'HO1Close','WClose','CClose','SClose','BO1Close','HG1Close','GC1Close','CT1Close','LC1Close']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:17:37.817061Z","iopub.execute_input":"2025-06-02T22:17:37.817496Z","iopub.status.idle":"2025-06-02T22:17:37.823274Z","shell.execute_reply.started":"2025-06-02T22:17:37.817466Z","shell.execute_reply":"2025-06-02T22:17:37.822178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T20:11:48.646580Z","iopub.execute_input":"2025-05-26T20:11:48.646982Z","iopub.status.idle":"2025-05-26T20:11:48.654150Z","shell.execute_reply.started":"2025-05-26T20:11:48.646937Z","shell.execute_reply":"2025-05-26T20:11:48.653127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikeras\nfrom scikeras.wrappers import KerasRegressor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:32:01.724851Z","iopub.execute_input":"2025-06-02T22:32:01.725267Z","execution_failed":"2025-06-02T22:35:11.435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Error metric\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n# Rolling forecast for sklearn models (LR, RF, GBR)\ndef rolling_forecast(df, target_column, model_instance, window_size=50):\n    y_true_all = []\n    y_pred_all = []\n\n    for i in range(window_size, len(df) - 1):\n        train = df.iloc[i - window_size:i]\n        test = df.iloc[i + 1:i + 2]  # 1-step ahead\n\n        X_train = train.drop(columns=[target_column])\n        y_train = train[target_column]\n        X_test = test.drop(columns=[target_column])\n        y_test = test[target_column]\n\n        model = model_instance\n        model.fit(X_train, y_train)\n        prediction = model.predict(X_test)[0]\n\n        y_true_all.append(y_test.values[0])\n        y_pred_all.append(prediction)\n\n    mse = mean_squared_error(y_true_all, y_pred_all)\n    r2 = r2_score(y_true_all, y_pred_all)\n    mape = mean_absolute_percentage_error(y_true_all, y_pred_all)\n\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n# Hyperparameter tuning for RF and GBR\ndef tune_rf_hyperparams(df, target_column, window_size=100):\n    train = df.iloc[:window_size]\n    X_train = train.drop(columns=[target_column])\n    y_train = train[target_column]\n\n    rf = RandomForestRegressor(random_state=42)\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [20, 30, 10, None]\n    }\n    grid = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n    grid.fit(X_train, y_train)\n    print(f\"Best RF params for {target_column}: {grid.best_params_}\")\n    return RandomForestRegressor(**grid.best_params_, random_state=42)\n\ndef tune_gb_hyperparams(df, target_column, window_size=100):\n    train = df.iloc[:window_size]\n    X_train = train.drop(columns=[target_column])\n    y_train = train[target_column]\n\n    gb = GradientBoostingRegressor(random_state=42)\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'learning_rate': [0.01, 0.1, 0.2],\n        'max_depth': [3, 4, 5],\n        'subsample': [0.8, 0.9, 1.0]\n    }\n    grid = GridSearchCV(gb, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n    grid.fit(X_train, y_train)\n    print(f\"Best GB params for {target_column}: {grid.best_params_}\")\n    return GradientBoostingRegressor(**grid.best_params_, random_state=42)\n\n# Build LSTM model function for KerasRegressor\ndef create_lstm_model(units=50, lr=0.001):\n    model = Sequential()\n    model.add(LSTM(units=units, input_shape=(1,1)))\n    model.add(Dense(1))\n    optimizer = Adam(learning_rate=lr)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n\n# Build DNN model function for KerasRegressor\ndef create_dnn_model(units=50, lr=0.001):\n    model = Sequential()\n    model.add(Dense(units, activation='relu', input_dim=1))\n    model.add(Dense(units, activation='relu'))\n    model.add(Dense(1))\n    optimizer = Adam(learning_rate=lr)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n\n# Prepare data for keras models (LSTM and DNN)\ndef prepare_lstm_data(series, window_size=50):\n    scaler = MinMaxScaler()\n    scaled = scaler.fit_transform(series.values.reshape(-1, 1))\n    X, y = [], []\n    for i in range(len(scaled) - window_size):\n        X.append(scaled[i:i+window_size])\n        y.append(scaled[i+window_size])\n    X = np.array(X).reshape(-1, window_size, 1)\n    y = np.array(y)\n    return X, y, scaler\n\ndef prepare_dnn_data(series, window_size=50):\n    scaler = MinMaxScaler()\n    scaled = scaler.fit_transform(series.values.reshape(-1, 1))\n    X, y = [], []\n    for i in range(len(scaled) - window_size):\n        X.append(scaled[i:i+window_size].flatten())  # flatten window for DNN\n        y.append(scaled[i+window_size])\n    X = np.array(X)\n    y = np.array(y)\n    return X, y, scaler\n\n# Tune LSTM hyperparameters \ndef tune_lstm(df, target_column, window_size=50, n_iter=5):\n    series = df[target_column]\n    X, y, scaler = prepare_lstm_data(series, window_size)\n\n    model = KerasRegressor(build_fn=create_lstm_model, verbose=0)\n\n    param_dist = {\n        'units': [50, 100, 150],\n        'epochs': [50, 100, 150],\n        'batch_size': [32, 64, 128]\n    }\n\n    random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n                                       n_iter=n_iter, cv=3, verbose=1, n_jobs=1)\n    random_search.fit(X, y)\n    print(f\"LSTM best params for {target_column}: {random_search.best_params_}\")\n    return random_search.best_params_, scaler\n\n# Tune DNN hyperparameters with RandomizedSearchCV\ndef tune_dnn(df, target_column, window_size=50, n_iter=5):\n    series = df[target_column]\n    X, y, scaler = prepare_dnn_data(series, window_size)\n\n    model = KerasRegressor(build_fn=create_dnn_model, verbose=0)\n\n    param_dist = {\n        'units': [50, 100, 150],\n        'epochs': [50, 100, 150],\n        'batch_size': [32, 64, 128]\n    }\n\n    random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n                                       n_iter=n_iter, cv=3, verbose=1, n_jobs=1)\n    random_search.fit(X, y)\n    print(f\"DNN best params for {target_column}: {random_search.best_params_}\")\n    return random_search.best_params_, scaler\n\n# Rolling forecast for LSTM with tuned params\ndef rolling_forecast_lstm(df, target_column, best_params, scaler, window_size=50):\n    y_true_all = []\n    y_pred_all = []\n\n    series = df[target_column].values\n    scaled = scaler.transform(series.reshape(-1,1))\n\n    for i in range(window_size, len(df) - 1):\n        train_seq = scaled[i-window_size:i].reshape(1, window_size, 1)\n        y_true = series[i]\n\n        model = create_lstm_model(units=best_params['units'], lr=best_params['lr'])\n        model.fit(train_seq, scaled[i-window_size:i], epochs=best_params['epochs'], verbose=0)\n        pred_scaled = model.predict(train_seq)\n        pred = scaler.inverse_transform(pred_scaled)[0][0]\n\n        y_true_all.append(y_true)\n        y_pred_all.append(pred)\n\n    mse = mean_squared_error(y_true_all, y_pred_all)\n    r2 = r2_score(y_true_all, y_pred_all)\n    mape = mean_absolute_percentage_error(y_true_all, y_pred_all)\n\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n# Rolling forecast for DNN with tuned params\ndef rolling_forecast_dnn(df, target_column, best_params, scaler, window_size=50):\n    y_true_all = []\n    y_pred_all = []\n\n    series = df[target_column].values\n    scaled = scaler.transform(series.reshape(-1,1))\n\n    for i in range(window_size, len(df) - 1):\n        train_seq = scaled[i-window_size:i].reshape(1, window_size)  # Flattened for DNN\n        y_true = series[i]\n\n        model = create_dnn_model(units=best_params['units'], lr=best_params['lr'])\n        model.fit(train_seq, scaled[i-window_size:i], epochs=best_params['epochs'], verbose=0)\n        pred_scaled = model.predict(train_seq)\n        pred = scaler.inverse_transform(pred_scaled)[0][0]\n\n        y_true_all.append(y_true)\n        y_pred_all.append(pred)\n\n    mse = mean_squared_error(y_true_all, y_pred_all)\n    r2 = r2_score(y_true_all, y_pred_all)\n    mape = mean_absolute_percentage_error(y_true_all, y_pred_all)\n\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n\nwindow_size = 50\n\nresults = {}\n\nfor col in columns_of_interest:\n    print(f\"\\n=== Processing {col} ===\")\n\n    # Linear Regression (no tuning)\n    lr_model = LinearRegression()\n    lr_results = rolling_forecast(df, col, lr_model, window_size=window_size)\n\n    # Random Forest tuning and forecast\n    rf_model = tune_rf_hyperparams(df, col, window_size=100)\n    rf_results = rolling_forecast(df, col, rf_model, window_size=window_size)\n\n    # Gradient Boosting tuning and forecast\n    gb_model = tune_gb_hyperparams(df, col, window_size=100)\n    gb_results = rolling_forecast(df, col, gb_model, window_size=window_size)\n\n    # LSTM tuning\n    lstm_best_params, lstm_scaler = tune_lstm(df, col, window_size=window_size, n_iter=5)\n    lstm_results = rolling_forecast_lstm(df, col, lstm_best_params, lstm_scaler, window_size=window_size)\n\n    # DNN tuning\n    dnn_best_params, dnn_scaler = tune_dnn(df, col, window_size=window_size, n_iter=5)\n    dnn_results = rolling_forecast_dnn(df, col, dnn_best_params, dnn_scaler, window_size=window_size)\n\n    results[col] = {\n        'LinearRegression': lr_results,\n        'RandomForest': rf_results,\n        'GradientBoosting': gb_results,\n        'LSTM': lstm_results,\n        'DNN': dnn_results\n    }\n\n# Print summarized results\nfor commodity, res in results.items():\n    print(f\"\\nResults for {commodity}:\")\n    for model_name, metrics in res.items():\n        print(f\" {model_name}: MSE={metrics['MSE']:.3f}, R2={metrics['R2']:.3f}, MAPE={metrics['MAPE']:.3f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:18:32.291024Z","iopub.execute_input":"2025-06-02T22:18:32.291539Z","iopub.status.idle":"2025-06-02T22:30:31.663955Z","shell.execute_reply.started":"2025-06-02T22:18:32.291495Z","shell.execute_reply":"2025-06-02T22:30:31.661307Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Calculate HE and LLE**","metadata":{}},{"cell_type":"code","source":"!pip install hurst\n!pip install nolds","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from hurst import compute_Hc\nimport nolds\n\n\n# Function to calculate HE using R/S analysis\ndef calculate_hurst_exponent(data):\n    H, c, data = compute_Hc(data)\n    return H\n\n# Function to calculate LLE\ndef calculate_local_lyapunov_exponent(data, emb_dim=10):\n    le = nolds.lyap_e(data, emb_dim=emb_dim)\n    return le\n\ndef split_data(data, test_size=0.2):\n    train_data, test_data = train_test_split(data, test_size=test_size, shuffle=False)\n    return train_data, test_data\n\nfor column in df.columns:\n    if 'Close' in column:\n        # Extract commodity name\n        commodity_name = column[:-5]\n        # Extract close prices\n        close_prices = df[column].dropna().values\n        train_data, test_data = split_data(close_prices)\n\n        # Calculate HE for training and testing sets\n        hurst_train = calculate_hurst_exponent(train_data)\n        hurst_test = calculate_hurst_exponent(test_data)\n\n        # Calculate LLE for training and testing sets\n        lle_train = np.mean(calculate_local_lyapunov_exponent(train_data))\n        lle_test = np.mean(calculate_local_lyapunov_exponent(test_data))\n\n        print(f\"Commodity: {commodity_name}\")\n        print(\"Training Set:\")\n        print(f\"Hurst Exponent: {hurst_train}\")\n        print(f\"Average Local Lyapunov Exponent: {lle_train}\")\n        print(\"Testing Set:\")\n        print(f\"Hurst Exponent: {hurst_test}\")\n        print(f\"Average Local Lyapunov Exponent: {lle_test}\")\n        print()\n","metadata":{},"outputs":[],"execution_count":null}]}