{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7250000,"sourceType":"datasetVersion","datasetId":4200475},{"sourceId":7250232,"sourceType":"datasetVersion","datasetId":4200624}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport random\nimport os\nimport numpy as np \nimport pandas as pd \nimport requests\nimport pandas_datareader as web\n# Date\nimport datetime as dt\nfrom datetime import date, timedelta, datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error,explained_variance_score, r2_score, mean_absolute_percentage_error\nimport math\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.base import BaseEstimator\nfrom tensorflow.keras.layers import GRU, SimpleRNN\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('file30.csv')\ndisplay(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['CL1Dates'] = pd.to_datetime(df['CL1Dates'], format='%m/%d/%y %H:%M')\ndf['NG1Dates'] = pd.to_datetime(df['NG1Dates'], format='%m/%d/%y %H:%M')\ndf['HO1Dates'] = pd.to_datetime(df['HO1Dates'], format='%m/%d/%y %H:%M')\ndf['WDates'] = pd.to_datetime(df['WDates'], format='%m/%d/%y %H:%M')\ndf['CDates'] = pd.to_datetime(df['CDates'], format='%m/%d/%Y %H:%M')\ndf['SDates'] = pd.to_datetime(df['SDates'], format='%m/%d/%y %H:%M')\ndf['BO1Dates'] = pd.to_datetime(df['BO1Dates'], format='%m/%d/%y %H:%M')\ndf['HG1Dates'] = pd.to_datetime(df['HG1Dates'], format='%m/%d/%y %H:%M')\ndf['GC1Dates'] = pd.to_datetime(df['GC1Dates'], format='%m/%d/%y %H:%M')\ndf['CT1Dates'] = pd.to_datetime(df['CT1Dates'], format='%m/%d/%y %H:%M')\ndf['LC1Dates'] = pd.to_datetime(df['LC1Dates'], format='%m/%d/%y %H:%M')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_types = df.dtypes\nprint(column_types)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a column for the day of the week \ndf['DayOfWeekCL'] = df['CL1Dates'].dt.dayofweek\ndf['DayOfWeekNG'] = df['NG1Dates'].dt.dayofweek\ndf['DayOfWeekLC'] = df['LC1Dates'].dt.dayofweek\ndf['DayOfWeekHO'] = df['HO1Dates'].dt.dayofweek\ndf['DayOfWeekW'] = df['WDates'].dt.dayofweek\ndf['DayOfWeekC'] = df['CDates'].dt.dayofweek\ndf['DayOfWeekS'] = df['SDates'].dt.dayofweek\ndf['DayOfWeekBO'] = df['BO1Dates'].dt.dayofweek\ndf['DayOfWeekHG'] = df['HG1Dates'].dt.dayofweek\ndf['DayOfWeekGC'] = df['GC1Dates'].dt.dayofweek\ndf['DayOfWeekCT'] = df['CT1Dates'].dt.dayofweek\n# Add a binary column indicating whether it's a weekend (1 = Saturday or Sunday, 0 = other days)\ndf['IsWeekendCL1'] = df['DayOfWeekCL'].isin([5, 6]).astype(int)\ndf['IsWeekendNG1'] = df['DayOfWeekNG'].isin([5, 6]).astype(int)\ndf['IsWeekendLC1'] = df['DayOfWeekLC'].isin([5, 6]).astype(int)\ndf['IsWeekendHO1'] = df['DayOfWeekHO'].isin([5, 6]).astype(int)\ndf['IsWeekendW'] = df['DayOfWeekW'].isin([5, 6]).astype(int)\ndf['IsWeekendC'] = df['DayOfWeekC'].isin([5, 6]).astype(int)\ndf['IsWeekendS'] = df['DayOfWeekS'].isin([5, 6]).astype(int)\ndf['IsWeekendBO1'] = df['DayOfWeekBO'].isin([5, 6]).astype(int)\ndf['IsWeekendHG1'] = df['DayOfWeekHG'].isin([5, 6]).astype(int)\ndf['IsWeekendGC1'] = df['DayOfWeekGC'].isin([5, 6]).astype(int)\ndf['IsWeekendCT1'] = df['DayOfWeekCT'].isin([5, 6]).astype(int)\n\nfig, axs = plt.subplots(11, 1, figsize=(10, 15), sharex=True)\nfor i, commodity in enumerate(['CL1', 'NG1', 'LC1', 'HO1', 'W', 'C', 'S', 'BO1', 'HG1', 'GC1', 'CT1']):\n    dates_column = f'{commodity}Dates'\n    close_column = f'{commodity}Close'\n    is_weekend_column = f'IsWeekend{commodity}'\n    axs[i].plot(df[dates_column], df[close_column], label=f'{commodity} Price', alpha=0.5)\n    weekend_data = df[df[is_weekend_column] == 1]\n    axs[i].scatter(weekend_data[dates_column], weekend_data[close_column], label='Weekends', color='red', marker='o', s=20)\n    axs[i].set_ylabel(f'{commodity} Price')\n    axs[i].legend()\n    axs[i].grid(True)\n\nfig.suptitle(\"30min Commodity Prices with Weekends Highlighted\", fontsize=16)\naxs[10].set_xlabel(\"Date and Time\")\nplt.tight_layout(rect=[0, 0, 1, 0.96])  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for colonne in df.columns:\n    df = df.dropna(subset=[colonne])\n    \ndf.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert nanoseconds to seconds\ndf['CL1Dates'] = df['CL1Dates'].astype(int) / 10**9  \ndf['NG1Dates'] = df['NG1Dates'].astype(int) / 10**9  \ndf['HO1Dates'] = df['HO1Dates'].astype(int) / 10**9  \ndf['WDates'] = df['WDates'].astype(int) / 10**9  \ndf['CDates'] = df['CDates'].astype(int) / 10**9  \ndf['SDates'] = df['SDates'].astype(int) / 10**9  \ndf['BO1Dates'] = df['BO1Dates'].astype(int) / 10**9  \ndf['HG1Dates'] = df['HG1Dates'].astype(int) / 10**9  \ndf['GC1Dates'] = df['GC1Dates'].astype(int) / 10**9  \ndf['CT1Dates'] = df['CT1Dates'].astype(int) / 10**9  \ndf['LC1Dates'] = df['LC1Dates'].astype(int) / 10**9 \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ncolumns_of_interest = ['CL1Close', 'NG1Close', 'HO1Close','WClose','CClose','SClose','BO1Close','HG1Close','GC1Close','CT1Close','LC1Close']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model LR","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    lr_model = LinearRegression()\n    lr_model.fit(X_train, y_train)\n    predictions = lr_model.predict(X_test)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    # Print evaluation metrics\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model RF","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    rf_model = RandomForestRegressor()\n    # Specify hyperparameters to tune\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [None, 10, 20, 30],\n    }\n    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search.fit(X_train, y_train)\n    # Get the best hyperparameters\n    best_params = grid_search.best_params_\n    print(\"Best Hyperparameters:\", best_params)\n    best_model = grid_search.best_estimator_\n    predictions = best_model.predict(X_test)\n    # Evaluate the model\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model DT","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    param_grid = {\n        'max_depth': [None, 10, 20, 30],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4]\n    }\n    dt_model = DecisionTreeRegressor()\n    grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n    grid_search.fit(X_train, y_train)\n    best_params = grid_search.best_params_\n    best_dt_model = DecisionTreeRegressor(**best_params)\n    best_dt_model.fit(X_train, y_train)\n    predictions = best_dt_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print({target_column})\n    print(f\"Best Hyperparameters: {best_params}\")\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model GBR","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    gb_regressor = GradientBoostingRegressor(random_state=42)\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'learning_rate': [0.01, 0.1, 0.2],\n        'max_depth': [3, 4, 5],\n        'subsample': [0.8, 0.9, 1.0],\n    }\n    grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search.fit(X_train, y_train)\n    # Get the best hyperparameters\n    best_params = grid_search.best_params_\n    print(\"Best Hyperparameters:\", best_params)\n    # Use the best hyperparameters to create the final model\n    final_gb_model = GradientBoostingRegressor(**best_params, random_state=42)\n    final_gb_model.fit(X_train, y_train)\n    predictions = final_gb_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print({target_column})\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model kNN","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    # Normalize the data\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    param_grid = {\n    'n_neighbors': np.arange(1, 21),\n    'weights': ['uniform', 'distance']\n    }\n    # Create kNN model\n    knn_model = KNeighborsRegressor()\n    grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n    grid_search.fit(X_train, y_train)\n    best_knn_model = grid_search.best_estimator_\n    predictions = best_knn_model.predict(X_test)\n    # Evaluate the model\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print({target_column})\n    print(\"Best Hyperparameters:\", grid_search.best_params_)\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Models LSTM/GRU/DNN","metadata":{}},{"cell_type":"code","source":"class KerasLSTMRegressor(BaseEstimator):\n    def __init__(self, model_type='LSTM', units=50, epochs=50, batch_size=32, verbose=0):\n        self.model_type = model_type\n        self.units = units\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.model = self._build_model()\n\n    def _build_model(self):\n        model = Sequential()\n        if self.model_type == 'LSTM':\n            model.add(LSTM(units=self.units, input_shape=(1, 1)))\n        elif self.model_type == 'GRU':\n            model.add(GRU(units=self.units, input_shape=(1, 1)))\n        elif self.model_type == 'DNN':\n            model.add(Dense(units=self.units, input_shape=(1,)))\n            model.add(Dense(units=self.units))\n        model.add(Dense(units=1))\n        model.compile(optimizer='adam', loss='mean_squared_error')\n        return model\n\n    def fit(self, X, y):\n        X = X.reshape((X.shape[0], 1, 1))\n        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n        return self\n\n    def predict(self, X):\n        X = X.reshape((X.shape[0], 1, 1))\n        return self.model.predict(X)\n\n    def score(self, X, y):\n        X = X.reshape((X.shape[0], 1, 1))\n        return -self.model.evaluate(X, y, verbose=self.verbose)\n\ndef train_and_predict(df, target_column):\n    target = df[target_column].values.reshape(-1, 1)\n    scaler = MinMaxScaler()\n    scaled_target = scaler.fit_transform(target)\n    X_train, X_test, y_train, y_test = train_test_split(scaled_target[:-1], scaled_target[1:], test_size=0.2, random_state=42)\n\n    param_grid = {\n        'units': [50, 100, 150],  # Adjust units for LSTM, GRU, and DNN\n        'epochs': [50, 100, 150],  # Adjust epochs\n        'batch_size': [32, 64, 128]  # Adjust batch_size\n    }\n\n    # Create KerasRegressor for LSTM\n    keras_lstm_regressor = KerasLSTMRegressor(model_type='LSTM', epochs=50, batch_size=32, verbose=0)\n    grid_search_lstm = GridSearchCV(estimator=keras_lstm_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_lstm.fit(X_train, y_train)\n    # Get the best hyperparameters for LSTM\n    best_params_lstm = grid_search_lstm.best_params_\n    best_lstm_regressor = KerasLSTMRegressor(model_type='LSTM', units=best_params_lstm['units'], epochs=best_params_lstm['epochs'], batch_size=best_params_lstm['batch_size'], verbose=0)\n    best_lstm_regressor.fit(X_train, y_train)\n    predictions_lstm = best_lstm_regressor.predict(X_test)\n    mse_lstm = mean_squared_error(y_test, predictions_lstm)\n    mape_lstm = mean_absolute_percentage_error(y_test, predictions_lstm)\n    r2_lstm = r2_score(y_test, predictions_lstm)\n\n    # Create KerasRegressor for GRU\n    keras_gru_regressor = KerasLSTMRegressor(model_type='GRU', epochs=50, batch_size=32, verbose=0)\n    grid_search_gru = GridSearchCV(estimator=keras_gru_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_gru.fit(X_train, y_train)\n    # Get the best hyperparameters for GRU\n    best_params_gru = grid_search_gru.best_params_\n    best_gru_regressor = KerasLSTMRegressor(model_type='GRU', units=best_params_gru['units'], epochs=best_params_gru['epochs'], batch_size=best_params_gru['batch_size'], verbose=0)\n    best_gru_regressor.fit(X_train, y_train)\n    predictions_gru = best_gru_regressor.predict(X_test)\n    mape_gru = mean_absolute_percentage_error(y_test, predictions_gru)\n    mse_gru = mean_squared_error(y_test, predictions_gru)\n    r2_gru = r2_score(y_test, predictions_gru)\n\n    # Create KerasRegressor for DNN\n    keras_dnn_regressor = KerasLSTMRegressor(model_type='DNN', epochs=50, batch_size=32, verbose=0)\n    grid_search_dnn = GridSearchCV(estimator=keras_dnn_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_dnn.fit(X_train, y_train)\n    # Get the best hyperparameters for DNN\n    best_params_dnn = grid_search_dnn.best_params_\n    best_dnn_regressor = KerasLSTMRegressor(model_type='DNN', units=best_params_dnn['units'], epochs=best_params_dnn['epochs'], batch_size=best_params_dnn['batch_size'], verbose=0)\n    best_dnn_regressor.fit(X_train, y_train)\n    predictions_dnn = best_dnn_regressor.predict(X_test)\n    mape_dnn = mean_absolute_percentage_error(y_test, predictions_dnn)\n    mse_dnn = mean_squared_error(y_test, predictions_dnn)\n    r2_dnn = r2_score(y_test, predictions_dnn)\n\n    print(f\"Target Column: {target_column}\")\n    print(\"Best parameters for LSTM:\", best_params_lstm)\n    print(f\"MSE for LSTM: {mse_lstm}, R-squared for LSTM: {r2_lstm}, MAPE:\", mape_lstm)\n    print(\"Best parameters for GRU:\", best_params_gru)\n    print(f\"MSE for GRU: {mse_gru}, R-squared for GRU: {r2_gru}, MAPE:\", mape_gru)\n    print(\"Best parameters for DNN:\", best_params_dnn)\n    print(f\"MSE for DNN: {mse_dnn}, R-squared for DNN: {r2_dnn}, MAPE:\", mape_dnn)\n    print()\n    \nfor column in columns_of_interest:\n    train_and_predict(df, column)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate HE and LLE**","metadata":{}},{"cell_type":"code","source":"!pip install hurst\n!pip install nolds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hurst import compute_Hc\nimport nolds\n\n\n# Function to calculate HE using R/S analysis\ndef calculate_hurst_exponent(data):\n    H, c, data = compute_Hc(data)\n    return H\n\n# Function to calculate LLE\ndef calculate_local_lyapunov_exponent(data, emb_dim=10):\n    le = nolds.lyap_e(data, emb_dim=emb_dim)\n    return le\n\ndef split_data(data, test_size=0.2):\n    train_data, test_data = train_test_split(data, test_size=test_size, shuffle=False)\n    return train_data, test_data\n\nfor column in df.columns:\n    if 'Close' in column:\n        # Extract commodity name\n        commodity_name = column[:-5]\n        # Extract close prices\n        close_prices = df[column].dropna().values\n        train_data, test_data = split_data(close_prices)\n\n        # Calculate HE for training and testing sets\n        hurst_train = calculate_hurst_exponent(train_data)\n        hurst_test = calculate_hurst_exponent(test_data)\n\n        # Calculate LLE for training and testing sets\n        lle_train = np.mean(calculate_local_lyapunov_exponent(train_data))\n        lle_test = np.mean(calculate_local_lyapunov_exponent(test_data))\n\n        print(f\"Commodity: {commodity_name}\")\n        print(\"Training Set:\")\n        print(f\"Hurst Exponent: {hurst_train}\")\n        print(f\"Average Local Lyapunov Exponent: {lle_train}\")\n        print(\"Testing Set:\")\n        print(f\"Hurst Exponent: {hurst_test}\")\n        print(f\"Average Local Lyapunov Exponent: {lle_test}\")\n        print()\n","metadata":{},"execution_count":null,"outputs":[]}]}