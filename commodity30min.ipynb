{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7250000,"sourceType":"datasetVersion","datasetId":4200475},{"sourceId":7250232,"sourceType":"datasetVersion","datasetId":4200624}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport random\nimport os\nimport numpy as np \nimport pandas as pd \nimport requests\nimport pandas_datareader as web\n# Date\nimport datetime as dt\nfrom datetime import date, timedelta, datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error,explained_variance_score, r2_score, mean_absolute_percentage_error\nimport math\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.base import BaseEstimator\nfrom tensorflow.keras.layers import GRU, SimpleRNN\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T10:34:01.320881Z","iopub.execute_input":"2024-05-20T10:34:01.321253Z","iopub.status.idle":"2024-05-20T10:34:16.574334Z","shell.execute_reply.started":"2024-05-20T10:34:01.321221Z","shell.execute_reply":"2024-05-20T10:34:16.573377Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/testtttt/file30.csv')\ndisplay(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:36:10.770812Z","iopub.execute_input":"2024-05-20T10:36:10.772028Z","iopub.status.idle":"2024-05-20T10:36:10.935892Z","shell.execute_reply.started":"2024-05-20T10:36:10.771975Z","shell.execute_reply":"2024-05-20T10:36:10.934903Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"           CL1Dates  CL1Open  CL1Close   CL1Value  CL1Volume        NG1Dates  \\\n0     5/31/23 23:30    67.51     67.70   37470.11      554.0   5/31/23 23:30   \n1       6/1/23 0:00    67.69     67.69   22132.73      327.0     6/1/23 0:00   \n2       6/1/23 0:30    67.69     67.72   32567.97      481.0     6/1/23 0:30   \n3       6/1/23 1:00    67.71     67.68  109403.03     1615.0     6/1/23 1:00   \n4       6/1/23 1:30    67.68     67.65   44868.65      663.0     6/1/23 1:30   \n...             ...      ...       ...        ...        ...             ...   \n6364            NaN      NaN       NaN        NaN        NaN  12/14/23 13:00   \n6365            NaN      NaN       NaN        NaN        NaN  12/14/23 13:30   \n6366            NaN      NaN       NaN        NaN        NaN  12/14/23 14:00   \n6367            NaN      NaN       NaN        NaN        NaN  12/14/23 14:30   \n6368            NaN      NaN       NaN        NaN        NaN  12/14/23 15:00   \n\n      NG1Open  NG1Close  NG1Value  NG1Volume  ...     CT1Dates  CT1Open  \\\n0       2.256     2.256   176.027         78  ...  6/1/23 2:30    83.96   \n1       2.256     2.258   374.734        166  ...  6/1/23 3:00    84.04   \n2       2.257     2.256   266.267        118  ...  6/1/23 3:30    84.14   \n3       2.257     2.262   890.737        394  ...  6/1/23 4:00    84.35   \n4       2.262     2.262   499.603        221  ...  6/1/23 4:30    84.65   \n...       ...       ...       ...        ...  ...          ...      ...   \n6364    2.358     2.347  1814.080        773  ...          NaN      NaN   \n6365    2.347     2.354  2488.357       1056  ...          NaN      NaN   \n6366    2.355     2.372  4343.645       1839  ...          NaN      NaN   \n6367    2.371     2.382  8986.259       3761  ...          NaN      NaN   \n6368    2.382     2.357  7780.330       3292  ...          NaN      NaN   \n\n      CT1Close  CT1Value  CT1Volume      LC1Dates  LC1Open  LC1Close  \\\n0        84.04   7728.27       92.0  6/1/23 15:00  169.400   169.175   \n1        84.10   1765.94       21.0  6/1/23 15:30  169.200   170.275   \n2        84.30   4039.77       48.0  6/1/23 16:00  170.275   171.375   \n3        84.60   4730.37       56.0  6/1/23 16:30  171.350   172.075   \n4        84.77  22369.38      264.0  6/1/23 17:00  172.125   172.675   \n...        ...       ...        ...           ...      ...       ...   \n6364       NaN       NaN        NaN           NaN      NaN       NaN   \n6365       NaN       NaN        NaN           NaN      NaN       NaN   \n6366       NaN       NaN        NaN           NaN      NaN       NaN   \n6367       NaN       NaN        NaN           NaN      NaN       NaN   \n6368       NaN       NaN        NaN           NaN      NaN       NaN   \n\n        LC1Value  LC1Volume  \n0     185458.422     1095.0  \n1     313593.531     1845.0  \n2     320612.938     1876.0  \n3     383372.250     2228.0  \n4     171082.078      992.0  \n...          ...        ...  \n6364         NaN        NaN  \n6365         NaN        NaN  \n6366         NaN        NaN  \n6367         NaN        NaN  \n6368         NaN        NaN  \n\n[6369 rows x 55 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CL1Dates</th>\n      <th>CL1Open</th>\n      <th>CL1Close</th>\n      <th>CL1Value</th>\n      <th>CL1Volume</th>\n      <th>NG1Dates</th>\n      <th>NG1Open</th>\n      <th>NG1Close</th>\n      <th>NG1Value</th>\n      <th>NG1Volume</th>\n      <th>...</th>\n      <th>CT1Dates</th>\n      <th>CT1Open</th>\n      <th>CT1Close</th>\n      <th>CT1Value</th>\n      <th>CT1Volume</th>\n      <th>LC1Dates</th>\n      <th>LC1Open</th>\n      <th>LC1Close</th>\n      <th>LC1Value</th>\n      <th>LC1Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5/31/23 23:30</td>\n      <td>67.51</td>\n      <td>67.70</td>\n      <td>37470.11</td>\n      <td>554.0</td>\n      <td>5/31/23 23:30</td>\n      <td>2.256</td>\n      <td>2.256</td>\n      <td>176.027</td>\n      <td>78</td>\n      <td>...</td>\n      <td>6/1/23 2:30</td>\n      <td>83.96</td>\n      <td>84.04</td>\n      <td>7728.27</td>\n      <td>92.0</td>\n      <td>6/1/23 15:00</td>\n      <td>169.400</td>\n      <td>169.175</td>\n      <td>185458.422</td>\n      <td>1095.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6/1/23 0:00</td>\n      <td>67.69</td>\n      <td>67.69</td>\n      <td>22132.73</td>\n      <td>327.0</td>\n      <td>6/1/23 0:00</td>\n      <td>2.256</td>\n      <td>2.258</td>\n      <td>374.734</td>\n      <td>166</td>\n      <td>...</td>\n      <td>6/1/23 3:00</td>\n      <td>84.04</td>\n      <td>84.10</td>\n      <td>1765.94</td>\n      <td>21.0</td>\n      <td>6/1/23 15:30</td>\n      <td>169.200</td>\n      <td>170.275</td>\n      <td>313593.531</td>\n      <td>1845.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6/1/23 0:30</td>\n      <td>67.69</td>\n      <td>67.72</td>\n      <td>32567.97</td>\n      <td>481.0</td>\n      <td>6/1/23 0:30</td>\n      <td>2.257</td>\n      <td>2.256</td>\n      <td>266.267</td>\n      <td>118</td>\n      <td>...</td>\n      <td>6/1/23 3:30</td>\n      <td>84.14</td>\n      <td>84.30</td>\n      <td>4039.77</td>\n      <td>48.0</td>\n      <td>6/1/23 16:00</td>\n      <td>170.275</td>\n      <td>171.375</td>\n      <td>320612.938</td>\n      <td>1876.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6/1/23 1:00</td>\n      <td>67.71</td>\n      <td>67.68</td>\n      <td>109403.03</td>\n      <td>1615.0</td>\n      <td>6/1/23 1:00</td>\n      <td>2.257</td>\n      <td>2.262</td>\n      <td>890.737</td>\n      <td>394</td>\n      <td>...</td>\n      <td>6/1/23 4:00</td>\n      <td>84.35</td>\n      <td>84.60</td>\n      <td>4730.37</td>\n      <td>56.0</td>\n      <td>6/1/23 16:30</td>\n      <td>171.350</td>\n      <td>172.075</td>\n      <td>383372.250</td>\n      <td>2228.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6/1/23 1:30</td>\n      <td>67.68</td>\n      <td>67.65</td>\n      <td>44868.65</td>\n      <td>663.0</td>\n      <td>6/1/23 1:30</td>\n      <td>2.262</td>\n      <td>2.262</td>\n      <td>499.603</td>\n      <td>221</td>\n      <td>...</td>\n      <td>6/1/23 4:30</td>\n      <td>84.65</td>\n      <td>84.77</td>\n      <td>22369.38</td>\n      <td>264.0</td>\n      <td>6/1/23 17:00</td>\n      <td>172.125</td>\n      <td>172.675</td>\n      <td>171082.078</td>\n      <td>992.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6364</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12/14/23 13:00</td>\n      <td>2.358</td>\n      <td>2.347</td>\n      <td>1814.080</td>\n      <td>773</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6365</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12/14/23 13:30</td>\n      <td>2.347</td>\n      <td>2.354</td>\n      <td>2488.357</td>\n      <td>1056</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6366</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12/14/23 14:00</td>\n      <td>2.355</td>\n      <td>2.372</td>\n      <td>4343.645</td>\n      <td>1839</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6367</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12/14/23 14:30</td>\n      <td>2.371</td>\n      <td>2.382</td>\n      <td>8986.259</td>\n      <td>3761</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6368</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12/14/23 15:00</td>\n      <td>2.382</td>\n      <td>2.357</td>\n      <td>7780.330</td>\n      <td>3292</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>6369 rows × 55 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['CL1Dates'] = pd.to_datetime(df['CL1Dates'], format='%m/%d/%y %H:%M')\ndf['NG1Dates'] = pd.to_datetime(df['NG1Dates'], format='%m/%d/%y %H:%M')\ndf['HO1Dates'] = pd.to_datetime(df['HO1Dates'], format='%m/%d/%y %H:%M')\ndf['WDates'] = pd.to_datetime(df['WDates'], format='%m/%d/%y %H:%M')\ndf['CDates'] = pd.to_datetime(df['CDates'], format='%m/%d/%Y %H:%M')\ndf['SDates'] = pd.to_datetime(df['SDates'], format='%m/%d/%y %H:%M')\ndf['BO1Dates'] = pd.to_datetime(df['BO1Dates'], format='%m/%d/%y %H:%M')\ndf['HG1Dates'] = pd.to_datetime(df['HG1Dates'], format='%m/%d/%y %H:%M')\ndf['GC1Dates'] = pd.to_datetime(df['GC1Dates'], format='%m/%d/%y %H:%M')\ndf['CT1Dates'] = pd.to_datetime(df['CT1Dates'], format='%m/%d/%y %H:%M')\ndf['LC1Dates'] = pd.to_datetime(df['LC1Dates'], format='%m/%d/%y %H:%M')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_types = df.dtypes\nprint(column_types)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a column for the day of the week \ndf['DayOfWeekCL'] = df['CL1Dates'].dt.dayofweek\ndf['DayOfWeekNG'] = df['NG1Dates'].dt.dayofweek\ndf['DayOfWeekLC'] = df['LC1Dates'].dt.dayofweek\ndf['DayOfWeekHO'] = df['HO1Dates'].dt.dayofweek\ndf['DayOfWeekW'] = df['WDates'].dt.dayofweek\ndf['DayOfWeekC'] = df['CDates'].dt.dayofweek\ndf['DayOfWeekS'] = df['SDates'].dt.dayofweek\ndf['DayOfWeekBO'] = df['BO1Dates'].dt.dayofweek\ndf['DayOfWeekHG'] = df['HG1Dates'].dt.dayofweek\ndf['DayOfWeekGC'] = df['GC1Dates'].dt.dayofweek\ndf['DayOfWeekCT'] = df['CT1Dates'].dt.dayofweek\n# Add a binary column indicating whether it's a weekend (1 = Saturday or Sunday, 0 = other days)\ndf['IsWeekendCL1'] = df['DayOfWeekCL'].isin([5, 6]).astype(int)\ndf['IsWeekendNG1'] = df['DayOfWeekNG'].isin([5, 6]).astype(int)\ndf['IsWeekendLC1'] = df['DayOfWeekLC'].isin([5, 6]).astype(int)\ndf['IsWeekendHO1'] = df['DayOfWeekHO'].isin([5, 6]).astype(int)\ndf['IsWeekendW'] = df['DayOfWeekW'].isin([5, 6]).astype(int)\ndf['IsWeekendC'] = df['DayOfWeekC'].isin([5, 6]).astype(int)\ndf['IsWeekendS'] = df['DayOfWeekS'].isin([5, 6]).astype(int)\ndf['IsWeekendBO1'] = df['DayOfWeekBO'].isin([5, 6]).astype(int)\ndf['IsWeekendHG1'] = df['DayOfWeekHG'].isin([5, 6]).astype(int)\ndf['IsWeekendGC1'] = df['DayOfWeekGC'].isin([5, 6]).astype(int)\ndf['IsWeekendCT1'] = df['DayOfWeekCT'].isin([5, 6]).astype(int)\n\nfig, axs = plt.subplots(11, 1, figsize=(10, 15), sharex=True)\nfor i, commodity in enumerate(['CL1', 'NG1', 'LC1', 'HO1', 'W', 'C', 'S', 'BO1', 'HG1', 'GC1', 'CT1']):\n    dates_column = f'{commodity}Dates'\n    close_column = f'{commodity}Close'\n    is_weekend_column = f'IsWeekend{commodity}'\n    axs[i].plot(df[dates_column], df[close_column], label=f'{commodity} Price', alpha=0.5)\n    weekend_data = df[df[is_weekend_column] == 1]\n    axs[i].scatter(weekend_data[dates_column], weekend_data[close_column], label='Weekends', color='red', marker='o', s=20)\n    axs[i].set_ylabel(f'{commodity} Price')\n    axs[i].legend()\n    axs[i].grid(True)\n\nfig.suptitle(\"30min Commodity Prices with Weekends Highlighted\", fontsize=16)\naxs[10].set_xlabel(\"Date and Time\")\nplt.tight_layout(rect=[0, 0, 1, 0.96])  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for colonne in df.columns:\n    df = df.dropna(subset=[colonne])\n    \ndf.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert nanoseconds to seconds\ndf['CL1Dates'] = df['CL1Dates'].astype(int) / 10**9  \ndf['NG1Dates'] = df['NG1Dates'].astype(int) / 10**9  \ndf['HO1Dates'] = df['HO1Dates'].astype(int) / 10**9  \ndf['WDates'] = df['WDates'].astype(int) / 10**9  \ndf['CDates'] = df['CDates'].astype(int) / 10**9  \ndf['SDates'] = df['SDates'].astype(int) / 10**9  \ndf['BO1Dates'] = df['BO1Dates'].astype(int) / 10**9  \ndf['HG1Dates'] = df['HG1Dates'].astype(int) / 10**9  \ndf['GC1Dates'] = df['GC1Dates'].astype(int) / 10**9  \ndf['CT1Dates'] = df['CT1Dates'].astype(int) / 10**9  \ndf['LC1Dates'] = df['LC1Dates'].astype(int) / 10**9 \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ncolumns_of_interest = ['CL1Close', 'NG1Close', 'HO1Close','WClose','CClose','SClose','BO1Close','HG1Close','GC1Close','CT1Close','LC1Close']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model LR","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    lr_model = LinearRegression()\n    lr_model.fit(X_train, y_train)\n    predictions = lr_model.predict(X_test)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    # Print evaluation metrics\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model RF","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    rf_model = RandomForestRegressor()\n    # Specify hyperparameters to tune\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [None, 10, 20, 30],\n    }\n    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search.fit(X_train, y_train)\n    # Get the best hyperparameters\n    best_params = grid_search.best_params_\n    print(\"Best Hyperparameters:\", best_params)\n    best_model = grid_search.best_estimator_\n    predictions = best_model.predict(X_test)\n    # Evaluate the model\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model DT","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    param_grid = {\n        'max_depth': [None, 10, 20, 30],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4]\n    }\n    dt_model = DecisionTreeRegressor()\n    grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n    grid_search.fit(X_train, y_train)\n    best_params = grid_search.best_params_\n    best_dt_model = DecisionTreeRegressor(**best_params)\n    best_dt_model.fit(X_train, y_train)\n    predictions = best_dt_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print({target_column})\n    print(f\"Best Hyperparameters: {best_params}\")\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model GBR","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    gb_regressor = GradientBoostingRegressor(random_state=42)\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'learning_rate': [0.01, 0.1, 0.2],\n        'max_depth': [3, 4, 5],\n        'subsample': [0.8, 0.9, 1.0],\n    }\n    grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search.fit(X_train, y_train)\n    # Get the best hyperparameters\n    best_params = grid_search.best_params_\n    print(\"Best Hyperparameters:\", best_params)\n    # Use the best hyperparameters to create the final model\n    final_gb_model = GradientBoostingRegressor(**best_params, random_state=42)\n    final_gb_model.fit(X_train, y_train)\n    predictions = final_gb_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print({target_column})\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model kNN","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    # Normalize the data\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    param_grid = {\n    'n_neighbors': np.arange(1, 21),\n    'weights': ['uniform', 'distance']\n    }\n    # Create kNN model\n    knn_model = KNeighborsRegressor()\n    grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n    grid_search.fit(X_train, y_train)\n    best_knn_model = grid_search.best_estimator_\n    predictions = best_knn_model.predict(X_test)\n    # Evaluate the model\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print({target_column})\n    print(\"Best Hyperparameters:\", grid_search.best_params_)\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Models LSTM/GRU/DNN","metadata":{}},{"cell_type":"code","source":"class KerasLSTMRegressor(BaseEstimator):\n    def __init__(self, model_type='LSTM', units=50, epochs=50, batch_size=32, verbose=0):\n        self.model_type = model_type\n        self.units = units\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.model = self._build_model()\n\n    def _build_model(self):\n        model = Sequential()\n        if self.model_type == 'LSTM':\n            model.add(LSTM(units=self.units, input_shape=(1, 1)))\n        elif self.model_type == 'GRU':\n            model.add(GRU(units=self.units, input_shape=(1, 1)))\n        elif self.model_type == 'DNN':\n            model.add(Dense(units=self.units, input_shape=(1,)))\n            model.add(Dense(units=self.units))\n        model.add(Dense(units=1))\n        model.compile(optimizer='adam', loss='mean_squared_error')\n        return model\n\n    def fit(self, X, y):\n        X = X.reshape((X.shape[0], 1, 1))\n        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n        return self\n\n    def predict(self, X):\n        X = X.reshape((X.shape[0], 1, 1))\n        return self.model.predict(X)\n\n    def score(self, X, y):\n        X = X.reshape((X.shape[0], 1, 1))\n        return -self.model.evaluate(X, y, verbose=self.verbose)\n\ndef train_and_predict(df, target_column):\n    target = df[target_column].values.reshape(-1, 1)\n    scaler = MinMaxScaler()\n    scaled_target = scaler.fit_transform(target)\n    X_train, X_test, y_train, y_test = train_test_split(scaled_target[:-1], scaled_target[1:], test_size=0.2, random_state=42)\n\n    param_grid = {\n        'units': [50, 100, 150],  # Adjust units for LSTM, GRU, and DNN\n        'epochs': [50, 100, 150],  # Adjust epochs\n        'batch_size': [32, 64, 128]  # Adjust batch_size\n    }\n\n    # Create KerasRegressor for LSTM\n    keras_lstm_regressor = KerasLSTMRegressor(model_type='LSTM', epochs=50, batch_size=32, verbose=0)\n    grid_search_lstm = GridSearchCV(estimator=keras_lstm_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_lstm.fit(X_train, y_train)\n    # Get the best hyperparameters for LSTM\n    best_params_lstm = grid_search_lstm.best_params_\n    best_lstm_regressor = KerasLSTMRegressor(model_type='LSTM', units=best_params_lstm['units'], epochs=best_params_lstm['epochs'], batch_size=best_params_lstm['batch_size'], verbose=0)\n    best_lstm_regressor.fit(X_train, y_train)\n    predictions_lstm = best_lstm_regressor.predict(X_test)\n    mse_lstm = mean_squared_error(y_test, predictions_lstm)\n    mape_lstm = mean_absolute_percentage_error(y_test, predictions_lstm)\n    r2_lstm = r2_score(y_test, predictions_lstm)\n\n    # Create KerasRegressor for GRU\n    keras_gru_regressor = KerasLSTMRegressor(model_type='GRU', epochs=50, batch_size=32, verbose=0)\n    grid_search_gru = GridSearchCV(estimator=keras_gru_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_gru.fit(X_train, y_train)\n    # Get the best hyperparameters for GRU\n    best_params_gru = grid_search_gru.best_params_\n    best_gru_regressor = KerasLSTMRegressor(model_type='GRU', units=best_params_gru['units'], epochs=best_params_gru['epochs'], batch_size=best_params_gru['batch_size'], verbose=0)\n    best_gru_regressor.fit(X_train, y_train)\n    predictions_gru = best_gru_regressor.predict(X_test)\n    mape_gru = mean_absolute_percentage_error(y_test, predictions_gru)\n    mse_gru = mean_squared_error(y_test, predictions_gru)\n    r2_gru = r2_score(y_test, predictions_gru)\n\n    # Create KerasRegressor for DNN\n    keras_dnn_regressor = KerasLSTMRegressor(model_type='DNN', epochs=50, batch_size=32, verbose=0)\n    grid_search_dnn = GridSearchCV(estimator=keras_dnn_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_dnn.fit(X_train, y_train)\n    # Get the best hyperparameters for DNN\n    best_params_dnn = grid_search_dnn.best_params_\n    best_dnn_regressor = KerasLSTMRegressor(model_type='DNN', units=best_params_dnn['units'], epochs=best_params_dnn['epochs'], batch_size=best_params_dnn['batch_size'], verbose=0)\n    best_dnn_regressor.fit(X_train, y_train)\n    predictions_dnn = best_dnn_regressor.predict(X_test)\n    mape_dnn = mean_absolute_percentage_error(y_test, predictions_dnn)\n    mse_dnn = mean_squared_error(y_test, predictions_dnn)\n    r2_dnn = r2_score(y_test, predictions_dnn)\n\n    print(f\"Target Column: {target_column}\")\n    print(\"Best parameters for LSTM:\", best_params_lstm)\n    print(f\"MSE for LSTM: {mse_lstm}, R-squared for LSTM: {r2_lstm}, MAPE:\", mape_lstm)\n    print(\"Best parameters for GRU:\", best_params_gru)\n    print(f\"MSE for GRU: {mse_gru}, R-squared for GRU: {r2_gru}, MAPE:\", mape_gru)\n    print(\"Best parameters for DNN:\", best_params_dnn)\n    print(f\"MSE for DNN: {mse_dnn}, R-squared for DNN: {r2_dnn}, MAPE:\", mape_dnn)\n    print()\n    \nfor column in columns_of_interest:\n    train_and_predict(df, column)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate HE and LLE**","metadata":{}},{"cell_type":"code","source":"!pip install hurst\n!pip install nolds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hurst import compute_Hc\nimport nolds\n\n\n# Function to calculate HE using R/S analysis\ndef calculate_hurst_exponent(data):\n    H, c, data = compute_Hc(data)\n    return H\n\n# Function to calculate LLE\ndef calculate_local_lyapunov_exponent(data, emb_dim=10):\n    le = nolds.lyap_e(data, emb_dim=emb_dim)\n    return le\n\ndef split_data(data, test_size=0.2):\n    train_data, test_data = train_test_split(data, test_size=test_size, shuffle=False)\n    return train_data, test_data\n\nfor column in df.columns:\n    if 'Close' in column:\n        # Extract commodity name\n        commodity_name = column[:-5]\n        # Extract close prices\n        close_prices = df[column].dropna().values\n        train_data, test_data = split_data(close_prices)\n\n        # Calculate HE for training and testing sets\n        hurst_train = calculate_hurst_exponent(train_data)\n        hurst_test = calculate_hurst_exponent(test_data)\n\n        # Calculate LLE for training and testing sets\n        lle_train = np.mean(calculate_local_lyapunov_exponent(train_data))\n        lle_test = np.mean(calculate_local_lyapunov_exponent(test_data))\n\n        print(f\"Commodity: {commodity_name}\")\n        print(\"Training Set:\")\n        print(f\"Hurst Exponent: {hurst_train}\")\n        print(f\"Average Local Lyapunov Exponent: {lle_train}\")\n        print(\"Testing Set:\")\n        print(f\"Hurst Exponent: {hurst_test}\")\n        print(f\"Average Local Lyapunov Exponent: {lle_test}\")\n        print()\n","metadata":{},"execution_count":null,"outputs":[]}]}