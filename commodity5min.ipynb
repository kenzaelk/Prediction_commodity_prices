{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7256254,"sourceType":"datasetVersion","datasetId":4204850}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport random\nimport os\nimport numpy as np \nimport pandas as pd \nimport requests\nimport pandas_datareader as web\n\n# Date\nimport datetime as dt\nfrom datetime import date, timedelta, datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error,explained_variance_score, r2_score , mean_absolute_percentage_error\nimport math\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom keras.models import Sequential\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom keras.layers import LSTM, Dense, Dropout\nfrom sklearn.base import BaseEstimator\nfrom tensorflow.keras.layers import GRU, SimpleRNN\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T10:36:58.445692Z","iopub.execute_input":"2024-05-20T10:36:58.446481Z","iopub.status.idle":"2024-05-20T10:37:15.166825Z","shell.execute_reply.started":"2024-05-20T10:36:58.446439Z","shell.execute_reply":"2024-05-20T10:37:15.165269Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/commmm5/commodities_5mnFF.csv')\ndisplay(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:37:15.169203Z","iopub.execute_input":"2024-05-20T10:37:15.170061Z","iopub.status.idle":"2024-05-20T10:37:15.791402Z","shell.execute_reply.started":"2024-05-20T10:37:15.170022Z","shell.execute_reply":"2024-05-20T10:37:15.789995Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"               CL1Dates  CL1Open  CL1Close   CL1Value  CL1Volume  \\\n0       5/31/2023 23:05    67.65     67.58    6287.77         93   \n1       5/31/2023 23:10    67.58     67.57   17089.73        253   \n2       5/31/2023 23:15    67.57     67.54    5335.84         79   \n3       5/31/2023 23:20    67.51     67.55    5132.33         76   \n4       5/31/2023 23:25    67.55     67.52    2701.65         40   \n...                 ...      ...       ...        ...        ...   \n36792  12/14/2023 14:55    71.39     71.49  107288.04       1502   \n36793  12/14/2023 15:00    71.50     71.54  285467.16       3990   \n36794  12/14/2023 15:05    71.54     71.62  108174.64       1511   \n36795  12/14/2023 15:10    71.62     71.62  193266.97       2697   \n36796  12/14/2023 15:15    71.62     71.63   13107.43        183   \n\n              NG1Dates  NG1Open  NG1Close  NG1Value  NG1Volume  ...  \\\n0      5/31/2023 23:05    2.257     2.259   142.248       63.0  ...   \n1      5/31/2023 23:10    2.258     2.258   121.879       54.0  ...   \n2      5/31/2023 23:15    2.257     2.257   110.578       49.0  ...   \n3      5/31/2023 23:20    2.255     2.256    36.088       16.0  ...   \n4      5/31/2023 23:25    2.256     2.255    42.858       19.0  ...   \n...                ...      ...       ...       ...        ...  ...   \n36792              NaN      NaN       NaN       NaN        NaN  ...   \n36793              NaN      NaN       NaN       NaN        NaN  ...   \n36794              NaN      NaN       NaN       NaN        NaN  ...   \n36795              NaN      NaN       NaN       NaN        NaN  ...   \n36796              NaN      NaN       NaN       NaN        NaN  ...   \n\n            CT1Dates  CT1Open  CT1Close  CT1Value  CT1Volume        LC1Dates  \\\n0      6/1/2023 2:05    83.68     83.92   3769.90       45.0  6/1/2023 14:35   \n1      6/1/2023 2:10    83.85     83.91    251.57        3.0  6/1/2023 14:40   \n2      6/1/2023 2:15    83.93     83.99    251.90        3.0  6/1/2023 14:45   \n3      6/1/2023 2:20    83.92     83.92     83.92        1.0  6/1/2023 14:50   \n4      6/1/2023 2:25    83.92     83.91    251.75        3.0  6/1/2023 14:55   \n...              ...      ...       ...       ...        ...             ...   \n36792            NaN      NaN       NaN       NaN        NaN             NaN   \n36793            NaN      NaN       NaN       NaN        NaN             NaN   \n36794            NaN      NaN       NaN       NaN        NaN             NaN   \n36795            NaN      NaN       NaN       NaN        NaN             NaN   \n36796            NaN      NaN       NaN       NaN        NaN             NaN   \n\n       LC1Open  LC1Close   LC1Value  LC1Volume  \n0      169.200   169.275  42137.199      249.0  \n1      169.250   169.250  33680.449      199.0  \n2      169.275   169.325  39617.148      234.0  \n3      169.350   169.325  20149.150      119.0  \n4      169.325   169.425  41496.199      245.0  \n...        ...       ...        ...        ...  \n36792      NaN       NaN        NaN        NaN  \n36793      NaN       NaN        NaN        NaN  \n36794      NaN       NaN        NaN        NaN  \n36795      NaN       NaN        NaN        NaN  \n36796      NaN       NaN        NaN        NaN  \n\n[36797 rows x 55 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CL1Dates</th>\n      <th>CL1Open</th>\n      <th>CL1Close</th>\n      <th>CL1Value</th>\n      <th>CL1Volume</th>\n      <th>NG1Dates</th>\n      <th>NG1Open</th>\n      <th>NG1Close</th>\n      <th>NG1Value</th>\n      <th>NG1Volume</th>\n      <th>...</th>\n      <th>CT1Dates</th>\n      <th>CT1Open</th>\n      <th>CT1Close</th>\n      <th>CT1Value</th>\n      <th>CT1Volume</th>\n      <th>LC1Dates</th>\n      <th>LC1Open</th>\n      <th>LC1Close</th>\n      <th>LC1Value</th>\n      <th>LC1Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5/31/2023 23:05</td>\n      <td>67.65</td>\n      <td>67.58</td>\n      <td>6287.77</td>\n      <td>93</td>\n      <td>5/31/2023 23:05</td>\n      <td>2.257</td>\n      <td>2.259</td>\n      <td>142.248</td>\n      <td>63.0</td>\n      <td>...</td>\n      <td>6/1/2023 2:05</td>\n      <td>83.68</td>\n      <td>83.92</td>\n      <td>3769.90</td>\n      <td>45.0</td>\n      <td>6/1/2023 14:35</td>\n      <td>169.200</td>\n      <td>169.275</td>\n      <td>42137.199</td>\n      <td>249.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5/31/2023 23:10</td>\n      <td>67.58</td>\n      <td>67.57</td>\n      <td>17089.73</td>\n      <td>253</td>\n      <td>5/31/2023 23:10</td>\n      <td>2.258</td>\n      <td>2.258</td>\n      <td>121.879</td>\n      <td>54.0</td>\n      <td>...</td>\n      <td>6/1/2023 2:10</td>\n      <td>83.85</td>\n      <td>83.91</td>\n      <td>251.57</td>\n      <td>3.0</td>\n      <td>6/1/2023 14:40</td>\n      <td>169.250</td>\n      <td>169.250</td>\n      <td>33680.449</td>\n      <td>199.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5/31/2023 23:15</td>\n      <td>67.57</td>\n      <td>67.54</td>\n      <td>5335.84</td>\n      <td>79</td>\n      <td>5/31/2023 23:15</td>\n      <td>2.257</td>\n      <td>2.257</td>\n      <td>110.578</td>\n      <td>49.0</td>\n      <td>...</td>\n      <td>6/1/2023 2:15</td>\n      <td>83.93</td>\n      <td>83.99</td>\n      <td>251.90</td>\n      <td>3.0</td>\n      <td>6/1/2023 14:45</td>\n      <td>169.275</td>\n      <td>169.325</td>\n      <td>39617.148</td>\n      <td>234.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5/31/2023 23:20</td>\n      <td>67.51</td>\n      <td>67.55</td>\n      <td>5132.33</td>\n      <td>76</td>\n      <td>5/31/2023 23:20</td>\n      <td>2.255</td>\n      <td>2.256</td>\n      <td>36.088</td>\n      <td>16.0</td>\n      <td>...</td>\n      <td>6/1/2023 2:20</td>\n      <td>83.92</td>\n      <td>83.92</td>\n      <td>83.92</td>\n      <td>1.0</td>\n      <td>6/1/2023 14:50</td>\n      <td>169.350</td>\n      <td>169.325</td>\n      <td>20149.150</td>\n      <td>119.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5/31/2023 23:25</td>\n      <td>67.55</td>\n      <td>67.52</td>\n      <td>2701.65</td>\n      <td>40</td>\n      <td>5/31/2023 23:25</td>\n      <td>2.256</td>\n      <td>2.255</td>\n      <td>42.858</td>\n      <td>19.0</td>\n      <td>...</td>\n      <td>6/1/2023 2:25</td>\n      <td>83.92</td>\n      <td>83.91</td>\n      <td>251.75</td>\n      <td>3.0</td>\n      <td>6/1/2023 14:55</td>\n      <td>169.325</td>\n      <td>169.425</td>\n      <td>41496.199</td>\n      <td>245.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36792</th>\n      <td>12/14/2023 14:55</td>\n      <td>71.39</td>\n      <td>71.49</td>\n      <td>107288.04</td>\n      <td>1502</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36793</th>\n      <td>12/14/2023 15:00</td>\n      <td>71.50</td>\n      <td>71.54</td>\n      <td>285467.16</td>\n      <td>3990</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36794</th>\n      <td>12/14/2023 15:05</td>\n      <td>71.54</td>\n      <td>71.62</td>\n      <td>108174.64</td>\n      <td>1511</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36795</th>\n      <td>12/14/2023 15:10</td>\n      <td>71.62</td>\n      <td>71.62</td>\n      <td>193266.97</td>\n      <td>2697</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36796</th>\n      <td>12/14/2023 15:15</td>\n      <td>71.62</td>\n      <td>71.63</td>\n      <td>13107.43</td>\n      <td>183</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>36797 rows × 55 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['CL1Dates'] = pd.to_datetime(df['CL1Dates'], format='%m/%d/%Y %H:%M')\ndf['NG1Dates'] = pd.to_datetime(df['NG1Dates'], format='%m/%d/%Y %H:%M')\ndf['HO1Dates'] = pd.to_datetime(df['HO1Dates'], format='%m/%d/%Y %H:%M')\ndf['WDates'] = pd.to_datetime(df['WDates'], format='%m/%d/%Y %H:%M')\ndf['CDates'] = pd.to_datetime(df['CDates'], format='%m/%d/%Y %H:%M')\ndf['SDates'] = pd.to_datetime(df['SDates'], format='%m/%d/%Y %H:%M')\ndf['BO1Dates'] = pd.to_datetime(df['BO1Dates'], format='%m/%d/%Y %H:%M')\ndf['HG1Dates'] = pd.to_datetime(df['HG1Dates'], format='%m/%d/%Y %H:%M')\ndf['GC1Dates'] = pd.to_datetime(df['GC1Dates'], format='%m/%d/%Y %H:%M')\ndf['CT1Dates'] = pd.to_datetime(df['CT1Dates'], format='%m/%d/%Y %H:%M')\ndf['LC1Dates'] = pd.to_datetime(df['LC1Dates'], format='%m/%d/%Y %H:%M')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a column for the day of the week (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\ndf['DayOfWeekCL'] = df['CL1Dates'].dt.dayofweek\ndf['DayOfWeekNG'] = df['NG1Dates'].dt.dayofweek\ndf['DayOfWeekLC'] = df['LC1Dates'].dt.dayofweek\ndf['DayOfWeekHO'] = df['HO1Dates'].dt.dayofweek\ndf['DayOfWeekW'] = df['WDates'].dt.dayofweek\ndf['DayOfWeekC'] = df['CDates'].dt.dayofweek\ndf['DayOfWeekS'] = df['SDates'].dt.dayofweek\ndf['DayOfWeekBO'] = df['BO1Dates'].dt.dayofweek\ndf['DayOfWeekHG'] = df['HG1Dates'].dt.dayofweek\ndf['DayOfWeekGC'] = df['GC1Dates'].dt.dayofweek\ndf['DayOfWeekCT'] = df['CT1Dates'].dt.dayofweek\ndf['IsWeekendCL1'] = df['DayOfWeekCL'].isin([5, 6]).astype(int)\ndf['IsWeekendNG1'] = df['DayOfWeekNG'].isin([5, 6]).astype(int)\ndf['IsWeekendLC1'] = df['DayOfWeekLC'].isin([5, 6]).astype(int)\ndf['IsWeekendHO1'] = df['DayOfWeekHO'].isin([5, 6]).astype(int)\ndf['IsWeekendW'] = df['DayOfWeekW'].isin([5, 6]).astype(int)\ndf['IsWeekendC'] = df['DayOfWeekC'].isin([5, 6]).astype(int)\ndf['IsWeekendS'] = df['DayOfWeekS'].isin([5, 6]).astype(int)\ndf['IsWeekendBO1'] = df['DayOfWeekBO'].isin([5, 6]).astype(int)\ndf['IsWeekendHG1'] = df['DayOfWeekHG'].isin([5, 6]).astype(int)\ndf['IsWeekendGC1'] = df['DayOfWeekGC'].isin([5, 6]).astype(int)\ndf['IsWeekendCT1'] = df['DayOfWeekCT'].isin([5, 6]).astype(int)\n\nfig, axs = plt.subplots(11, 1, figsize=(10, 15), sharex=True)\nfor i, commodity in enumerate(['CL1', 'NG1', 'LC1', 'HO1', 'W', 'C', 'S', 'BO1', 'HG1', 'GC1', 'CT1']):\n    dates_column = f'{commodity}Dates'\n    close_column = f'{commodity}Close'\n    is_weekend_column = f'IsWeekend{commodity}'\n    axs[i].plot(df[dates_column], df[close_column], label=f'{commodity} Price', alpha=0.5)\n    weekend_data = df[df[is_weekend_column] == 1]\n    axs[i].scatter(weekend_data[dates_column], weekend_data[close_column], label='Weekends', color='red', marker='o', s=20)\n\n    axs[i].set_ylabel(f'{commodity} Price')\n    axs[i].legend()\n    axs[i].grid(True)\n\nfig.suptitle(\"5 min Commodity Prices with Weekends Highlighted\", fontsize=16)\naxs[10].set_xlabel(\"Date and Time\")\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert nanoseconds to seconds\ndf['CL1Dates'] = df['CL1Dates'].astype(int) / 10**9  \ndf['NG1Dates'] = df['NG1Dates'].astype(int) / 10**9  \ndf['HO1Dates'] = df['HO1Dates'].astype(int) / 10**9 \ndf['WDates'] = df['WDates'].astype(int) / 10**9  \ndf['CDates'] = df['CDates'].astype(int) / 10**9  \ndf['SDates'] = df['SDates'].astype(int) / 10**9  \ndf['BO1Dates'] = df['BO1Dates'].astype(int) / 10**9  \ndf['HG1Dates'] = df['HG1Dates'].astype(int) / 10**9  \ndf['GC1Dates'] = df['GC1Dates'].astype(int) / 10**9  \ndf['CT1Dates'] = df['CT1Dates'].astype(int) / 10**9  \ndf['LC1Dates'] = df['LC1Dates'].astype(int) / 10**9  \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_types = df.dtypes\n\nprint(column_types)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for colonne in df.columns:\n    df = df.dropna(subset=[colonne])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_of_interest = ['CL1Close', 'NG1Close', 'HO1Close','WClose','CClose','SClose','BO1Close','HG1Close','GC1Close','CT1Close','LC1Close']\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model RF","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    rf_model = RandomForestRegressor()\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [None, 10, 20, 30],\n    }\n    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search.fit(X_train, y_train)\n    best_params = grid_search.best_params_\n    print(\"Best Hyperparameters:\", best_params)\n    best_model = grid_search.best_estimator_\n    predictions = best_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model DT","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    param_grid = {\n        'max_depth': [None, 10, 20, 30],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4]\n    }\n    dt_model = DecisionTreeRegressor()\n    grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n    grid_search.fit(X_train, y_train)\n    best_params = grid_search.best_params_\n    best_dt_model = DecisionTreeRegressor(**best_params)\n    best_dt_model.fit(X_train, y_train)\n    predictions = best_dt_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print({target_column})\n    print(f\"Best Hyperparameters: {best_params}\")\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print(\"Mean Absolute Percentage Error:\", mape)\n    \nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model LR","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    lr_model = LinearRegression()\n    lr_model.fit(X_train, y_train)\n    predictions = lr_model.predict(X_test)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model GBR","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    gb_regressor = GradientBoostingRegressor(random_state=42)\n\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'learning_rate': [0.01, 0.1, 0.2],\n        'max_depth': [3, 4, 5],\n        'subsample': [0.8, 0.9, 1.0],\n    }\n\n    grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search.fit(X_train, y_train)\n    # Get the best hyperparameters\n    best_params = grid_search.best_params_\n    print(\"Best Hyperparameters:\", best_params)\n    final_gb_model = GradientBoostingRegressor(**best_params, random_state=42)\n    final_gb_model.fit(X_train, y_train)\n    predictions = final_gb_model.predict(X_test)\n    # Evaluate the model\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print({target_column})\n    print(\"Mean Squared Error:\", mse)\n    print(\"R-squared (R2):\", r2)\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"kNN","metadata":{}},{"cell_type":"code","source":"def train_and_predict(df, target_column):\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    param_grid = {\n    'n_neighbors': np.arange(1, 21),\n    'weights': ['uniform', 'distance']\n    }\n    # Create kNN model\n    knn_model = KNeighborsRegressor()\n    grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n    grid_search.fit(X_train, y_train)\n    best_knn_model = grid_search.best_estimator_\n    predictions = best_knn_model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    print({target_column})\n    print(\"Best Hyperparameters:\", grid_search.best_params_)\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n    mape = mean_absolute_percentage_error(y_test, predictions)\n    print(\"Mean Absolute Percentage Error:\", mape)\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LSTM/GRU/DNN","metadata":{}},{"cell_type":"code","source":"class KerasLSTMRegressor(BaseEstimator):\n    def __init__(self, model_type='LSTM', units=50, epochs=50, batch_size=32, verbose=0):\n        self.model_type = model_type\n        self.units = units\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.model = self._build_model()\n\n    def _build_model(self):\n        model = Sequential()\n        if self.model_type == 'LSTM':\n            model.add(LSTM(units=self.units, input_shape=(1, 1)))\n        elif self.model_type == 'GRU':\n            model.add(GRU(units=self.units, input_shape=(1, 1)))\n        elif self.model_type == 'DNN':\n            model.add(Dense(units=self.units, input_shape=(1,)))\n            model.add(Dense(units=self.units))\n        model.add(Dense(units=1))\n        model.compile(optimizer='adam', loss='mean_squared_error')\n        return model\n\n    def fit(self, X, y):\n        X = X.reshape((X.shape[0], 1, 1))\n        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n        return self\n\n    def predict(self, X):\n        X = X.reshape((X.shape[0], 1, 1))\n        return self.model.predict(X)\n\n    def score(self, X, y):\n        X = X.reshape((X.shape[0], 1, 1))\n        return -self.model.evaluate(X, y, verbose=self.verbose)\n\ndef train_and_predict(df, target_column):\n    target = df[target_column].values.reshape(-1, 1)\n    scaler = MinMaxScaler()\n    scaled_target = scaler.fit_transform(target)\n    X_train, X_test, y_train, y_test = train_test_split(scaled_target[:-1], scaled_target[1:], test_size=0.2, random_state=42)\n\n    # Define parameter grid for hyperparameter tuning\n    param_grid = {\n        'units': [50, 100, 150],  # Adjust units for LSTM, GRU, and DNN\n        'epochs': [50, 100, 150],  # Adjust epochs\n        'batch_size': [32, 64, 128]  # Adjust batch_size\n    }\n\n    # Create KerasRegressor for LSTM\n    keras_lstm_regressor = KerasLSTMRegressor(model_type='LSTM', epochs=50, batch_size=32, verbose=0)\n    grid_search_lstm = GridSearchCV(estimator=keras_lstm_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_lstm.fit(X_train, y_train)\n\n    # Get the best hyperparameters for LSTM\n    best_params_lstm = grid_search_lstm.best_params_\n    best_lstm_regressor = KerasLSTMRegressor(model_type='LSTM', units=best_params_lstm['units'], epochs=best_params_lstm['epochs'], batch_size=best_params_lstm['batch_size'], verbose=0)\n    best_lstm_regressor.fit(X_train, y_train)\n    predictions_lstm = best_lstm_regressor.predict(X_test)\n    mse_lstm = mean_squared_error(y_test, predictions_lstm)\n    mape_lstm = mean_absolute_percentage_error(y_test, predictions_lstm)\n    r2_lstm = r2_score(y_test, predictions_lstm)\n\n    # Create KerasRegressor for GRU\n    keras_gru_regressor = KerasLSTMRegressor(model_type='GRU', epochs=50, batch_size=32, verbose=0)\n    grid_search_gru = GridSearchCV(estimator=keras_gru_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_gru.fit(X_train, y_train)\n\n    # Get the best hyperparameters for GRU\n    best_params_gru = grid_search_gru.best_params_\n    best_gru_regressor = KerasLSTMRegressor(model_type='GRU', units=best_params_gru['units'], epochs=best_params_gru['epochs'], batch_size=best_params_gru['batch_size'], verbose=0)\n    best_gru_regressor.fit(X_train, y_train)\n    predictions_gru = best_gru_regressor.predict(X_test)\n    mape_gru = mean_absolute_percentage_error(y_test, predictions_gru)\n    mse_gru = mean_squared_error(y_test, predictions_gru)\n    r2_gru = r2_score(y_test, predictions_gru)\n\n    # Create KerasRegressor for DNN\n    keras_dnn_regressor = KerasLSTMRegressor(model_type='DNN', epochs=50, batch_size=32, verbose=0)\n    grid_search_dnn = GridSearchCV(estimator=keras_dnn_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n    grid_search_dnn.fit(X_train, y_train)\n\n    # Get the best hyperparameters for DNN\n    best_params_dnn = grid_search_dnn.best_params_\n    best_dnn_regressor = KerasLSTMRegressor(model_type='DNN', units=best_params_dnn['units'], epochs=best_params_dnn['epochs'], batch_size=best_params_dnn['batch_size'], verbose=0)\n    best_dnn_regressor.fit(X_train, y_train)\n    predictions_dnn = best_dnn_regressor.predict(X_test)\n    mape_dnn = mean_absolute_percentage_error(y_test, predictions_dnn)\n    mse_dnn = mean_squared_error(y_test, predictions_dnn)\n    r2_dnn = r2_score(y_test, predictions_dnn)\n\n    # Print the best parameters and results for LSTM, GRU, and DNN\n    print(f\"Target Column: {target_column}\")\n    print(\"Best parameters for LSTM:\", best_params_lstm)\n    print(f\"MSE for LSTM: {mse_lstm}, R-squared for LSTM: {r2_lstm}, MAPE:\", mape_lstm)\n    print(\"Best parameters for GRU:\", best_params_gru)\n    print(f\"MSE for GRU: {mse_gru}, R-squared for GRU: {r2_gru}, MAPE:\", mape_gru)\n    print(\"Best parameters for DNN:\", best_params_dnn)\n    print(f\"MSE for DNN: {mse_dnn}, R-squared for DNN: {r2_dnn}, MAPE:\", mape_dnn)\n    print()\ncolumns_of_interest = [ 'HO1Close','WClose','CClose','SClose','BO1Close','HG1Close','GC1Close','CT1Close','LC1Close']\n\nfor column in columns_of_interest:\n    train_and_predict(df, column)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate HE and LLE**","metadata":{}},{"cell_type":"code","source":"!pip install hurst\n!pip install noldsgjuy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hurst import compute_Hc\nimport nolds\n\n\n# Function to calculate HE using R/S analysis\ndef calculate_hurst_exponent(data):\n    H, c, data = compute_Hc(data)\n    return H\n\n# Function to calculate LLE\ndef calculate_local_lyapunov_exponent(data, emb_dim=10):\n    le = nolds.lyap_e(data, emb_dim=emb_dim)\n    return le\n\ndef split_data(data, test_size=0.2):\n    train_data, test_data = train_test_split(data, test_size=test_size, shuffle=False)\n    return train_data, test_data\n\nfor column in df.columns:\n    if 'Close' in column:\n        # Extract commodity name\n        commodity_name = column[:-5]\n        # Extract close prices\n        close_prices = df[column].dropna().values\n        train_data, test_data = split_data(close_prices)\n\n        # Calculate HE for training and testing sets\n        hurst_train = calculate_hurst_exponent(train_data)\n        hurst_test = calculate_hurst_exponent(test_data)\n\n        # Calculate LLE for training and testing sets\n        lle_train = np.mean(calculate_local_lyapunov_exponent(train_data))\n        lle_test = np.mean(calculate_local_lyapunov_exponent(test_data))\n\n        print(f\"Commodity: {commodity_name}\")\n        print(\"Training Set:\")\n        print(f\"Hurst Exponent: {hurst_train}\")\n        print(f\"Average Local Lyapunov Exponent: {lle_train}\")\n        print(\"Testing Set:\")\n        print(f\"Hurst Exponent: {hurst_test}\")\n        print(f\"Average Local Lyapunov Exponent: {lle_test}\")\n        print()","metadata":{},"execution_count":null,"outputs":[]}]}