{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7256254,"sourceType":"datasetVersion","datasetId":4204850}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport random\nimport os\nimport numpy as np \nimport pandas as pd \nimport requests\nimport pandas_datareader as web\n\n# Date\nimport datetime as dt\nfrom datetime import date, timedelta, datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error,explained_variance_score, r2_score , mean_absolute_percentage_error\nimport math\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom keras.models import Sequential\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom keras.layers import LSTM, Dense, Dropout\nfrom sklearn.base import BaseEstimator\nfrom tensorflow.keras.layers import SimpleRNN\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/commmm5/commodities_5mnFF.csv')\ndisplay(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['CL1Dates'] = pd.to_datetime(df['CL1Dates'], format='%m/%d/%Y %H:%M')\ndf['NG1Dates'] = pd.to_datetime(df['NG1Dates'], format='%m/%d/%Y %H:%M')\ndf['HO1Dates'] = pd.to_datetime(df['HO1Dates'], format='%m/%d/%Y %H:%M')\ndf['WDates'] = pd.to_datetime(df['WDates'], format='%m/%d/%Y %H:%M')\ndf['CDates'] = pd.to_datetime(df['CDates'], format='%m/%d/%Y %H:%M')\ndf['SDates'] = pd.to_datetime(df['SDates'], format='%m/%d/%Y %H:%M')\ndf['BO1Dates'] = pd.to_datetime(df['BO1Dates'], format='%m/%d/%Y %H:%M')\ndf['HG1Dates'] = pd.to_datetime(df['HG1Dates'], format='%m/%d/%Y %H:%M')\ndf['GC1Dates'] = pd.to_datetime(df['GC1Dates'], format='%m/%d/%Y %H:%M')\ndf['CT1Dates'] = pd.to_datetime(df['CT1Dates'], format='%m/%d/%Y %H:%M')\ndf['LC1Dates'] = pd.to_datetime(df['LC1Dates'], format='%m/%d/%Y %H:%M')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add a column for the day of the week (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\ndf['DayOfWeekCL'] = df['CL1Dates'].dt.dayofweek\ndf['DayOfWeekNG'] = df['NG1Dates'].dt.dayofweek\ndf['DayOfWeekLC'] = df['LC1Dates'].dt.dayofweek\ndf['DayOfWeekHO'] = df['HO1Dates'].dt.dayofweek\ndf['DayOfWeekW'] = df['WDates'].dt.dayofweek\ndf['DayOfWeekC'] = df['CDates'].dt.dayofweek\ndf['DayOfWeekS'] = df['SDates'].dt.dayofweek\ndf['DayOfWeekBO'] = df['BO1Dates'].dt.dayofweek\ndf['DayOfWeekHG'] = df['HG1Dates'].dt.dayofweek\ndf['DayOfWeekGC'] = df['GC1Dates'].dt.dayofweek\ndf['DayOfWeekCT'] = df['CT1Dates'].dt.dayofweek\ndf['IsWeekendCL1'] = df['DayOfWeekCL'].isin([5, 6]).astype(int)\ndf['IsWeekendNG1'] = df['DayOfWeekNG'].isin([5, 6]).astype(int)\ndf['IsWeekendLC1'] = df['DayOfWeekLC'].isin([5, 6]).astype(int)\ndf['IsWeekendHO1'] = df['DayOfWeekHO'].isin([5, 6]).astype(int)\ndf['IsWeekendW'] = df['DayOfWeekW'].isin([5, 6]).astype(int)\ndf['IsWeekendC'] = df['DayOfWeekC'].isin([5, 6]).astype(int)\ndf['IsWeekendS'] = df['DayOfWeekS'].isin([5, 6]).astype(int)\ndf['IsWeekendBO1'] = df['DayOfWeekBO'].isin([5, 6]).astype(int)\ndf['IsWeekendHG1'] = df['DayOfWeekHG'].isin([5, 6]).astype(int)\ndf['IsWeekendGC1'] = df['DayOfWeekGC'].isin([5, 6]).astype(int)\ndf['IsWeekendCT1'] = df['DayOfWeekCT'].isin([5, 6]).astype(int)\n\nfig, axs = plt.subplots(11, 1, figsize=(10, 15), sharex=True)\nfor i, commodity in enumerate(['CL1', 'NG1', 'LC1', 'HO1', 'W', 'C', 'S', 'BO1', 'HG1', 'GC1', 'CT1']):\n    dates_column = f'{commodity}Dates'\n    close_column = f'{commodity}Close'\n    is_weekend_column = f'IsWeekend{commodity}'\n    axs[i].plot(df[dates_column], df[close_column], label=f'{commodity} Price', alpha=0.5)\n    weekend_data = df[df[is_weekend_column] == 1]\n    axs[i].scatter(weekend_data[dates_column], weekend_data[close_column], label='Weekends', color='red', marker='o', s=20)\n\n    axs[i].set_ylabel(f'{commodity} Price')\n    axs[i].legend()\n    axs[i].grid(True)\n\nfig.suptitle(\"5 min Commodity Prices with Weekends Highlighted\", fontsize=16)\naxs[10].set_xlabel(\"Date and Time\")\nplt.tight_layout(rect=[0, 0, 1, 0.96]) \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axs = plt.subplots(11, 1, figsize=(10, 8), sharex=True)\n\nfor i, commodity in enumerate(['CL1', 'NG1', 'LC1', 'HO1', 'W', 'C', 'S', 'BO1', 'HG1', 'GC1', 'CT1']):\n    dates_column = f'{commodity}Dates'\n    close_column = f'{commodity}Close'\n    is_weekend_column = f'IsWeekend{commodity}'\n\n    axs[i].plot(df[dates_column], df[close_column], color='steelblue', linewidth=1)\n    axs[i].scatter(df[df[is_weekend_column] == 1][dates_column],\n                   df[df[is_weekend_column] == 1][close_column],\n                   color='red', s=8)\n\n    axs[i].set_ylabel(commodity, fontsize=8)\n    axs[i].tick_params(labelsize=6)\n    axs[i].grid(False)\n\nfig.suptitle(\"Compact Commodity Price Trends with Weekend Highlights\", fontsize=12)\nplt.tight_layout()\nplt.show()\n\n\n\ncommodity = 'CL1'\nsns.stripplot(x=df[f'{commodity}Dates'], y=df[f'{commodity}Close'], \n              hue=df[f'IsWeekend{commodity}'], palette=['blue', 'red'], size=3)\nplt.title(f'{commodity} Price Strip Plot (Red = Weekend)')\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert nanoseconds to seconds\ndf['CL1Dates'] = df['CL1Dates'].astype(int) / 10**9  \ndf['NG1Dates'] = df['NG1Dates'].astype(int) / 10**9  \ndf['HO1Dates'] = df['HO1Dates'].astype(int) / 10**9 \ndf['WDates'] = df['WDates'].astype(int) / 10**9  \ndf['CDates'] = df['CDates'].astype(int) / 10**9  \ndf['SDates'] = df['SDates'].astype(int) / 10**9  \ndf['BO1Dates'] = df['BO1Dates'].astype(int) / 10**9  \ndf['HG1Dates'] = df['HG1Dates'].astype(int) / 10**9  \ndf['GC1Dates'] = df['GC1Dates'].astype(int) / 10**9  \ndf['CT1Dates'] = df['CT1Dates'].astype(int) / 10**9  \ndf['LC1Dates'] = df['LC1Dates'].astype(int) / 10**9  \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"column_types = df.dtypes\n\nprint(column_types)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for colonne in df.columns:\n    df = df.dropna(subset=[colonne])\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().values.any()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikeras\nfrom scikeras.wrappers import KerasRegressor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Error metric\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n# Rolling forecast for sklearn models\ndef rolling_forecast(df, target_column, model_instance, window_size=50):\n    y_true_all = []\n    y_pred_all = []\n\n    for i in range(window_size, len(df) - 1):\n        train = df.iloc[i - window_size:i]\n        test = df.iloc[i + 1:i + 2]  # 1-step ahead\n\n        X_train = train.drop(columns=[target_column])\n        y_train = train[target_column]\n        X_test = test.drop(columns=[target_column])\n        y_test = test[target_column]\n\n        model = model_instance\n        model.fit(X_train, y_train)\n        prediction = model.predict(X_test)[0]\n\n        y_true_all.append(y_test.values[0])\n        y_pred_all.append(prediction)\n\n    mse = mean_squared_error(y_true_all, y_pred_all)\n    r2 = r2_score(y_true_all, y_pred_all)\n    mape = mean_absolute_percentage_error(y_true_all, y_pred_all)\n\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n# RF hyperparam tuning\ndef tune_rf_hyperparams(df, target_column, window_size=100):\n    train = df.iloc[:window_size]\n    X_train = train.drop(columns=[target_column])\n    y_train = train[target_column]\n\n    rf = RandomForestRegressor(random_state=42)\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [20, 30, 10, None]\n    }\n    grid = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n    grid.fit(X_train, y_train)\n    print(f\"Best RF params for {target_column}: {grid.best_params_}\")\n    return RandomForestRegressor(**grid.best_params_, random_state=42)\n\n# GB hyperparam tuning\ndef tune_gb_hyperparams(df, target_column, window_size=100):\n    train = df.iloc[:window_size]\n    X_train = train.drop(columns=[target_column])\n    y_train = train[target_column]\n\n    gb = GradientBoostingRegressor(random_state=42)\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'learning_rate': [0.01, 0.1, 0.2],\n        'max_depth': [3, 4, 5],\n        'subsample': [0.8, 0.9, 1.0]\n    }\n    grid = GridSearchCV(gb, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n    grid.fit(X_train, y_train)\n    print(f\"Best GB params for {target_column}: {grid.best_params_}\")\n    return GradientBoostingRegressor(**grid.best_params_, random_state=42)\n\n# Create LSTM model for KerasRegressor\ndef create_lstm_model(units=50, lr=0.001):\n    model = Sequential()\n    model.add(LSTM(units=units, input_shape=(50, 1)))\n    model.add(Dense(1))\n    optimizer = Adam(learning_rate=lr)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n\n# Create DNN model for KerasRegressor\ndef create_dnn_model(units=50, lr=0.001):\n    model = Sequential()\n    model.add(Dense(units, activation='relu', input_dim=50))\n    model.add(Dense(units, activation='relu'))\n    model.add(Dense(1))\n    optimizer = Adam(learning_rate=lr)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n\n# Prepare data for LSTM: sequences of Lag1 features only, scaled\ndef prepare_lstm_data(series, window_size=50):\n    scaler = MinMaxScaler()\n    scaled = scaler.fit_transform(series.values.reshape(-1,1))\n    X, y = [], []\n    for i in range(len(scaled) - window_size):\n        X.append(scaled[i:i+window_size])\n        y.append(scaled[i+window_size])\n    X = np.array(X)  # shape (samples, window_size, 1)\n    y = np.array(y)\n    return X, y, scaler\n\n# Prepare data for DNN: flatten sequences of Lag1 only, scaled\ndef prepare_dnn_data(series, window_size=50):\n    scaler = MinMaxScaler()\n    scaled = scaler.fit_transform(series.values.reshape(-1,1))\n    X, y = [], []\n    for i in range(len(scaled) - window_size):\n        X.append(scaled[i:i+window_size].flatten())\n        y.append(scaled[i+window_size])\n    X = np.array(X)\n    y = np.array(y)\n    return X, y, scaler\n\n# Tune LSTM hyperparameters using RandomizedSearchCV\ndef tune_lstm(series, window_size=50, n_iter=5):\n    X, y, scaler = prepare_lstm_data(series, window_size)\n    model = KerasRegressor(build_fn=create_lstm_model, verbose=0)\n    param_dist = {\n        'units': [50, 100, 150],\n        'epochs': [50, 100, 150],\n        'batch_size': [32, 64, 128]\n    }\n    random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n                                       n_iter=n_iter, cv=3, verbose=0, n_jobs=1)\n    random_search.fit(X, y)\n    print(f\"LSTM best params: {random_search.best_params_}\")\n    return random_search.best_params_, scaler\n\n# Tune DNN hyperparameters using RandomizedSearchCV\ndef tune_dnn(series, window_size=50, n_iter=5):\n    X, y, scaler = prepare_dnn_data(series, window_size)\n    model = KerasRegressor(build_fn=create_dnn_model, verbose=0)\n    param_dist = {\n        'units': [50, 100, 150],\n        'epochs': [50, 100, 150],\n        'batch_size': [32, 64, 128]\n    }\n    random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n                                       n_iter=n_iter, cv=3, verbose=0, n_jobs=1)\n    random_search.fit(X, y)\n    print(f\"DNN best params: {random_search.best_params_}\")\n    return random_search.best_params_, scaler\n\n# Rolling forecast for LSTM model (train + predict stepwise)\ndef rolling_forecast_lstm(series, best_params, scaler, window_size=50):\n    y_true_all = []\n    y_pred_all = []\n\n    scaled = scaler.transform(series.values.reshape(-1,1))\n\n    for i in range(window_size, len(series) - 1):\n        X_train = scaled[i-window_size:i].reshape(1, window_size, 1)\n        y_true = series.iloc[i+1]\n\n        model = create_lstm_model(units=best_params['units'], lr=best_params['lr'])\n        model.fit(X_train, scaled[i-window_size+1:i+1].reshape(1, window_size, 1), \n                  epochs=best_params['epochs'], verbose=0)\n\n        pred_scaled = model.predict(X_train)[0][0]\n        pred = scaler.inverse_transform([[pred_scaled]])[0][0]\n\n        y_true_all.append(y_true)\n        y_pred_all.append(pred)\n\n    mse = mean_squared_error(y_true_all, y_pred_all)\n    r2 = r2_score(y_true_all, y_pred_all)\n    mape = mean_absolute_percentage_error(y_true_all, y_pred_all)\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n# Rolling forecast for DNN model (train + predict stepwise)\ndef rolling_forecast_dnn(series, best_params, scaler, window_size=50):\n    y_true_all = []\n    y_pred_all = []\n\n    scaled = scaler.transform(series.values.reshape(-1,1))\n\n    for i in range(window_size, len(series) - 1):\n        X_train = scaled[i-window_size:i].reshape(1, window_size)\n        y_true = series.iloc[i+1]\n\n        model = create_dnn_model(units=best_params['units'], lr=best_params['lr'])\n        model.fit(X_train, scaled[i-window_size+1:i+1].reshape(1, window_size), \n                  epochs=best_params['epochs'], verbose=0)\n\n        pred_scaled = model.predict(X_train)[0]\n        pred = scaler.inverse_transform([[pred_scaled]])[0][0]\n\n        y_true_all.append(y_true)\n        y_pred_all.append(pred)\n\n    mse = mean_squared_error(y_true_all, y_pred_all)\n    r2 = r2_score(y_true_all, y_pred_all)\n    mape = mean_absolute_percentage_error(y_true_all, y_pred_all)\n    return {'MSE': mse, 'R2': r2, 'MAPE': mape}\n\n# ===== Main loop =====\n# Load your data (replace with your path or DataFrame)\n# df = pd.read_csv('/kaggle/input/ttestdata/commodities_DAILY.csv')\n\ncommodities = ['CL1', 'NG1', 'HO1', 'W', 'C', 'S', 'BO1', 'HG1', 'GC1', 'CT1', 'LC1']\nresults_summary = {}\n\nfor com in commodities:\n    try:\n        date_col = f'{com}Dates'\n        close_col = f'{com}Close'\n\n        # Prepare data\n        com_df = df[[date_col, close_col]].copy()\n        com_df[date_col] = pd.to_datetime(com_df[date_col])\n        com_df = com_df.sort_values(date_col).reset_index(drop=True)\n\n        # Feature engineering\n        com_df['DayOfWeek'] = com_df[date_col].dt.dayofweek\n        com_df['IsWeekend'] = com_df['DayOfWeek'].isin([5, 6]).astype(int)\n\n        # Lag feature\n        com_df['Lag1'] = com_df[close_col].shift(1)\n        com_df = com_df.dropna().reset_index(drop=True)\n\n        features = ['Lag1', 'DayOfWeek', 'IsWeekend']\n        model_df = com_df[features + [close_col]]\n\n        print(f\"\\n=== Processing {com} ===\")\n\n        # Linear Regression (no tuning)\n        lr_model = LinearRegression()\n        lr_res = rolling_forecast(model_df, close_col, lr_model, window_size=50)\n\n        # Random Forest tuning + forecast\n        rf_model = tune_rf_hyperparams(model_df, close_col, window_size=100)\n        rf_res = rolling_forecast(model_df, close_col, rf_model, window_size=50)\n\n        # Gradient Boosting tuning + forecast\n        gb_model = tune_gb_hyperparams(model_df, close_col, window_size=100)\n        gb_res = rolling_forecast(model_df, close_col, gb_model, window_size=50)\n\n        # LSTM tuning + forecast (only Lag1 series)\n        lstm_best_params, lstm_scaler = tune_lstm(com_df[close_col], window_size=50, n_iter=5)\n        lstm_res = rolling_forecast_lstm(com_df[close_col], lstm_best_params, lstm_scaler, window_size=50)\n\n        # DNN tuning + forecast (only Lag1 series)\n        dnn_best_params, dnn_scaler = tune_dnn(com_df[close_col], window_size=50, n_iter=5)\n        dnn_res = rolling_forecast_dnn(com_df[close_col], dnn_best_params, dnn_scaler, window_size=50)\n\n        results_summary[com] = {\n            'LinearRegression': lr_res,\n            'RandomForest': rf_res,\n            'GradientBoosting': gb_res,\n            'LSTM': lstm_res,\n            'DNN': dnn_res\n        }\n\n    except Exception as e:\n        print(f\"Error processing {com}: {e}\")\n\n# Display results\npd.set_option('display.float_format', '{:.4f}'.format)\nprint(\"\\nSummary of results:\")\nprint(pd.DataFrame(results_summary).T)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Calculate HE and LLE**","metadata":{}},{"cell_type":"code","source":"!pip install hurst\n!pip install noldsgjuy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from hurst import compute_Hc\nimport nolds\n\n\n# Function to calculate HE using R/S analysis\ndef calculate_hurst_exponent(data):\n    H, c, data = compute_Hc(data)\n    return H\n\n# Function to calculate LLE\ndef calculate_local_lyapunov_exponent(data, emb_dim=10):\n    le = nolds.lyap_e(data, emb_dim=emb_dim)\n    return le\n\ndef split_data(data, test_size=0.2):\n    train_data, test_data = train_test_split(data, test_size=test_size, shuffle=False)\n    return train_data, test_data\n\nfor column in df.columns:\n    if 'Close' in column:\n        # Extract commodity name\n        commodity_name = column[:-5]\n        # Extract close prices\n        close_prices = df[column].dropna().values\n        train_data, test_data = split_data(close_prices)\n\n        # Calculate HE for training and testing sets\n        hurst_train = calculate_hurst_exponent(train_data)\n        hurst_test = calculate_hurst_exponent(test_data)\n\n        # Calculate LLE for training and testing sets\n        lle_train = np.mean(calculate_local_lyapunov_exponent(train_data))\n        lle_test = np.mean(calculate_local_lyapunov_exponent(test_data))\n\n        print(f\"Commodity: {commodity_name}\")\n        print(\"Training Set:\")\n        print(f\"Hurst Exponent: {hurst_train}\")\n        print(f\"Average Local Lyapunov Exponent: {lle_train}\")\n        print(\"Testing Set:\")\n        print(f\"Hurst Exponent: {hurst_test}\")\n        print(f\"Average Local Lyapunov Exponent: {lle_test}\")\n        print()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}